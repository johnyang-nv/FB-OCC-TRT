From 5f8f3f4cdf773d1e4c72a2c9fd91da4ffab91f18 Mon Sep 17 00:00:00 2001
From: "John Yang (SW-TEGRA)" <johnyang@nvidia.com>
Date: Tue, 17 Dec 2024 15:39:52 -0800
Subject: [PATCH] fb-occ_trt_plugin

---
 TensorRT/CMakeLists.txt                       |   37 -
 TensorRT/Makefile                             |   25 +
 TensorRT/Makefile.config                      |  207 +++
 TensorRT/README.md                            |  277 ----
 TensorRT/build/.gitignore                     |    2 -
 TensorRT/cudaComputeVersion.cu                |    9 -
 .../bev_pool_v2 => include}/bevPoolKernel.h   |    5 +-
 .../bev_pool_v2 => include}/bevPoolPlugin.h   |    2 +-
 .../{common => include}/checkMacrosPlugin.cpp |    0
 .../{common => include}/checkMacrosPlugin.h   |    0
 TensorRT/{common => include}/cuda_helper.cu   |    0
 TensorRT/{common => include}/cuda_helper.h    |    0
 TensorRT/{common => include}/cuda_int8.h      |    0
 .../gridSamplerKernel.h                       |    0
 .../gridSamplerPlugin.h                       |    0
 TensorRT/{common => include}/helper.h         |    0
 .../inverse => include}/inverseKernel.h       |    0
 .../inverse => include}/inversePlugin.h       |    0
 .../modulatedDeformableConv2dKernel.h         |    0
 .../modulatedDeformableConv2dPlugin.h         |    0
 .../multiHeadAttnKernel.h                     |    0
 .../multiHeadAttnPlugin.h                     |    0
 .../multiHeadFlashAttnKernel.h                |    0
 .../include/multiScaleDeformableAttnKernel.h  |   30 +
 .../multiScaleDeformableAttnPlugin.h          |    2 +-
 .../{plugin/rotate => include}/rotateKernel.h |    0
 .../{plugin/rotate => include}/rotatePlugin.h |    0
 TensorRT/{common => include}/serialize.h      |    0
 TensorRT/install.sh                           |    4 -
 .../bev_pool_v2 => kernels}/bevPoolKernel.cu  |    0
 .../gridSamplerKernel.cu                      |    0
 .../inverse => kernels}/inverseKernel.cu      |    0
 .../modulatedDeformableConv2dKernel.cu        |    0
 .../multiHeadAttnKernel.cu                    |    0
 .../multiHeadFlashAttnKernel.cu               |    0
 .../kernels/multiScaleDeformableAttnKernel.cu |  468 ++++++
 .../rotate => kernels}/rotateKernel.cu        |    0
 TensorRT/lib/.gitignore                       |    2 -
 .../multiScaleDeformableAttnKernel.cu         | 1254 -----------------
 .../multiScaleDeformableAttnKernel.h          |   40 -
 .../bev_pool_v2 => plugins}/bevPoolPlugin.cpp |  117 +-
 .../gridSamplerPlugin.cpp                     |    4 +
 .../inverse => plugins}/inversePlugin.cpp     |    0
 .../modulatedDeformableConv2dPlugin.cpp       |    0
 .../multiHeadAttnPlugin.cpp                   |    0
 .../multiScaleDeformableAttnPlugin.cpp        |  100 +-
 .../rotate => plugins}/rotatePlugin.cpp       |    0
 47 files changed, 852 insertions(+), 1733 deletions(-)
 delete mode 100644 TensorRT/CMakeLists.txt
 create mode 100644 TensorRT/Makefile
 create mode 100644 TensorRT/Makefile.config
 delete mode 100644 TensorRT/README.md
 delete mode 100644 TensorRT/build/.gitignore
 delete mode 100644 TensorRT/cudaComputeVersion.cu
 rename TensorRT/{plugin/bev_pool_v2 => include}/bevPoolKernel.h (94%)
 rename TensorRT/{plugin/bev_pool_v2 => include}/bevPoolPlugin.h (99%)
 rename TensorRT/{common => include}/checkMacrosPlugin.cpp (100%)
 rename TensorRT/{common => include}/checkMacrosPlugin.h (100%)
 rename TensorRT/{common => include}/cuda_helper.cu (100%)
 rename TensorRT/{common => include}/cuda_helper.h (100%)
 rename TensorRT/{common => include}/cuda_int8.h (100%)
 rename TensorRT/{plugin/grid_sampler => include}/gridSamplerKernel.h (100%)
 rename TensorRT/{plugin/grid_sampler => include}/gridSamplerPlugin.h (100%)
 rename TensorRT/{common => include}/helper.h (100%)
 rename TensorRT/{plugin/inverse => include}/inverseKernel.h (100%)
 rename TensorRT/{plugin/inverse => include}/inversePlugin.h (100%)
 rename TensorRT/{plugin/modulated_deformable_conv2d => include}/modulatedDeformableConv2dKernel.h (100%)
 rename TensorRT/{plugin/modulated_deformable_conv2d => include}/modulatedDeformableConv2dPlugin.h (100%)
 rename TensorRT/{plugin/multi_head_attn => include}/multiHeadAttnKernel.h (100%)
 rename TensorRT/{plugin/multi_head_attn => include}/multiHeadAttnPlugin.h (100%)
 rename TensorRT/{plugin/multi_head_attn => include}/multiHeadFlashAttnKernel.h (100%)
 create mode 100644 TensorRT/include/multiScaleDeformableAttnKernel.h
 rename TensorRT/{plugin/multi_scale_deformable_attn => include}/multiScaleDeformableAttnPlugin.h (98%)
 rename TensorRT/{plugin/rotate => include}/rotateKernel.h (100%)
 rename TensorRT/{plugin/rotate => include}/rotatePlugin.h (100%)
 rename TensorRT/{common => include}/serialize.h (100%)
 delete mode 100644 TensorRT/install.sh
 rename TensorRT/{plugin/bev_pool_v2 => kernels}/bevPoolKernel.cu (100%)
 rename TensorRT/{plugin/grid_sampler => kernels}/gridSamplerKernel.cu (100%)
 rename TensorRT/{plugin/inverse => kernels}/inverseKernel.cu (100%)
 rename TensorRT/{plugin/modulated_deformable_conv2d => kernels}/modulatedDeformableConv2dKernel.cu (100%)
 rename TensorRT/{plugin/multi_head_attn => kernels}/multiHeadAttnKernel.cu (100%)
 rename TensorRT/{plugin/multi_head_attn => kernels}/multiHeadFlashAttnKernel.cu (100%)
 create mode 100644 TensorRT/kernels/multiScaleDeformableAttnKernel.cu
 rename TensorRT/{plugin/rotate => kernels}/rotateKernel.cu (100%)
 delete mode 100644 TensorRT/lib/.gitignore
 delete mode 100644 TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.cu
 delete mode 100644 TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.h
 rename TensorRT/{plugin/bev_pool_v2 => plugins}/bevPoolPlugin.cpp (73%)
 rename TensorRT/{plugin/grid_sampler => plugins}/gridSamplerPlugin.cpp (99%)
 rename TensorRT/{plugin/inverse => plugins}/inversePlugin.cpp (100%)
 rename TensorRT/{plugin/modulated_deformable_conv2d => plugins}/modulatedDeformableConv2dPlugin.cpp (100%)
 rename TensorRT/{plugin/multi_head_attn => plugins}/multiHeadAttnPlugin.cpp (100%)
 rename TensorRT/{plugin/multi_scale_deformable_attn => plugins}/multiScaleDeformableAttnPlugin.cpp (77%)
 rename TensorRT/{plugin/rotate => plugins}/rotatePlugin.cpp (100%)

diff --git a/TensorRT/CMakeLists.txt b/TensorRT/CMakeLists.txt
deleted file mode 100644
index 8ff6da2..0000000
--- a/TensorRT/CMakeLists.txt
+++ /dev/null
@@ -1,37 +0,0 @@
-cmake_minimum_required(VERSION 3.12)
-
-project(tensorrt_ops LANGUAGES CXX CUDA)
-find_package(CUDA REQUIRED)
-
-execute_process(COMMAND nvcc -lcuda ${CMAKE_CURRENT_SOURCE_DIR}/cudaComputeVersion.cu -o /tmp/cudaComputeVersion)
-execute_process(COMMAND /tmp/cudaComputeVersion
-        RESULT_VARIABLE CUDA_RETURN_CODE
-        OUTPUT_VARIABLE ARCH)
-execute_process(COMMAND rm /tmp/cudaComputeVersion)
-
-if(NOT CMAKE_TENSORRT_PATH)
-    message( "Need: -DCMAKE_TENSORRT_PATH=/path/to/TensorRT")
-
-endif()
-
-include_directories(common)
-include_directories(/usr/local/cuda/include)
-include_directories(/usr/local/include)
-
-message(STATUS "Linking TensorRT directory: ${CMAKE_TENSORRT_PATH}")
-link_directories(${CMAKE_TENSORRT_PATH}/lib)
-include_directories(${CMAKE_TENSORRT_PATH}/include)
-
-file(GLOB_RECURSE TENSORRT_OPS_SRCS common/*.cpp common/*.cu plugin/*/*.cu plugin/*/*.cpp)
-
-set(SHARED_TARGET tensorrt_ops)
-add_library(${SHARED_TARGET} SHARED ${TENSORRT_OPS_SRCS})
-target_compile_options(${SHARED_TARGET} PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:-arch=sm_${ARCH}>)
-
-target_link_libraries(${SHARED_TARGET} PUBLIC cudnn nvinfer cublas)
-set_target_properties(${SHARED_TARGET} PROPERTIES CUDA_ARCHITECTURES "${ARCH}")
-
-install(
-        TARGETS ${SHARED_TARGET}
-        LIBRARY DESTINATION ${CMAKE_SOURCE_DIR}/lib
-)
diff --git a/TensorRT/Makefile b/TensorRT/Makefile
new file mode 100644
index 0000000..b6fd841
--- /dev/null
+++ b/TensorRT/Makefile
@@ -0,0 +1,25 @@
+# SPDX-FileCopyrightText: Copyright (c) 2023-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+SHELL = /bin/bash -o pipefail
+TARGET ?= $(shell uname -m)
+VERBOSE ?= 0
+ifeq ($(VERBOSE), 1)
+	AT =
+else
+	AT = @
+endif
+
+OUTNAME = fb-occ_trt_plugin
+SRC_FILES = plugins/*.cpp
+CU_FILES = kernels/*.cu
+EXTRA_INCPATHS = -Iinclude
+MAKEFILE ?= ./Makefile.config
+include $(MAKEFILE)
diff --git a/TensorRT/Makefile.config b/TensorRT/Makefile.config
new file mode 100644
index 0000000..358b03d
--- /dev/null
+++ b/TensorRT/Makefile.config
@@ -0,0 +1,207 @@
+# SPDX-FileCopyrightText: Copyright (c) 2023-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
+#
+# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
+# property and proprietary rights in and to this material, related
+# documentation and any modifications thereto. Any use, reproduction,
+# disclosure or distribution of this material and related documentation
+# without an express license agreement from NVIDIA CORPORATION or
+# its affiliates is strictly prohibited.
+
+SRC = $(SRC_FILES)
+
+EXTRA_DIRECTORIES =
+EXTRA_FILES =
+
+USE_QCC ?= 1
+CUDA ?= cuda-11.4
+TARGET ?= $(shell uname -m)
+
+.SUFFIXES:
+CUDA_TRIPLE ?= x86_64-linux
+DLSW_TRIPLE ?= x86_64-linux-gnu
+TARGET_RELEASE = $(OUTNAME)_x86_64.so
+TARGET_DEBUG   = $(OUTNAME)_x86_64_debug.so
+ifeq ($(TARGET), aarch64)
+  CUDA_TRIPLE = aarch64-linux
+  DLSW_TRIPLE = aarch64-linux-gnu
+  PDK_DIR ?= /drive/drive-linux
+  TARGET_RELEASE = $(OUTNAME)_aarch64.so
+  TARGET_DEBUG = $(OUTNAME)_aarch64_debug.so
+endif
+
+ifeq ($(CUDA_INSTALL_DIR),)
+  CUDA_INSTALL_DIR ?= /usr/local/cuda
+  $(warning CUDA_INSTALL_DIR variable is not specified, using $(CUDA_INSTALL_DIR) by default, use CUDA_INSTALL_DIR=<cuda_directory> to change.)
+endif
+
+PDK_LIB_DIR :=
+PDK_INC_DIR :=
+ifneq ($(PDK_DIR),)
+  PDK_LIB_DIR := $(PDK_DIR)/lib-target
+  PDK_INC_DIR := $(PDK_DIR)/include
+endif
+
+ifeq ($(TRT_INC_DIR),)
+  TRT_INC_DIR ?= /usr/include/$(DLSW_TRIPLE)
+  $(warning TRT_INC_DIR is not specified, searching $(TRT_INC_DIR) by default, use TRT_INC_DIR=<trt_inc_directory> to change.)
+endif
+
+ifeq ($(TRT_LIB_DIR),)
+  TRT_LIB_DIR ?= /usr/lib/$(DLSW_TRIPLE)
+  $(warning TRT_LIB_DIR is not specified, searching $(TRT_LIB_DIR) by default, use TRT_LIB_DIR=<trt_lib_directory> to change.)
+endif
+
+NVCC ?= $(CUDA_INSTALL_DIR)/bin/nvcc
+
+ifeq ($(TARGET), aarch64)
+  ifeq ($(shell uname -m), aarch64)
+    CC = g++
+  else
+    CC = aarch64-linux-gnu-g++
+  endif
+  CUCC = $(NVCC) -m64 -ccbin $(CC) --target-directory $(CUDA_TRIPLE)
+else ifeq ($(TARGET), x86_64)
+  CC = g++
+  CUCC = $(NVCC) -m64 --target-directory $(CUDA_TRIPLE)
+else
+  $(error Auto-detection of platform failed. Please specify one of the following arguments to make: TARGET=[aarch64|x86_64])
+endif
+
+ifdef VERBOSE
+  AT=
+else
+  AT=@
+endif
+
+AR = ar cr
+ECHO = @echo
+
+SHELL = /bin/sh
+
+ROOT_PATH ?= .
+OUT_PATH = $(ROOT_PATH)/bin/$(TARGET)
+OUTDIR = $(OUT_PATH)
+
+define concat
+$1$2$3$4$5$6$7$8
+endef
+
+define make-dir
+  if [ ! -d $1 ]; then mkdir -p $1; fi
+endef
+
+
+CUDART_LIB = -lcudart
+NVINFER_LIB = -lnvinfer -lnvinfer_plugin 
+
+
+######################
+INCPATHS=
+LIBPATHS=
+COMMON_LIBS=
+
+ifneq ($(shell uname -m), $(TARGET))
+  LIBPATHS += -L"/usr/lib/$(DLSW_TRIPLE)/stubs"
+  LIBPATHS += -L"$(CUDA_INSTALL_DIR)/targets/$(CUDA_TRIPLE)/lib/stubs"
+else
+  INCPATHS += -I"$(CUDA_INSTALL_DIR)/include"
+endif
+INCPATHS += -I"$(CUDA_INSTALL_DIR)/targets/$(CUDA_TRIPLE)/include"
+LIBPATHS += -L"$(CUDA_INSTALL_DIR)/targets/$(CUDA_TRIPLE)/lib"
+INCPATHS += -I"$(TRT_INC_DIR)"
+LIBPATHS += -L"$(TRT_LIB_DIR)"
+
+CU_LIBPATHS ::= $(LIBPATHS)
+
+INCPATHS += $(EXTRA_INCPATHS)
+
+.SUFFIXES:
+vpath %.h $(EXTRA_DIRECTORIES)
+vpath %.cpp $(EXTRA_DIRECTORIES)
+
+COMMON_FLAGS += -Wall -std=c++14
+COMMON_FLAGS += $(INCPATHS)
+
+COMMON_LD_FLAGS += $(LIBPATHS) -L$(OUTDIR) -Wl,-rpath-link="$(TRT_LIB_DIR)" $(STUBS_DIR)
+
+OBJDIR = $(call concat,$(OUTDIR),/chobj)
+DOBJDIR = $(call concat,$(OUTDIR),/dchobj)
+
+COMMON_LIBS += $(CUDART_LIB)
+COMMON_LIBS += -lrt -ldl -lpthread
+
+CFLAGS = $(COMMON_FLAGS) -fPIC
+CFLAGSD = $(COMMON_FLAGS) -fPIC -g
+LFLAGS = $(COMMON_LD_FLAGS) -shared
+LFLAGSD = $(COMMON_LD_FLAGS) -shared
+
+OBJS    =$(patsubst %.cpp, $(OBJDIR)/%.o, $(wildcard $(SRC) $(EXTRA_FILE)))
+DOBJS   =$(patsubst %.cpp, $(DOBJDIR)/%.o, $(wildcard $(SRC) $(EXTRA_FILE)))
+CUOBJS  =$(patsubst %.cu, $(OBJDIR)/%.o, $(wildcard $(CU_FILES)))
+CUDOBJS =$(patsubst %.cu, $(DOBJDIR)/%.o, $(wildcard $(CU_FILES)))
+
+ifeq ($(TARGET),$(filter $(TARGET),aarch64))
+  ifeq ($(CUDA), cuda-11.4)
+    GENCODES = -gencode arch=compute_87,code=sm_87
+  else
+    $(error CUDA version $(CUDA) is not supported, please use cuda-11.4 instead.)
+  endif
+else
+  GENCODES = -arch=sm_70 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86
+endif
+
+# Always use C+=14 for nvcc
+COMMON_CUFLAGS := -std=c++14 -Xcompiler=-fPIC
+
+CUFLAGS = $(COMMON_CUFLAGS)
+CUFLAGS += -Xcompiler=-Wno-deprecated-declarations
+CUFLAGS += -Xcompiler=-O2,-fno-aggressive-loop-optimizations
+CUFLAGS += $(GENCODES) -DNDEBUG
+CUFLAGSD = $(CUFLAGS) -g
+
+all: debug release
+
+release : $(OUTDIR)/$(TARGET_RELEASE)
+
+debug   : $(OUTDIR)/$(TARGET_DEBUG)
+
+$(OUTDIR)/$(TARGET_RELEASE) : $(OBJS) $(CUOBJS)
+	$(call make-dir,$(dir $@))
+	$(ECHO) Linking: $@
+	$(AT)$(CC) -o $@ $^ $(LFLAGS) -Wl,--start-group $(NVINFER_LIB) $(COMMON_LIBS) -Wl,--end-group
+
+$(OUTDIR)/$(TARGET_DEBUG) : $(DOBJS) $(CUDOBJS)
+	$(call make-dir,$(dir $@))
+	$(ECHO) Linking: $@
+	$(AT)$(CC) -o $@ $^ $(LFLAGSD) -Wl,--start-group $(NVINFER_LIB) $(COMMON_LIBS) -Wl,--end-group
+
+$(OBJDIR)/%.o: %.cpp
+	$(call make-dir,$(dir $@))
+	$(ECHO) Compiling release: $<
+	$(AT)$(CC) -o $@ -c $< $(CFLAGS)
+
+$(DOBJDIR)/%.o: %.cpp
+	$(call make-dir,$(dir $@))
+	$(ECHO) Compiling debug: $<
+	$(AT)$(CC) -o $@ -c $< $(CFLAGSD)
+
+$(OBJDIR)/%.o: %.cu
+	$(call make-dir,$(dir $@))
+	$(ECHO) Compiling CUDA release: $<
+	$(AT)$(CUCC) $(CUFLAGS) $(INCPATHS) $(CU_LIBPATH) -c -o $@ $<
+
+$(DOBJDIR)/%.o: %.cu
+	$(call make-dir,$(dir $@))
+	$(ECHO) Compiling CUDA debug: $<
+	$(AT)$(CUCC) $(CUFLAGSD) $(INCPATHS) $(CU_LIBPATH) -c -o $@ $<
+
+OBJDIRS = $(OBJDIR)/$(EXTRA_DIRECTORIES) $(OBJDIR) $(DOBJDIR) $(OUTDIR)/$(TARGET_RELEASE) $(OUTDIR)/$(TARGET_DEBUG)
+
+clean:
+	$(ECHO) Cleaning...
+	$(AT)-rm -rf $(OBJDIRS)
+
+ifneq "$(MAKECMDGOALS)" "clean"
+  -include $(OBJDIR)/*.d $(DOBJDIR)/*.d
+endif
diff --git a/TensorRT/README.md b/TensorRT/README.md
deleted file mode 100644
index b5e930e..0000000
--- a/TensorRT/README.md
+++ /dev/null
@@ -1,277 +0,0 @@
-# TensorRT Plugins
-
-## Plugins
-
-### Grid Sampler
-
-|      OP Name      |                          Attributes                          |        Inputs         |  Outputs  | FP32 Speed | FP16 Speed | INT8 Speed | Half Type |     Tensor Format     | Test Device |
-| :---------------: | :----------------------------------------------------------: | :-------------------: | :-------: | :--------: | :--------: | :--------: | :-------: | :-------------------: | :---------: |
-| GridSampler2DTRT  | interpolation_mode: int<br />padding_mode: int<br />align_corners: int | input: T<br />grid: T | output: T |     x1     |    x2.0    |    x3.8    |  nv_half  |    kLinear, kCHW4     | RTX 2080Ti  |
-| GridSampler2DTRT2 | interpolation_mode: int<br />padding_mode: int<br />align_corners: int | input: T<br />grid: T | output: T |     x1     |    x3.1    |    x3.8    | nv_half2  | kLinear, kCHW2, kCHW4 | RTX 2080Ti  |
-| GridSampler3DTRT  | interpolation_mode: int<br />padding_mode: int<br />align_corners: int | input: T<br />grid: T | output: T |     x1     |    x1.3    |     -      |  nv_half  |        kLinear        | RTX 2080Ti  |
-| GridSampler3DTRT2 | interpolation_mode: int<br />padding_mode: int<br />align_corners: int | input: T<br />grid: T | output: T |     x1     |    x2.2    |     -      | nv_half2  |        kLinear        | RTX 2080Ti  |
-
-#### Inputs
-
-* input: T[float/half/half2/int8]
-
-  Tensor shape: `[N, C, H_in, W_in]` (4D case) or `[N, C, D_in, H_in, W_in]` (5D case)
-
-* grid: T[float/half/half2/int8]
-
-  Tensor shape: `[N, 2, H_out, W_out]` (4D case) or `[N, 3, D_out, H_out, W_out]` (5D case)
-
-  `grid` specifies the sampling pixel locations normalized by the `input` spatial dimensions. Therefore, it should have most values in the range of ``[-10, 10]``. For example, values ``x = -10, y = -10`` is the left-top pixel of `input`, and values  ``x = 10, y = 10`` is the right-bottom pixel of `input`.
-
-#### Attributes
-
-* interpolation_mode: int
-
-  Interpolation mode to calculate output values. (0: `bilinear` , 1: `nearest`, 2: `bicubic`) 
-
-  Note:  `bicubic` supports only 4-D input.
-
-* padding_mode: int
-
-  Padding mode for outside grid values. (0: `zeros`, 1: `border`, 2: `reflection`)
-
-* align_corners: int
-
-  If `align_corners=1`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels. If `align_corners=0`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.
-
-#### Outputs
-
-* output: T[float/half/half2/int8]
-
-  Tensor shape: `[N, C, H_out, W_out]` (4D case) or `[N, C, D_out, H_out, W_out]` (5D case)
-
-### Multi-scale Deformable Attention
-
-|           OP Name            | Attributes |                            Inputs                            |  Outputs  | FP32 Speed | FP16 Speed | INT8/FP16 Speed | Half Type | Tensor Format | Test Device |
-| :--------------------------: | :--------: | :----------------------------------------------------------: | :-------: | :--------: | :--------: | :-------------: | :-------: | :-----------: | :---------: |
-| MultiScaleDeformableAttnTRT  |     -      | value: T<br />value_spatial_shapes: T<br />sampling_locations: T<br />attention_weights: T | output: T |     x1     |    x1.3    |      x3.2       |  nv_half  |    kLinear    | RTX 2080Ti  |
-| MultiScaleDeformableAttnTRT2 |     -      | value: T<br />value_spatial_shapes: T<br />value_level_start_index: T<br />sampling_locations: T<br />attention_weights: T | output: T |     x1     |    x2.0    |      x2.7       | nv_half2  |    kLinear    | RTX 2080Ti  |
-
-#### Inputs
-
-* value: T[float/half/half2/int8]
-
-  Tensor shape: `[N, num_keys, mum_heads, channel]` 
-
-* value_spatial_shapes: T[int32]
-
-  Spatial shape of each feature map, has shape `[num_levels, 2]`, last dimension 2 represent (h, w)
-
-* reference_points: T[float/half2]
-
-  The reference points.
-
-  Tensor shape: `[N, num_queries, 1, points_per_group * 2]` 
-
-* sampling_offsets: T[float/half/half2/int8]
-
-  The offset of sampling points.
-
-  Tensor shape: `[N, num_queries, num_heads, num_levels * num_points * 2]` 
-
-* attention_weights: T[float/half/int8]
-
-  The weight of sampling points used when calculate the attention (before softmax), has shape` [N ,num_queries, num_heads, num_levels * num_points]`.
-
-#### Attributes
-
-​	-
-
-#### Outputs
-
-* output: T[float/half/int8]
-
-  Tensor shape: `[N, num_queries, mum_heads, channel]`
-
-### Modulated Deformable Conv2d
-
-|            OP Name            |                          Attributes                          |                            Inputs                            |  Outputs  | FP32 Speed | FP16 Speed | INT8/FP16 Speed | Half Type |     Tensor Format     | Test Device |
-| :---------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :-------: | :--------: | :--------: | :-------------: | :-------: | :-------------------: | :---------: |
-| ModulatedDeformableConv2dTRT  | stride: int[2]<br />padding: int[2]<br />dilation: int[2]<br />groups: int<br />deform_groups: int | input: T<br />offset: T<br />mask: T<br />weight: T<br />bias: T (optional) | output: T |     x1     |    x2.9    |      x3.7       |  nv_half  |    kLinear, kCHW4     | RTX 2080Ti  |
-| ModulatedDeformableConv2dTRT2 | stride: int[2]<br />padding: int[2]<br />dilation: int[2]<br />groups: int<br />deform_groups: int | input: T<br />offset: T<br />mask: T<br />weight: T<br />bias: T (optional) | output: T |     x1     |    x3.5    |      x3.7       | nv_half2  | kLinear, kCHW2, kCHW4 | RTX 2080Ti  |
-
-#### Inputs
-
-* input: T[float/half/half2/int8]
-
-  Tensor shape: `[N, C_in, H_in, W_in]` 
-
-* offset: T[float/half/half2/int8]
-
-  Tensor shape: `[N, deform_groups*K_h*K_w*2, H_out, W_out]`
-
-* mask: T[float/half/half2/int8]
-
-  Tensor shape: `[N, deform_groups*K_h*K_w, H_out, W_out]`
-
-* weight: T[float/half/half2/int8]
-
-  Tensor shape: `[C_out, C_in/groups, K_h, K_w]`
-
-* bias: T[float/half/half2] (optional)
-
-  Tensor shape: `[C_out]`
-
-#### Attributes
-
-* stride: int[2]
-
-  Same as torch.nn.Conv2d.
-
-* padding: int[2]
-
-  Same as torch.nn.Conv2d.
-
-* dilation: int[2]
-
-  Same as torch.nn.Conv2d.
-
-* groups: int
-
-  Same as torch.nn.Conv2d.
-
-* deform_groups: int
-
-  Deformable conv2d groups.
-
-#### Outputs
-
-* output: T[float/half/half2/int8]
-
-  Tensor shape: `[N, C_out, H_out, W_out]`
-
-**NOTE: Values (C_in / groups) and (C_in / deform_groups) should be even numbers.**
-
-### Rotate
-
-|  OP Name   |     Attributes     |               Inputs                |  Outputs  | FP32 Speed | FP16 Speed | INT8/FP16 Speed | Half Type |     Tensor Format     | Test Device |
-| :--------: | :----------------: | :---------------------------------: | :-------: | :--------: | :--------: | :-------------: | :-------: | :-------------------: | :---------: |
-| RotateTRT  | interpolation: int | img: T<br />angle: T<br />center: T | output: T |     x1     |    X1.8    |      X4.4       |  nv_half  |    kLinear, kCHW4     | RTX 2080Ti  |
-| RotateTRT2 | interpolation: int | img: T<br />angle: T<br />center: T | output: T |     x1     |    x2.2    |      x4.4       | nv_half2  | kLinear, kCHW2, kCHW4 | RTX 2080Ti  |
-
-#### Inputs
-
-* img: T[float/half/half2/int8]
-
-  Tensor shape: `[C, H, W]` 
-
-* angle: T[float/half/half2]
-
-  Tensor shape: `[1]`
-
-* center: T[float/half/half2]
-
-  Tensor shape: `[2]`
-
-
-#### Attributes
-
-* interpolation: int
-
-  Interpolation mode to calculate output values. (0: `bilinear` , 1: `nearest`) 
-
-#### Outputs
-
-* output: T[float/half/half2/int8]
-
-  Tensor shape: `[C, H, W]`
-
-### Inverse
-
-|  OP Name   | Attributes |     Inputs      |     Outputs      | Tensor Format | Test Device |
-| :--------: | :--------: | :-------------: | :--------------: | :-----------: | :---------: |
-| InverseTRT |     -      | input: T[float] | output: T[float] |    kLinear    | RTX 2080Ti  |
-
-#### Inputs
-
-* input: T[float]
-
-  Tensor shape: `[B, C, H, W]` 
-
-#### Outputs
-
-* output: T[float]
-
-  Tensor shape: `[B, C, H, W]`
-
-### BEV Pool
-
-|    OP Name    |             Attributes              |                            Inputs                            |  Outputs  | FP32 Speed | FP16 Speed | INT8 Speed | Half Type | Tensor Format | Test Device |
-| :-----------: | :---------------------------------: | :----------------------------------------------------------: | :-------: | :--------: | :--------: | :--------: | :-------: | :-----------: | :---------: |
-| BEVPoolV2TRT  | out_height: int<br />out_width: int | depth: T<br />feat: T<br />ranks_depth: T<br />ranks_feat: T<br /> ranks_bev: T<br /> interval_starts: T<br />interval_lengths: T | output: T |     x1     |    X1.1    |    X2.1    |  nv_half  |    kLinear    | RTX 2080Ti  |
-| BEVPoolV2TRT2 | out_height: int<br />out_width: int | depth: T<br />feat: T<br />ranks_depth: T<br />ranks_feat: T<br /> ranks_bev: T<br /> interval_starts: T<br />interval_lengths: T | output: T |     x1     |    x1.4    |    X2.1    | nv_half2  |    kLinear    | RTX 2080Ti  |
-
-#### Inputs
-
-* depth: T[float/half/half2/int8]
-
-  Tensor shape: `[Cam, D, H, W]` 
-
-* feat: T[float/half/half2/int8]
-
-  Tensor shape: `[Cam, H, W, C]`
-
-* ranks_depth: T[int32]
-
-* ranks_feat: T[int32]
-
-* ranks_bev: T[int32]
-
-* interval_starts: T[int32]
-
-* interval_lengths: T[int32]
-
-
-#### Attributes
-
-* out_height: int
-
-  BEV feature height
-
-* out_width: int
-
-  BEV feature width
-
-#### Outputs
-
-* output: T[float/half/half2/int8]
-
-  Tensor shape: `[1, out_height, out_width, C]`
-
-### Multi-Head Attention
-
-| OP Name |               Inputs               |  Outputs  | FP32 Speed NHMA | FP16 Speed NHMA | FP32 Speed FHMA | FP16 Speed FHMA | INT8 Speed FHMA | Half Type | Test Device |
-| :-----: | :--------------------------------: | :-------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------: | :---------: |
-| QKVTRT  | query: T<br />key: T<br />value: T | output: T |       x1        |      X2.0       |      x4.6       |      x6.1       |      x8.2       |  nv_half  | RTX 2080Ti  |
-| QKVTRT2 | query: T<br />key: T<br />value: T | output: T |       x1        |      X2.1       |      x4.6       |      x6.3       |      x8.2       | nv_half2  | RTX 2080Ti  |
-
-#### Inputs
-
-* query: T[float/half/half2/int8]
-
-  Tensor shape: `[batch, q_len, channel]` 
-
-* key: T[float/half/half2/int8]
-
-  Tensor shape: `[batch, kv_len, channel]`
-
-* value: T[float/half/half2/int8]
-
-  Tensor shape: `[batch, kv_len, channel]`
-
-#### Attributes
-
-​	-
-
-#### Outputs
-
-* output: T[float/half/half2/int8]
-
-  Tensor shape: `[batch, q_len, channel]` 
-
-**NOTE: If `q_len` and `kv_len` are both multiples of 64, the plugin will run with Flash Multi-Head Attention (FMHA), else Naive Multi-Head Attention (NMHA).**
diff --git a/TensorRT/build/.gitignore b/TensorRT/build/.gitignore
deleted file mode 100644
index d6b7ef3..0000000
--- a/TensorRT/build/.gitignore
+++ /dev/null
@@ -1,2 +0,0 @@
-*
-!.gitignore
diff --git a/TensorRT/cudaComputeVersion.cu b/TensorRT/cudaComputeVersion.cu
deleted file mode 100644
index 1ee83ba..0000000
--- a/TensorRT/cudaComputeVersion.cu
+++ /dev/null
@@ -1,9 +0,0 @@
-#include <cstdio>
-
-int main() {
-  cudaDeviceProp prop;
-  cudaGetDeviceProperties(&prop, 0);
-  int v = prop.major * 10 + prop.minor;
-  printf("%d", v);
-  return 0;
-}
diff --git a/TensorRT/plugin/bev_pool_v2/bevPoolKernel.h b/TensorRT/include/bevPoolKernel.h
similarity index 94%
rename from TensorRT/plugin/bev_pool_v2/bevPoolKernel.h
rename to TensorRT/include/bevPoolKernel.h
index e23eb81..f8f57c7 100644
--- a/TensorRT/plugin/bev_pool_v2/bevPoolKernel.h
+++ b/TensorRT/include/bevPoolKernel.h
@@ -1,6 +1,3 @@
-//
-// Created by Derry Lin on 2023/5/11.
-//
 
 #ifndef TENSORRT_OPS_BEVPOOLKERNEL_H
 #define TENSORRT_OPS_BEVPOOLKERNEL_H
@@ -30,4 +27,4 @@ void bev_pool_v2_int8(int c, int n_intervals, int num_points,
                       const int *interval_lengths, int8_4 *out,
                       const float &scale_o, cudaStream_t stream);
 
-#endif // TENSORRT_OPS_BEVPOOLKERNEL_H
+#endif // TENSORRT_OPS_BEVPOOLKERNEL_H
\ No newline at end of file
diff --git a/TensorRT/plugin/bev_pool_v2/bevPoolPlugin.h b/TensorRT/include/bevPoolPlugin.h
similarity index 99%
rename from TensorRT/plugin/bev_pool_v2/bevPoolPlugin.h
rename to TensorRT/include/bevPoolPlugin.h
index 75663cd..16bdb93 100644
--- a/TensorRT/plugin/bev_pool_v2/bevPoolPlugin.h
+++ b/TensorRT/include/bevPoolPlugin.h
@@ -136,4 +136,4 @@ private:
 
 } // namespace trt_plugin
 
-#endif // TENSORRT_OPS_BEVPOOLPLUGIN_H
+#endif // TENSORRT_OPS_BEVPOOLPLUGIN_H
\ No newline at end of file
diff --git a/TensorRT/common/checkMacrosPlugin.cpp b/TensorRT/include/checkMacrosPlugin.cpp
similarity index 100%
rename from TensorRT/common/checkMacrosPlugin.cpp
rename to TensorRT/include/checkMacrosPlugin.cpp
diff --git a/TensorRT/common/checkMacrosPlugin.h b/TensorRT/include/checkMacrosPlugin.h
similarity index 100%
rename from TensorRT/common/checkMacrosPlugin.h
rename to TensorRT/include/checkMacrosPlugin.h
diff --git a/TensorRT/common/cuda_helper.cu b/TensorRT/include/cuda_helper.cu
similarity index 100%
rename from TensorRT/common/cuda_helper.cu
rename to TensorRT/include/cuda_helper.cu
diff --git a/TensorRT/common/cuda_helper.h b/TensorRT/include/cuda_helper.h
similarity index 100%
rename from TensorRT/common/cuda_helper.h
rename to TensorRT/include/cuda_helper.h
diff --git a/TensorRT/common/cuda_int8.h b/TensorRT/include/cuda_int8.h
similarity index 100%
rename from TensorRT/common/cuda_int8.h
rename to TensorRT/include/cuda_int8.h
diff --git a/TensorRT/plugin/grid_sampler/gridSamplerKernel.h b/TensorRT/include/gridSamplerKernel.h
similarity index 100%
rename from TensorRT/plugin/grid_sampler/gridSamplerKernel.h
rename to TensorRT/include/gridSamplerKernel.h
diff --git a/TensorRT/plugin/grid_sampler/gridSamplerPlugin.h b/TensorRT/include/gridSamplerPlugin.h
similarity index 100%
rename from TensorRT/plugin/grid_sampler/gridSamplerPlugin.h
rename to TensorRT/include/gridSamplerPlugin.h
diff --git a/TensorRT/common/helper.h b/TensorRT/include/helper.h
similarity index 100%
rename from TensorRT/common/helper.h
rename to TensorRT/include/helper.h
diff --git a/TensorRT/plugin/inverse/inverseKernel.h b/TensorRT/include/inverseKernel.h
similarity index 100%
rename from TensorRT/plugin/inverse/inverseKernel.h
rename to TensorRT/include/inverseKernel.h
diff --git a/TensorRT/plugin/inverse/inversePlugin.h b/TensorRT/include/inversePlugin.h
similarity index 100%
rename from TensorRT/plugin/inverse/inversePlugin.h
rename to TensorRT/include/inversePlugin.h
diff --git a/TensorRT/plugin/modulated_deformable_conv2d/modulatedDeformableConv2dKernel.h b/TensorRT/include/modulatedDeformableConv2dKernel.h
similarity index 100%
rename from TensorRT/plugin/modulated_deformable_conv2d/modulatedDeformableConv2dKernel.h
rename to TensorRT/include/modulatedDeformableConv2dKernel.h
diff --git a/TensorRT/plugin/modulated_deformable_conv2d/modulatedDeformableConv2dPlugin.h b/TensorRT/include/modulatedDeformableConv2dPlugin.h
similarity index 100%
rename from TensorRT/plugin/modulated_deformable_conv2d/modulatedDeformableConv2dPlugin.h
rename to TensorRT/include/modulatedDeformableConv2dPlugin.h
diff --git a/TensorRT/plugin/multi_head_attn/multiHeadAttnKernel.h b/TensorRT/include/multiHeadAttnKernel.h
similarity index 100%
rename from TensorRT/plugin/multi_head_attn/multiHeadAttnKernel.h
rename to TensorRT/include/multiHeadAttnKernel.h
diff --git a/TensorRT/plugin/multi_head_attn/multiHeadAttnPlugin.h b/TensorRT/include/multiHeadAttnPlugin.h
similarity index 100%
rename from TensorRT/plugin/multi_head_attn/multiHeadAttnPlugin.h
rename to TensorRT/include/multiHeadAttnPlugin.h
diff --git a/TensorRT/plugin/multi_head_attn/multiHeadFlashAttnKernel.h b/TensorRT/include/multiHeadFlashAttnKernel.h
similarity index 100%
rename from TensorRT/plugin/multi_head_attn/multiHeadFlashAttnKernel.h
rename to TensorRT/include/multiHeadFlashAttnKernel.h
diff --git a/TensorRT/include/multiScaleDeformableAttnKernel.h b/TensorRT/include/multiScaleDeformableAttnKernel.h
new file mode 100644
index 0000000..6332669
--- /dev/null
+++ b/TensorRT/include/multiScaleDeformableAttnKernel.h
@@ -0,0 +1,30 @@
+//
+// Created by Derry Lin on 2022/11/7.
+//
+
+#ifndef TENSORRT_OPS_MULTISCALEDEFORMABLEATTNKERNEL_H
+#define TENSORRT_OPS_MULTISCALEDEFORMABLEATTNKERNEL_H
+
+#include "cuda_int8.h"
+#include <cuda_fp16.h>
+#include <cuda_runtime.h>
+
+template <typename T>
+void ms_deformable_im2col_cuda(
+    const T *data_value, 
+    const int *data_spatial_shapes,
+    const int *data_level_start_index, 
+    const T *data_sampling_loc,
+    const T *data_attn_weight, 
+    const int batch_size, 
+    const int spatial_size,
+    const int num_heads, 
+    const int channels, 
+    const int num_levels,
+    const int num_query, 
+    const int num_point,
+    T *data_col, 
+    cudaStream_t stream);
+    
+
+#endif // TENSORRT_OPS_MULTISCALEDEFORMABLEATTNKERNEL_H
\ No newline at end of file
diff --git a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnPlugin.h b/TensorRT/include/multiScaleDeformableAttnPlugin.h
similarity index 98%
rename from TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnPlugin.h
rename to TensorRT/include/multiScaleDeformableAttnPlugin.h
index c93610b..8794e32 100644
--- a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnPlugin.h
+++ b/TensorRT/include/multiScaleDeformableAttnPlugin.h
@@ -132,4 +132,4 @@ private:
 };
 } // namespace trt_plugin
 
-#endif // TENSORRT_OPS_MULTISCALEDEFORMABLEATTN_H
+#endif // TENSORRT_OPS_MULTISCALEDEFORMABLEATTN_H
\ No newline at end of file
diff --git a/TensorRT/plugin/rotate/rotateKernel.h b/TensorRT/include/rotateKernel.h
similarity index 100%
rename from TensorRT/plugin/rotate/rotateKernel.h
rename to TensorRT/include/rotateKernel.h
diff --git a/TensorRT/plugin/rotate/rotatePlugin.h b/TensorRT/include/rotatePlugin.h
similarity index 100%
rename from TensorRT/plugin/rotate/rotatePlugin.h
rename to TensorRT/include/rotatePlugin.h
diff --git a/TensorRT/common/serialize.h b/TensorRT/include/serialize.h
similarity index 100%
rename from TensorRT/common/serialize.h
rename to TensorRT/include/serialize.h
diff --git a/TensorRT/install.sh b/TensorRT/install.sh
deleted file mode 100644
index 2f42916..0000000
--- a/TensorRT/install.sh
+++ /dev/null
@@ -1,4 +0,0 @@
-cd build
-cmake .. -DCMAKE_TENSORRT_PATH=/usr/local/TensorRT
-make -j$(nproc)
-make install
diff --git a/TensorRT/plugin/bev_pool_v2/bevPoolKernel.cu b/TensorRT/kernels/bevPoolKernel.cu
similarity index 100%
rename from TensorRT/plugin/bev_pool_v2/bevPoolKernel.cu
rename to TensorRT/kernels/bevPoolKernel.cu
diff --git a/TensorRT/plugin/grid_sampler/gridSamplerKernel.cu b/TensorRT/kernels/gridSamplerKernel.cu
similarity index 100%
rename from TensorRT/plugin/grid_sampler/gridSamplerKernel.cu
rename to TensorRT/kernels/gridSamplerKernel.cu
diff --git a/TensorRT/plugin/inverse/inverseKernel.cu b/TensorRT/kernels/inverseKernel.cu
similarity index 100%
rename from TensorRT/plugin/inverse/inverseKernel.cu
rename to TensorRT/kernels/inverseKernel.cu
diff --git a/TensorRT/plugin/modulated_deformable_conv2d/modulatedDeformableConv2dKernel.cu b/TensorRT/kernels/modulatedDeformableConv2dKernel.cu
similarity index 100%
rename from TensorRT/plugin/modulated_deformable_conv2d/modulatedDeformableConv2dKernel.cu
rename to TensorRT/kernels/modulatedDeformableConv2dKernel.cu
diff --git a/TensorRT/plugin/multi_head_attn/multiHeadAttnKernel.cu b/TensorRT/kernels/multiHeadAttnKernel.cu
similarity index 100%
rename from TensorRT/plugin/multi_head_attn/multiHeadAttnKernel.cu
rename to TensorRT/kernels/multiHeadAttnKernel.cu
diff --git a/TensorRT/plugin/multi_head_attn/multiHeadFlashAttnKernel.cu b/TensorRT/kernels/multiHeadFlashAttnKernel.cu
similarity index 100%
rename from TensorRT/plugin/multi_head_attn/multiHeadFlashAttnKernel.cu
rename to TensorRT/kernels/multiHeadFlashAttnKernel.cu
diff --git a/TensorRT/kernels/multiScaleDeformableAttnKernel.cu b/TensorRT/kernels/multiScaleDeformableAttnKernel.cu
new file mode 100644
index 0000000..8b5fbbe
--- /dev/null
+++ b/TensorRT/kernels/multiScaleDeformableAttnKernel.cu
@@ -0,0 +1,468 @@
+//
+// Created by Derry Lin on 2022/11/7.
+//
+
+#include <cstdio>
+#include <cuda_fp16.h>
+
+#include <algorithm>
+#include <cmath>
+#include <vector>
+
+#include "cuda_helper.h"
+#include "helper.h"
+#include "multiScaleDeformableAttnKernel.h"
+#include <cstdio>
+#include <cuda/std/limits>
+#include <unistd.h>
+
+template <typename T>
+__forceinline__ __device__ T hmax(const T &a, const T &b) {
+  return max(a, b);
+}
+
+#if __CUDA_ARCH__ >= 800
+template <>
+__forceinline__ __device__ __half hmax(const __half &a, const __half &b) {
+  return __hmax(a, b);
+}
+template <>
+__forceinline__ __device__ __half2 hmax(const __half2 &a, const __half2 &b) {
+  return __hmax2(a, b);
+}
+#else
+template <>
+__forceinline__ __device__ __half hmax(const __half &a, const __half &b) {
+  return __hgt(a, b) ? a : b;
+}
+template <>
+__forceinline__ __device__ __half2 hmax(const __half2 &a, const __half2 &b) {
+  return __hfma2(__hgt2(a, b), a, __hmul2(__hle2(a, b), b));
+}
+#endif
+
+template <typename T> __forceinline__ __device__ T sign_05(T x) {
+  if (x > 0) {
+    return 0.5f;
+  }
+  return -0.5f;
+}
+
+template <typename T> __forceinline__ __device__ int8_t T2int8(T a) {
+  a = a > 127 ? 127 : a;
+  a = a < -128 ? -128 : a;
+  return int8_t(a + sign_05<T>(a));
+}
+
+template <> __forceinline__ __device__ int8_t T2int8(__half a) {
+  short temp = __half2short_rn(a);
+  temp = temp > static_cast<short>(127) ? static_cast<short>(127) : temp;
+  temp = temp < static_cast<short>(-128) ? static_cast<short>(-128) : temp;
+  return static_cast<int8_t>(temp);
+}
+
+template <typename T> __forceinline__ __device__ uint8_t T2uint8(T a) {
+  a = a > 255 ? 255 : a;
+  a = a < 0 ? 0 : a;
+  return uint8_t(a + 0.5);
+}
+
+template <> __forceinline__ __device__ uint8_t T2uint8(__half a) {
+  unsigned short temp = __half2ushort_rn(a);
+  temp = temp > static_cast<short>(255) ? static_cast<short>(255) : temp;
+  return static_cast<uint8_t>(temp);
+}
+
+__forceinline__ __device__ int8_t half2int8(const __half &hval,
+                                            const float &scale) {
+  __half ret = __hdiv(hval, __float2half(scale));
+  return T2int8<__half>(ret);
+}
+
+__forceinline__ __device__ uint8_t half2uint8(const __half &hval,
+                                              const float &scale) {
+  __half ret = __hdiv(hval, __float2half(scale));
+  return T2uint8<__half>(ret);
+}
+
+__forceinline__ __device__ void qmulf(const int32_4 &a, int8_4 &c,
+                                      const float &b) {
+  c.x = T2int8<float>(a.x * b);
+  c.y = T2int8<float>(a.y * b);
+  c.z = T2int8<float>(a.z * b);
+  c.w = T2int8<float>(a.w * b);
+}
+
+__forceinline__ __device__ void qmulh(const int32_4 &a, int8_4 &c,
+                                      const __half &b) {
+  c.x = T2int8<__half>(__int2half_rn(a.x) * b);
+  c.y = T2int8<__half>(__int2half_rn(a.y) * b);
+  c.z = T2int8<__half>(__int2half_rn(a.z) * b);
+  c.w = T2int8<__half>(__int2half_rn(a.w) * b);
+}
+
+__forceinline__ __device__ void dp4a(const int32_t *a, const int32_t *b,
+                                     int32_t &c) {
+#if __CUDA_ARCH__ >= 610
+  asm("dp4a.s32.s32 %0, %1, %2, %3;" : "+r"(c) : "r"(*a), "r"(*b), "r"(c));
+#else
+  auto ap = (int8_4 *)a, bp = (int8_4 *)b;
+
+  c += ap->x * bp->x;
+  c += ap->y * bp->y;
+  c += ap->z * bp->z;
+  c += ap->w * bp->w;
+#endif
+}
+
+__forceinline__ __device__ void dp4a(const int32_t *a, const uint32_t *b,
+                                     int32_t &c) {
+#if __CUDA_ARCH__ >= 610
+  asm("dp4a.s32.u32 %0, %1, %2, %3;" : "+r"(c) : "r"(*a), "r"(*b), "r"(c));
+#else
+  auto ap = (int8_4 *)a;
+  auto bp = (uint8_4 *)b;
+
+  c += ap->x * bp->x;
+  c += ap->y * bp->y;
+  c += ap->z * bp->z;
+  c += ap->w * bp->w;
+#endif
+}
+
+template <typename scalar_t>
+__device__ scalar_t ms_deform_attn_im2col_bilinear(
+    const scalar_t *&bottom_data, const int &height, const int &width,
+    const int &nheads, const int &channels, const scalar_t &h,
+    const scalar_t &w, const int &m, const int &c) {
+  const int h_low = floor(h);
+  const int w_low = floor(w);
+  const int h_high = h_low + 1;
+  const int w_high = w_low + 1;
+
+  const scalar_t lh = h - h_low;
+  const scalar_t lw = w - w_low;
+  const scalar_t hh = 1 - lh, hw = 1 - lw;
+
+  const int w_stride = nheads * channels;
+  const int h_stride = width * w_stride;
+  const int h_low_ptr_offset = h_low * h_stride;
+  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;
+  const int w_low_ptr_offset = w_low * w_stride;
+  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;
+  const int base_ptr = m * channels + c;
+
+  scalar_t v1 = 0;
+  if (h_low >= 0 && w_low >= 0) {
+    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;
+    v1 = bottom_data[ptr1];
+  }
+  scalar_t v2 = 0;
+  if (h_low >= 0 && w_high <= width - 1) {
+    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;
+    v2 = bottom_data[ptr2];
+  }
+  scalar_t v3 = 0;
+  if (h_high <= height - 1 && w_low >= 0) {
+    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;
+    v3 = bottom_data[ptr3];
+  }
+  scalar_t v4 = 0;
+  if (h_high <= height - 1 && w_high <= width - 1) {
+    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;
+    v4 = bottom_data[ptr4];
+  }
+
+  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;
+  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);
+  return val;
+}
+
+template <>
+__device__ __half ms_deform_attn_im2col_bilinear(
+    const __half *&bottom_data, const int &height, const int &width,
+    const int &nheads, const int &channels, const __half &h, 
+    const __half &w, const int &m, const int &c) {
+  
+  const int h_low = __half2int_rn(h);
+  const int w_low = __half2int_rn(w);
+  const int h_high = h_low + 1;
+  const int w_high = w_low + 1;
+
+  const __half lh = __hsub(h, __int2half_rn(h_low));
+  const __half lw = __hsub(w, __int2half_rn(w_low));
+  const __half hh = __hsub(__float2half(1.f), lh);
+  const __half hw = __hsub(__float2half(1.f), lw);
+
+  const int w_stride = nheads * channels;
+  const int h_stride = width * w_stride;
+  const int h_low_ptr_offset = h_low * h_stride;
+  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;
+  const int w_low_ptr_offset = w_low * w_stride;
+  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;
+  const int base_ptr = m * channels + c;
+
+  __half v1 = 0;
+  if (h_low >= 0 && w_low >= 0) {
+    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;
+    v1 = bottom_data[ptr1];
+  }
+  __half v2 = 0;
+  if (h_low >= 0 && w_high <= width - 1) {
+    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;
+    v2 = bottom_data[ptr2];
+  }
+  __half v3 = 0;
+  if (h_high <= height - 1 && w_low >= 0) {
+    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;
+    v3 = bottom_data[ptr3];
+  }
+  __half v4 = 0;
+  if (h_high <= height - 1 && w_high <= width - 1) {
+    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;
+    v4 = bottom_data[ptr4];
+  }
+
+  const __half w1 = __hmul(hh, hw);
+  const __half w2 = __hmul(hh, lw);
+  const __half w3 = __hmul(lh, hw);
+  const __half w4 = __hmul(lh, lw);
+
+  return __hadd(__hadd(__hmul(w1, v1), __hmul(w2, v2)),
+                __hadd(__hmul(w3, v3), __hmul(w4, v4)));
+}
+
+
+
+
+template <typename scalar_t>
+__global__ void ms_deformable_im2col_gpu_kernel(
+    const int n,
+    const scalar_t *data_value, 
+    const int *data_spatial_shapes,
+    const int *data_level_start_index, 
+    const scalar_t *data_sampling_loc,
+    const scalar_t *data_attn_weight, 
+    const int batch_size,
+    const int spatial_size, 
+    const int num_heads, 
+    const int channels,
+    const int num_levels, 
+    const int num_query, 
+    const int num_point,
+    scalar_t *data_col) {
+  CUDA_1D_KERNEL_LOOP(index, n) {
+    int _temp = index;
+    const int c_col = _temp % channels;
+    _temp /= channels;
+    const int sampling_index = _temp;
+    const int m_col = _temp % num_heads;
+    _temp /= num_heads;
+    _temp /= num_query;
+    const int b_col = _temp;
+
+    scalar_t *data_col_ptr = data_col + index;
+    int data_weight_ptr = sampling_index * num_levels * num_point;
+    int data_loc_w_ptr = data_weight_ptr << 1;
+    const int qid_stride = num_heads * channels;
+    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;
+    scalar_t col = 0;
+    
+    for (int l_col = 0; l_col < num_levels; ++l_col) {
+      const int level_start_id = data_level_start_index[l_col];
+      const int spatial_h_ptr = l_col << 1;
+      const int spatial_h = data_spatial_shapes[spatial_h_ptr];
+      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];
+      
+      const scalar_t *data_value_ptr =
+          data_value + (data_value_ptr_init_offset + level_start_id * qid_stride);
+      for (int p_col = 0; p_col < num_point; ++p_col) {
+        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];
+        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];
+        const scalar_t weight = data_attn_weight[data_weight_ptr];
+
+        const scalar_t h_im = loc_h * spatial_h - 0.5;
+        const scalar_t w_im = loc_w * spatial_w - 0.5;
+
+        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w) {
+          col += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h,
+                                                spatial_w, num_heads, channels,
+                                                h_im, w_im, m_col, c_col) * weight;
+        }
+        data_weight_ptr += 1;
+        data_loc_w_ptr += 2;
+      }
+    }
+    *data_col_ptr = col;
+  }
+}
+
+template <>
+__global__ void ms_deformable_im2col_gpu_kernel(
+    const int n, 
+    const __half *data_value, 
+    const int *data_spatial_shapes,
+    const int *data_level_start_index, 
+    const __half *data_sampling_loc,
+    const __half *data_attn_weight, 
+    const int batch_size, 
+    const int spatial_size,
+    const int num_heads, 
+    const int channels, 
+    const int num_levels,
+    const int num_query, 
+    const int num_point, 
+    __half *data_col) {
+  CUDA_1D_KERNEL_LOOP(index, n) {
+    int _temp = index;
+    const int c_col = _temp % channels;
+    _temp /= channels;
+    const int sampling_index = _temp;
+    const int m_col = _temp % num_heads;
+    _temp /= num_heads;
+    _temp /= num_query;
+    const int b_col = _temp;
+
+    __half *data_col_ptr = data_col + index;
+    int data_weight_ptr = sampling_index * num_levels * num_point;
+    int data_loc_w_ptr = data_weight_ptr << 1;
+    const int qid_stride = num_heads * channels;
+    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;
+    __half col = 0;
+    
+    for (int l_col = 0; l_col < num_levels; ++l_col) {
+      const int level_start_id = data_level_start_index[l_col];
+      const int spatial_h_ptr = l_col << 1;
+      const int spatial_h = data_spatial_shapes[spatial_h_ptr];
+      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];
+      
+      const __half *data_value_ptr =
+          data_value + (data_value_ptr_init_offset + level_start_id * qid_stride);
+      for (int p_col = 0; p_col < num_point; ++p_col) {
+        const __half loc_w = data_sampling_loc[data_loc_w_ptr];
+        const __half loc_h = data_sampling_loc[data_loc_w_ptr + 1];
+        const __half weight = data_attn_weight[data_weight_ptr];
+
+        const __half h_im = __hsub(__hmul(loc_h, __int2half_rn(spatial_h)), __float2half(0.5f));
+        const __half w_im = __hsub(__hmul(loc_w, __int2half_rn(spatial_w)), __float2half(0.5f));
+
+        if (__hgt(h_im, __float2half(-1.f)) && __hgt(w_im, __float2half(-1.f)) &&
+            __hlt(h_im, __int2half_rn(spatial_h)) && __hlt(w_im, __int2half_rn(spatial_w))) {
+          // Pass data_value_ptr by reference
+          col += ms_deform_attn_im2col_bilinear(data_value_ptr,
+                                                   spatial_h, 
+                                                   spatial_w, 
+                                                   num_heads, 
+                                                   channels, 
+                                                   h_im, w_im, 
+                                                   m_col, c_col) * weight;
+        }
+        
+        data_weight_ptr += 1;
+        data_loc_w_ptr += 2;
+      }
+    }
+    *data_col_ptr = col;
+  }
+}
+
+
+
+template <typename scalar_t>
+void ms_deformable_im2col_cuda(const scalar_t *data_value,
+                               const int *data_spatial_shapes,
+                               const int *data_level_start_index,
+                               const scalar_t *data_sampling_loc,
+                               const scalar_t *data_attn_weight,
+                               const int batch_size, 
+                               const int spatial_size,
+                               const int num_heads,
+                               const int channels,
+                               const int num_levels, 
+                               const int num_query,
+                               const int num_point,
+                               scalar_t *data_col, 
+                               cudaStream_t stream) {
+  
+  const int num_kernels = batch_size * num_query * num_heads * channels;
+  const int num_threads = THREADS_PER_BLOCK;
+  // printf("%d", data_spatial_shapes);
+  cudaMemset((scalar_t *)data_col, 0, num_kernels * sizeof(scalar_t));
+  
+  ms_deformable_im2col_gpu_kernel<scalar_t>
+      <<<GET_BLOCKS(num_kernels), num_threads, 0, stream>>>(
+          num_kernels, data_value, data_spatial_shapes, 
+          data_level_start_index, data_sampling_loc, 
+          data_attn_weight, batch_size, 
+          spatial_size, num_heads, channels, 
+          num_levels, num_query, num_point,
+          data_col);
+  cudaError_t err = cudaGetLastError();
+  if (err != cudaSuccess) {
+    printf("error in ms_deformable_im2col_cuda: %s\n", cudaGetErrorString(err));
+  }
+}
+
+
+template <>
+void ms_deformable_im2col_cuda(
+    const __half *data_value, const int *data_spatial_shapes,
+    const int *data_level_start_index, const __half *data_sampling_loc,
+    const __half *data_attn_weight, const int batch_size,
+    const int spatial_size, const int num_heads, const int channels,
+    const int num_levels, const int num_query, const int num_point,
+    __half *data_col, cudaStream_t stream) {
+  
+  const int num_kernels = batch_size * num_query * num_heads * channels;
+  const int num_threads = THREADS_PER_BLOCK;
+  
+  // Initialize output memory
+  cudaMemset(data_col, 0, num_kernels * sizeof(__half));
+
+  ms_deformable_im2col_gpu_kernel<__half>
+      <<<GET_BLOCKS(num_kernels), num_threads, 0, stream>>>(
+          num_kernels, data_value, data_spatial_shapes, 
+          data_level_start_index, data_sampling_loc, 
+          data_attn_weight, batch_size, spatial_size, 
+          num_heads, channels, num_levels, num_query, num_point,
+          data_col);
+  
+  cudaError_t err = cudaGetLastError();
+  if (err != cudaSuccess) {
+    printf("error in ms_deformable_im2col_cuda: %s\n", cudaGetErrorString(err));
+  }
+}
+
+
+template void ms_deformable_im2col_cuda<float>(
+    const float *data_value, 
+    const int *data_spatial_shapes,
+    const int *data_level_start_index, 
+    const float *data_sampling_loc,
+    const float *data_attn_weight, 
+    const int batch_size, 
+    const int spatial_size,
+    const int num_heads, 
+    const int channels, 
+    const int num_levels,
+    const int num_query, 
+    const int num_point,
+    float *data_col, 
+    cudaStream_t stream);
+
+
+template void ms_deformable_im2col_cuda<__half>(
+    const __half *data_value, 
+    const int *data_spatial_shapes,
+    const int *data_level_start_index, 
+    const __half *data_sampling_loc,
+    const __half *data_attn_weight, 
+    const int batch_size,
+    const int spatial_size, 
+    const int num_heads, 
+    const int channels,
+    const int num_levels, 
+    const int num_query, 
+    const int num_point,
+    __half *data_col, 
+    cudaStream_t stream);
diff --git a/TensorRT/plugin/rotate/rotateKernel.cu b/TensorRT/kernels/rotateKernel.cu
similarity index 100%
rename from TensorRT/plugin/rotate/rotateKernel.cu
rename to TensorRT/kernels/rotateKernel.cu
diff --git a/TensorRT/lib/.gitignore b/TensorRT/lib/.gitignore
deleted file mode 100644
index d6b7ef3..0000000
--- a/TensorRT/lib/.gitignore
+++ /dev/null
@@ -1,2 +0,0 @@
-*
-!.gitignore
diff --git a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.cu b/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.cu
deleted file mode 100644
index e1a70ac..0000000
--- a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.cu
+++ /dev/null
@@ -1,1254 +0,0 @@
-//
-// Created by Derry Lin on 2022/11/7.
-//
-
-#include <cstdio>
-#include <cuda_fp16.h>
-
-#include <algorithm>
-#include <cmath>
-#include <vector>
-
-#include "cuda_helper.h"
-#include "helper.h"
-#include "multiScaleDeformableAttnKernel.h"
-#include <cstdio>
-#include <cuda/std/limits>
-#include <unistd.h>
-
-template <typename T>
-__forceinline__ __device__ T hmax(const T &a, const T &b) {
-  return max(a, b);
-}
-
-#if __CUDA_ARCH__ >= 800
-template <>
-__forceinline__ __device__ __half hmax(const __half &a, const __half &b) {
-  return __hmax(a, b);
-}
-template <>
-__forceinline__ __device__ __half2 hmax(const __half2 &a, const __half2 &b) {
-  return __hmax2(a, b);
-}
-#else
-template <>
-__forceinline__ __device__ __half hmax(const __half &a, const __half &b) {
-  return __hgt(a, b) ? a : b;
-}
-template <>
-__forceinline__ __device__ __half2 hmax(const __half2 &a, const __half2 &b) {
-  return __hfma2(__hgt2(a, b), a, __hmul2(__hle2(a, b), b));
-}
-#endif
-
-template <typename T> __forceinline__ __device__ T sign_05(T x) {
-  if (x > 0) {
-    return 0.5f;
-  }
-  return -0.5f;
-}
-
-template <typename T> __forceinline__ __device__ int8_t T2int8(T a) {
-  a = a > 127 ? 127 : a;
-  a = a < -128 ? -128 : a;
-  return int8_t(a + sign_05<T>(a));
-}
-
-template <> __forceinline__ __device__ int8_t T2int8(__half a) {
-  short temp = __half2short_rn(a);
-  temp = temp > static_cast<short>(127) ? static_cast<short>(127) : temp;
-  temp = temp < static_cast<short>(-128) ? static_cast<short>(-128) : temp;
-  return static_cast<int8_t>(temp);
-}
-
-template <typename T> __forceinline__ __device__ uint8_t T2uint8(T a) {
-  a = a > 255 ? 255 : a;
-  a = a < 0 ? 0 : a;
-  return uint8_t(a + 0.5);
-}
-
-template <> __forceinline__ __device__ uint8_t T2uint8(__half a) {
-  unsigned short temp = __half2ushort_rn(a);
-  temp = temp > static_cast<short>(255) ? static_cast<short>(255) : temp;
-  return static_cast<uint8_t>(temp);
-}
-
-__forceinline__ __device__ int8_t half2int8(const __half &hval,
-                                            const float &scale) {
-  __half ret = __hdiv(hval, __float2half(scale));
-  return T2int8<__half>(ret);
-}
-
-__forceinline__ __device__ uint8_t half2uint8(const __half &hval,
-                                              const float &scale) {
-  __half ret = __hdiv(hval, __float2half(scale));
-  return T2uint8<__half>(ret);
-}
-
-__forceinline__ __device__ void qmulf(const int32_4 &a, int8_4 &c,
-                                      const float &b) {
-  c.x = T2int8<float>(a.x * b);
-  c.y = T2int8<float>(a.y * b);
-  c.z = T2int8<float>(a.z * b);
-  c.w = T2int8<float>(a.w * b);
-}
-
-__forceinline__ __device__ void qmulh(const int32_4 &a, int8_4 &c,
-                                      const __half &b) {
-  c.x = T2int8<__half>(__int2half_rn(a.x) * b);
-  c.y = T2int8<__half>(__int2half_rn(a.y) * b);
-  c.z = T2int8<__half>(__int2half_rn(a.z) * b);
-  c.w = T2int8<__half>(__int2half_rn(a.w) * b);
-}
-
-__forceinline__ __device__ void dp4a(const int32_t *a, const int32_t *b,
-                                     int32_t &c) {
-#if __CUDA_ARCH__ >= 610
-  asm("dp4a.s32.s32 %0, %1, %2, %3;" : "+r"(c) : "r"(*a), "r"(*b), "r"(c));
-#else
-  auto ap = (int8_4 *)a, bp = (int8_4 *)b;
-
-  c += ap->x * bp->x;
-  c += ap->y * bp->y;
-  c += ap->z * bp->z;
-  c += ap->w * bp->w;
-#endif
-}
-
-__forceinline__ __device__ void dp4a(const int32_t *a, const uint32_t *b,
-                                     int32_t &c) {
-#if __CUDA_ARCH__ >= 610
-  asm("dp4a.s32.u32 %0, %1, %2, %3;" : "+r"(c) : "r"(*a), "r"(*b), "r"(c));
-#else
-  auto ap = (int8_4 *)a;
-  auto bp = (uint8_4 *)b;
-
-  c += ap->x * bp->x;
-  c += ap->y * bp->y;
-  c += ap->z * bp->z;
-  c += ap->w * bp->w;
-#endif
-}
-
-template <typename scalar_t>
-__device__ scalar_t ms_deform_attn_im2col_bilinear(
-    const scalar_t *&bottom_data, const int &height, const int &width,
-    const int &nheads, const int &channels, const scalar_t &h,
-    const scalar_t &w) {
-  const int h_low = floorf(h);
-  const int w_low = floorf(w);
-  const int h_high = h_low + 1;
-  const int w_high = w_low + 1;
-
-  const scalar_t lh = h - h_low;
-  const scalar_t lw = w - w_low;
-  const scalar_t hh = 1 - lh, hw = 1 - lw;
-
-  const int h_low_ptr_offset = h_low * width;
-  const int h_high_ptr_offset = h_low_ptr_offset + width;
-  const int w_low_ptr_offset = w_low;
-  const int w_high_ptr_offset = w_low_ptr_offset + 1;
-  const int step = channels * nheads;
-
-  scalar_t v1 = 0;
-  if (h_low >= 0 && w_low >= 0) {
-    const int ptr1 = (h_low_ptr_offset + w_low_ptr_offset) * step;
-    v1 = bottom_data[ptr1];
-  }
-  scalar_t v2 = 0;
-  if (h_low >= 0 && w_high <= width - 1) {
-    const int ptr2 = (h_low_ptr_offset + w_high_ptr_offset) * step;
-    v2 = bottom_data[ptr2];
-  }
-  scalar_t v3 = 0;
-  if (h_high <= height - 1 && w_low >= 0) {
-    const int ptr3 = (h_high_ptr_offset + w_low_ptr_offset) * step;
-    v3 = bottom_data[ptr3];
-  }
-  scalar_t v4 = 0;
-  if (h_high <= height - 1 && w_high <= width - 1) {
-    const int ptr4 = (h_high_ptr_offset + w_high_ptr_offset) * step;
-    v4 = bottom_data[ptr4];
-  }
-
-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;
-
-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);
-  return val;
-}
-
-template <>
-__device__ __half ms_deform_attn_im2col_bilinear(
-    const __half *&bottom_data, const int &height, const int &width,
-    const int &nheads, const int &channels, const __half &h, const __half &w) {
-  const __half h_low = hfloor(h);
-  const __half w_low = hfloor(w);
-  const __half h_high = __hadd(h_low, __float2half(1.f));
-  const __half w_high = __hadd(w_low, __float2half(1.f));
-
-  const __half lh = __hsub(h, h_low);
-  const __half lw = __hsub(w, w_low);
-  const __half hh = __hsub(__float2half(1.f), lh),
-               hw = __hsub(__float2half(1.f), lw);
-
-  const int h_low_ptr_offset = __half2int_rn(h_low) * width;
-  const int h_high_ptr_offset = h_low_ptr_offset + width;
-  const int w_low_ptr_offset = __half2int_rn(w_low);
-  const int w_high_ptr_offset = w_low_ptr_offset + 1;
-  const int step = channels * nheads;
-
-  __half v1 = 0;
-  if (__hge(h_low, __float2half(0.f)) && __hge(w_low, __float2half(0.f))) {
-    const int ptr1 = (h_low_ptr_offset + w_low_ptr_offset) * step;
-    v1 = bottom_data[ptr1];
-  }
-  __half v2 = 0;
-  if (__hge(h_low, __float2half(0.f)) &&
-      __hle(w_high, __float2half(static_cast<float>(width - 1)))) {
-    const int ptr2 = (h_low_ptr_offset + w_high_ptr_offset) * step;
-    v2 = bottom_data[ptr2];
-  }
-  __half v3 = 0;
-  if (__hle(h_high, __float2half(static_cast<float>(height - 1))) &&
-      __hge(w_low, __float2half(0.f))) {
-    const int ptr3 = (h_high_ptr_offset + w_low_ptr_offset) * step;
-    v3 = bottom_data[ptr3];
-  }
-  __half v4 = 0;
-  if (__hle(h_high, __float2half(static_cast<float>(height - 1))) &&
-      __hle(w_high, __float2half(static_cast<float>(width - 1)))) {
-    const int ptr4 = (h_high_ptr_offset + w_high_ptr_offset) * step;
-    v4 = bottom_data[ptr4];
-  }
-  const __half w1 = __hmul(hh, hw), w2 = __hmul(hh, lw), w3 = __hmul(lh, hw),
-               w4 = __hmul(lh, lw);
-  return __hadd(__hadd(__hmul(w1, v1), __hmul(w2, v2)),
-                __hadd(__hmul(w3, v3), __hmul(w4, v4)));
-}
-
-__device__ __half2 ms_deform_attn_im2col_bilinear_h2(
-    const __half2 *&bottom_data, const int &height, const int &width,
-    const int &nheads, const int &channels, const __half2 &wh) {
-  const __half2 wh_low = h2floor(wh);
-  const __half2 wh_high = __hadd2(wh_low, __float2half2_rn(1.f));
-
-  const __half2 lwh = __hsub2(wh, wh_low);
-  const __half2 hwh = __hsub2(__float2half2_rn(1.f), lwh);
-
-  const int h_low_ptr_offset = __high2float(wh_low) * width;
-  const int h_high_ptr_offset = h_low_ptr_offset + width;
-  const int w_low_ptr_offset = __low2float(wh_low);
-  const int w_high_ptr_offset = w_low_ptr_offset + 1;
-  const int step = channels * nheads;
-
-  __half2 v1 = __float2half2_rn(0.f);
-  if (__hge(__high2half(wh_low), __float2half(0.f)) &&
-      __hge(__low2half(wh_low), __float2half(0.f))) {
-    const int ptr1 = (h_low_ptr_offset + w_low_ptr_offset) * step;
-    v1 = bottom_data[ptr1];
-  }
-  __half2 v2 = __float2half2_rn(0.f);
-  if (__hge(__high2half(wh_low), __float2half(0.f)) &&
-      __hle(__low2half(wh_high), __float2half(static_cast<float>(width - 1)))) {
-    const int ptr2 = (h_low_ptr_offset + w_high_ptr_offset) * step;
-    v2 = bottom_data[ptr2];
-  }
-  __half2 v3 = __float2half2_rn(0.f);
-  if (__hle(__high2half(wh_high),
-            __float2half(static_cast<float>(height - 1))) &&
-      __hge(__low2half(wh_low), __float2half(0.f))) {
-    const int ptr3 = (h_high_ptr_offset + w_low_ptr_offset) * step;
-    v3 = bottom_data[ptr3];
-  }
-  __half2 v4 = __float2half2_rn(0.f);
-  if (__hle(__high2half(wh_high),
-            __float2half(static_cast<float>(height - 1))) &&
-      __hle(__low2half(wh_high), __float2half(static_cast<float>(width - 1)))) {
-    const int ptr4 = (h_high_ptr_offset + w_high_ptr_offset) * step;
-    v4 = bottom_data[ptr4];
-  }
-  const __half2 w1 = __half2half2(__hmul(__high2half(hwh), __low2half(hwh))),
-                w2 = __half2half2(__hmul(__high2half(hwh), __low2half(lwh))),
-                w3 = __half2half2(__hmul(__high2half(lwh), __low2half(hwh))),
-                w4 = __half2half2(__hmul(__high2half(lwh), __low2half(lwh)));
-  return __hadd2(__hadd2(__hmul2(w1, v1), __hmul2(w2, v2)),
-                 __hadd2(__hmul2(w3, v3), __hmul2(w4, v4)));
-}
-
-template <typename scalar_t>
-__device__ int8_4 ms_deform_attn_im2col_bilinear_int8(
-    const int8_4 *&bottom_data, const int &height, const int &width,
-    const int &nheads, const int &channels, const scalar_t &h,
-    const scalar_t &w) {
-  const int h_low = floorf(h);
-  const int w_low = floorf(w);
-  const int h_high = h_low + 1;
-  const int w_high = w_low + 1;
-
-  const scalar_t lh = h - h_low;
-  const scalar_t lw = w - w_low;
-  const scalar_t hh = 1 - lh, hw = 1 - lw;
-
-  const int h_low_ptr_offset = h_low * width;
-  const int h_high_ptr_offset = h_low_ptr_offset + width;
-  const int w_low_ptr_offset = w_low;
-  const int w_high_ptr_offset = w_low_ptr_offset + 1;
-  const int step = channels * nheads;
-
-  const float scale_area = 1 / 127.f;
-  int8_4 weight = {
-      T2int8<float>((hh * hw) / scale_area),
-      T2int8<float>((hh * lw) / scale_area),
-      T2int8<float>((lh * hw) / scale_area),
-      T2int8<float>((lh * lw) / scale_area),
-  };
-  int8_4 inps[4] = {0, 0, 0, 0}, output;
-  int32_t output_temp;
-
-  if (h_low >= 0 && w_low >= 0) {
-    const int ptr1 = (h_low_ptr_offset + w_low_ptr_offset) * step;
-    const int8_4 &inp = bottom_data[ptr1];
-    inps[0].x = inp.x;
-    inps[1].x = inp.y;
-    inps[2].x = inp.z;
-    inps[3].x = inp.w;
-  }
-
-  if (h_low >= 0 && w_high <= width - 1) {
-    const int ptr2 = (h_low_ptr_offset + w_high_ptr_offset) * step;
-    const int8_4 &inp = bottom_data[ptr2];
-    inps[0].y = inp.x;
-    inps[1].y = inp.y;
-    inps[2].y = inp.z;
-    inps[3].y = inp.w;
-  }
-
-  if (h_high <= height - 1 && w_low >= 0) {
-    const int ptr3 = (h_high_ptr_offset + w_low_ptr_offset) * step;
-    const int8_4 &inp = bottom_data[ptr3];
-    inps[0].z = inp.x;
-    inps[1].z = inp.y;
-    inps[2].z = inp.z;
-    inps[3].z = inp.w;
-  }
-
-  if (h_high <= height - 1 && w_high <= width - 1) {
-    const int ptr4 = (h_high_ptr_offset + w_high_ptr_offset) * step;
-    const int8_4 &inp = bottom_data[ptr4];
-    inps[0].w = inp.x;
-    inps[1].w = inp.y;
-    inps[2].w = inp.z;
-    inps[3].w = inp.w;
-  }
-
-  output_temp = 0;
-  dp4a((const int32_t *)inps, (const int32_t *)&weight, output_temp);
-  output.x = T2int8<float>(output_temp * scale_area);
-
-  output_temp = 0;
-  dp4a((const int32_t *)(inps + 1), (const int32_t *)&weight, output_temp);
-  output.y = T2int8<float>(output_temp * scale_area);
-
-  output_temp = 0;
-  dp4a((const int32_t *)(inps + 2), (const int32_t *)&weight, output_temp);
-  output.z = T2int8<float>(output_temp * scale_area);
-
-  output_temp = 0;
-  dp4a((const int32_t *)(inps + 3), (const int32_t *)&weight, output_temp);
-  output.w = T2int8<float>(output_temp * scale_area);
-
-  return output;
-}
-
-__device__ void ms_deform_attn_im2col_bilinear_int8_h2(
-    const int8_4 *&bottom_data, const int &height, const int &width,
-    const int &nheads, const int &channels, const __half2 &wh, int8_t *output) {
-  const __half2 wh_low = h2floor(wh);
-
-  const __half2 lwh = __hsub2(wh, wh_low);
-  const __half2 hwh = __hsub2(__float2half2_rn(1.f), lwh);
-
-  const int step = channels * nheads;
-  const int h_low_ptr_offset = __half2int_rn(wh_low.y) * width * step;
-  const int h_high_ptr_offset = h_low_ptr_offset + width * step;
-  const int w_low_ptr_offset = __half2int_rn(wh_low.x) * step;
-  const int w_high_ptr_offset = w_low_ptr_offset + step;
-
-  const __half scale_area = 255.f;
-  uint8_4 weight = {
-      __half2ushort_rn(
-          __hmul(__hmul(__high2half(hwh), __low2half(hwh)), scale_area)),
-      __half2ushort_rn(
-          __hmul(__hmul(__high2half(hwh), __low2half(lwh)), scale_area)),
-      __half2ushort_rn(
-          __hmul(__hmul(__high2half(lwh), __low2half(hwh)), scale_area)),
-      __half2ushort_rn(
-          __hmul(__hmul(__high2half(lwh), __low2half(lwh)), scale_area)),
-  };
-  auto weight_ui32 = reinterpret_cast<uint32_t *>(&weight);
-  int8_4 inps[4] = {0};
-  int32_t output_temp;
-  auto inps_i32 = reinterpret_cast<int32_t *>(inps);
-  auto inp = reinterpret_cast<int8_4 *>(&output_temp);
-
-  if (__hge(wh_low.y, __float2half(0.f)) &&
-      __hge(wh_low.x, __float2half(0.f))) {
-    *inp = bottom_data[h_low_ptr_offset + w_low_ptr_offset];
-    inps[0].x = inp->x;
-    inps[1].x = inp->y;
-    inps[2].x = inp->z;
-    inps[3].x = inp->w;
-  }
-
-  if (__hge(wh_low.y, __float2half(0.f)) &&
-      __hle(wh_low.x, __int2half_rn(width - 2))) {
-    *inp = bottom_data[h_low_ptr_offset + w_high_ptr_offset];
-    inps[0].y = inp->x;
-    inps[1].y = inp->y;
-    inps[2].y = inp->z;
-    inps[3].y = inp->w;
-  }
-
-  if (__hle(wh_low.y, __int2half_rn(height - 2)) &&
-      __hge(wh_low.x, __float2half(0.f))) {
-    *inp = bottom_data[h_high_ptr_offset + w_low_ptr_offset];
-    inps[0].z = inp->x;
-    inps[1].z = inp->y;
-    inps[2].z = inp->z;
-    inps[3].z = inp->w;
-  }
-
-  if (__hle(wh_low.y, __int2half_rn(height - 2)) &&
-      __hle(wh_low.x, __int2half_rn(width - 2))) {
-    *inp = bottom_data[h_high_ptr_offset + w_high_ptr_offset];
-    inps[0].w = inp->x;
-    inps[1].w = inp->y;
-    inps[2].w = inp->z;
-    inps[3].w = inp->w;
-  }
-
-  output_temp = 0;
-  dp4a(inps_i32, weight_ui32, output_temp);
-  output[0] = T2int8<__half>(__hdiv(__int2half_rn(output_temp), scale_area));
-
-  output_temp = 0;
-  dp4a(++inps_i32, weight_ui32, output_temp);
-  output[4] = T2int8<__half>(__hdiv(__int2half_rn(output_temp), scale_area));
-
-  output_temp = 0;
-  dp4a(++inps_i32, weight_ui32, output_temp);
-  output[8] = T2int8<__half>(__hdiv(__int2half_rn(output_temp), scale_area));
-
-  output_temp = 0;
-  dp4a(++inps_i32, weight_ui32, output_temp);
-  output[12] = T2int8<__half>(__hdiv(__int2half_rn(output_temp), scale_area));
-}
-
-__device__ void ms_deform_attn_im2col_bilinear_int8_h2_(
-    const int8_4 *&bottom_data, const int &height, const int &width,
-    const int &nheads, const int &channels, const __half2 &w, const __half2 &h,
-    const __half2 &condition, int8_t *output) {
-  const __half2 w_low = h2floor(w);
-  const __half2 h_low = h2floor(h);
-
-  const __half2 lw = __hsub2(w, w_low);
-  const __half2 hw = __hsub2(__float2half2_rn(1.f), lw);
-  const __half2 lh = __hsub2(h, h_low);
-  const __half2 hh = __hsub2(__float2half2_rn(1.f), lh);
-
-  const __half2 scale_area = __float2half2_rn(255.f);
-  const __half2 w1 = __hmul2(__hmul2(hh, hw), scale_area);
-  const __half2 w2 = __hmul2(__hmul2(hh, lw), scale_area);
-  const __half2 w3 = __hmul2(__hmul2(lh, hw), scale_area);
-  const __half2 w4 = __hmul2(__hmul2(lh, lw), scale_area);
-
-  const int step = channels * nheads;
-  int32_t output_temp;
-  if (condition.x) {
-    const int h_low_ptr_offset = __half2int_rn(h_low.x) * width * step;
-    const int h_high_ptr_offset = h_low_ptr_offset + width * step;
-    const int w_low_ptr_offset = __half2int_rn(w_low.x) * step;
-    const int w_high_ptr_offset = w_low_ptr_offset + step;
-
-    uint8_4 weight = {
-        __half2ushort_rn(w1.x),
-        __half2ushort_rn(w2.x),
-        __half2ushort_rn(w3.x),
-        __half2ushort_rn(w4.x),
-    };
-
-    auto weight_ui32 = reinterpret_cast<uint32_t *>(&weight);
-    int8_4 inps[4] = {0};
-    auto inps_i32 = reinterpret_cast<int32_t *>(inps);
-    auto inp = reinterpret_cast<int8_4 *>(&output_temp);
-
-    if (__hge(h_low.x, __float2half(0.f)) &&
-        __hge(w_low.x, __float2half(0.f))) {
-      *inp = bottom_data[h_low_ptr_offset + w_low_ptr_offset];
-      inps[0].x = inp->x;
-      inps[1].x = inp->y;
-      inps[2].x = inp->z;
-      inps[3].x = inp->w;
-    }
-    if (__hge(h_low.x, __float2half(0.f)) &&
-        __hle(w_low.x, __int2half_rn(width - 2))) {
-      *inp = bottom_data[h_low_ptr_offset + w_high_ptr_offset];
-      inps[0].y = inp->x;
-      inps[1].y = inp->y;
-      inps[2].y = inp->z;
-      inps[3].y = inp->w;
-    }
-    if (__hle(h_low.x, __int2half_rn(height - 2)) &&
-        __hge(w_low.x, __float2half(0.f))) {
-      *inp = bottom_data[h_high_ptr_offset + w_low_ptr_offset];
-      inps[0].z = inp->x;
-      inps[1].z = inp->y;
-      inps[2].z = inp->z;
-      inps[3].z = inp->w;
-    }
-    if (__hle(h_low.x, __int2half_rn(height - 2)) &&
-        __hle(w_low.x, __int2half_rn(width - 2))) {
-      *inp = bottom_data[h_high_ptr_offset + w_high_ptr_offset];
-      inps[0].w = inp->x;
-      inps[1].w = inp->y;
-      inps[2].w = inp->z;
-      inps[3].w = inp->w;
-    }
-
-    output_temp = 0;
-    dp4a(inps_i32, weight_ui32, output_temp);
-    output[0] =
-        T2int8<__half>(__hdiv(__int2half_rn(output_temp), scale_area.x));
-
-    output_temp = 0;
-    dp4a(++inps_i32, weight_ui32, output_temp);
-    output[4] =
-        T2int8<__half>(__hdiv(__int2half_rn(output_temp), scale_area.x));
-
-    output_temp = 0;
-    dp4a(++inps_i32, weight_ui32, output_temp);
-    output[8] =
-        T2int8<__half>(__hdiv(__int2half_rn(output_temp), scale_area.x));
-
-    output_temp = 0;
-    dp4a(++inps_i32, weight_ui32, output_temp);
-    output[12] =
-        T2int8<__half>(__hdiv(__int2half_rn(output_temp), scale_area.x));
-  }
-  if (condition.y) {
-    const int h_low_ptr_offset = __half2int_rn(h_low.y) * width * step;
-    const int h_high_ptr_offset = h_low_ptr_offset + width * step;
-    const int w_low_ptr_offset = __half2int_rn(w_low.y) * step;
-    const int w_high_ptr_offset = w_low_ptr_offset + step;
-
-    uint8_4 weight = {
-        __half2ushort_rn(w1.y),
-        __half2ushort_rn(w2.y),
-        __half2ushort_rn(w3.y),
-        __half2ushort_rn(w4.y),
-    };
-
-    auto weight_ui32 = reinterpret_cast<uint32_t *>(&weight);
-    int8_4 inps[4] = {0};
-    auto inps_i32 = reinterpret_cast<int32_t *>(inps);
-    auto inp = reinterpret_cast<int8_4 *>(&output_temp);
-
-    if (__hge(h_low.y, __float2half(0.f)) &&
-        __hge(w_low.y, __float2half(0.f))) {
-      *inp = bottom_data[h_low_ptr_offset + w_low_ptr_offset];
-      inps[0].x = inp->x;
-      inps[1].x = inp->y;
-      inps[2].x = inp->z;
-      inps[3].x = inp->w;
-    }
-    if (__hge(h_low.y, __float2half(0.f)) &&
-        __hle(w_low.y, __int2half_rn(width - 2))) {
-      *inp = bottom_data[h_low_ptr_offset + w_high_ptr_offset];
-      inps[0].y = inp->x;
-      inps[1].y = inp->y;
-      inps[2].y = inp->z;
-      inps[3].y = inp->w;
-    }
-    if (__hle(h_low.y, __int2half_rn(height - 2)) &&
-        __hge(w_low.y, __float2half(0.f))) {
-      *inp = bottom_data[h_high_ptr_offset + w_low_ptr_offset];
-      inps[0].z = inp->x;
-      inps[1].z = inp->y;
-      inps[2].z = inp->z;
-      inps[3].z = inp->w;
-    }
-    if (__hle(h_low.y, __int2half_rn(height - 2)) &&
-        __hle(w_low.y, __int2half_rn(width - 2))) {
-      *inp = bottom_data[h_high_ptr_offset + w_high_ptr_offset];
-      inps[0].w = inp->x;
-      inps[1].w = inp->y;
-      inps[2].w = inp->z;
-      inps[3].w = inp->w;
-    }
-
-    output_temp = 0;
-    dp4a(inps_i32, weight_ui32, output_temp);
-    output[1] =
-        T2int8<__half>(__hdiv(__int2half_rn(output_temp), scale_area.y));
-
-    output_temp = 0;
-    dp4a(++inps_i32, weight_ui32, output_temp);
-    output[5] =
-        T2int8<__half>(__hdiv(__int2half_rn(output_temp), scale_area.y));
-
-    output_temp = 0;
-    dp4a(++inps_i32, weight_ui32, output_temp);
-    output[9] =
-        T2int8<__half>(__hdiv(__int2half_rn(output_temp), scale_area.y));
-
-    output_temp = 0;
-    dp4a(++inps_i32, weight_ui32, output_temp);
-    output[13] =
-        T2int8<__half>(__hdiv(__int2half_rn(output_temp), scale_area.y));
-  }
-}
-
-template <typename scalar_t>
-__global__ void ms_deformable_im2col_gpu_kernel(
-    const int n, const scalar_t *data_value, const int32_t *data_spatial_shapes,
-    const scalar_t *data_reference_points,
-    const scalar_t *data_sampling_offsets, const scalar_t *data_attn_weight,
-    const int spatial_size, const int num_heads, const int channels,
-    const int num_levels, const int num_query, const int num_point,
-    const int points_per_group, scalar_t *data_col) {
-  CUDA_1D_KERNEL_LOOP(index, n) {
-    const int temp = index;
-    const int channel_index = index % channels;
-    index /= channels;
-    const int head_index = index % num_heads;
-    index /= num_heads;
-    const int query_index = index % num_query;
-    const int batch_index = index / num_query;
-
-    const scalar_t *data_value_ptr =
-        data_value +
-        (batch_index * spatial_size * num_heads + head_index) * channels +
-        channel_index;
-    int data_weight_ptr =
-        ((batch_index * num_query + query_index) * num_heads + head_index) *
-        num_levels * num_point;
-    int data_offset_w_ptr = data_weight_ptr << 1;
-    int data_points_ptr =
-        (batch_index * num_query + query_index) * points_per_group * 2;
-    scalar_t *data_output_ptr = data_col + temp;
-    scalar_t output = 0;
-
-    const int all_points = num_levels * num_point;
-    scalar_t max_weight = -cuda::std::numeric_limits<scalar_t>::infinity();
-    scalar_t sum_weight = 0.f;
-#pragma unroll
-    for (int w_index = 0; w_index < all_points; ++w_index) {
-      const scalar_t weight = data_attn_weight[data_weight_ptr + w_index];
-      max_weight = max(max_weight, weight);
-    }
-
-    for (int level_index = 0; level_index < num_levels; ++level_index) {
-      const int spatial_h_ptr = level_index << 1;
-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];
-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];
-
-      for (int point_index = 0; point_index < num_point; ++point_index) {
-        const int point_index_per_group = point_index % points_per_group;
-        const scalar_t reference_point_x =
-            data_reference_points[data_points_ptr + point_index_per_group * 2];
-        const scalar_t reference_point_y =
-            data_reference_points[data_points_ptr + point_index_per_group * 2 +
-                                  1];
-        const scalar_t loc_w = reference_point_x * spatial_w +
-                               data_sampling_offsets[data_offset_w_ptr];
-        const scalar_t loc_h = reference_point_y * spatial_h +
-                               data_sampling_offsets[data_offset_w_ptr + 1];
-
-        const scalar_t weight =
-            exp(data_attn_weight[data_weight_ptr] - max_weight);
-        sum_weight += weight;
-
-        const scalar_t h_im = loc_h - 0.5f;
-        const scalar_t w_im = loc_w - 0.5f;
-
-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w) {
-          output += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h,
-                                                   spatial_w, num_heads,
-                                                   channels, h_im, w_im) *
-                    weight;
-        }
-
-        data_weight_ptr += 1;
-        data_offset_w_ptr += 2;
-      }
-      data_value_ptr += spatial_h * spatial_w * channels * num_heads;
-    }
-    *data_output_ptr = output / sum_weight;
-  }
-}
-
-template <>
-__global__ void ms_deformable_im2col_gpu_kernel(
-    const int n, const __half *data_value, const int32_t *data_spatial_shapes,
-    const __half *data_reference_points, const __half *data_sampling_offsets,
-    const __half *data_attn_weight, const int spatial_size, const int num_heads,
-    const int channels, const int num_levels, const int num_query,
-    const int num_point, const int points_per_group, __half *data_col) {
-  CUDA_1D_KERNEL_LOOP(index, n) {
-    const int temp = index;
-    const int channel_index = index % channels;
-    index /= channels;
-    const int head_index = index % num_heads;
-    index /= num_heads;
-    const int query_index = index % num_query;
-    const int batch_index = index / num_query;
-
-    const __half *data_value_ptr =
-        data_value +
-        (batch_index * spatial_size * num_heads + head_index) * channels +
-        channel_index;
-    int data_weight_ptr =
-        ((batch_index * num_query + query_index) * num_heads + head_index) *
-        num_levels * num_point;
-    int data_offset_w_ptr = data_weight_ptr << 1;
-    int data_points_ptr =
-        (batch_index * num_query + query_index) * points_per_group * 2;
-    __half *data_output_ptr = data_col + temp;
-    __half output = 0.f;
-
-    const int all_points = num_levels * num_point;
-    __half max_weight = -cuda::std::numeric_limits<__half>::infinity();
-    __half sum_weight = 0.f;
-#pragma unroll
-    for (int w_index = 0; w_index < all_points; ++w_index) {
-      const __half weight = data_attn_weight[data_weight_ptr + w_index];
-      max_weight = hmax(max_weight, weight);
-    }
-
-    for (int level_index = 0; level_index < num_levels; ++level_index) {
-      const int spatial_h_ptr = level_index << 1;
-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];
-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];
-
-      for (int point_index = 0; point_index < num_point; ++point_index) {
-        const int point_index_per_group = point_index % points_per_group;
-        const __half reference_point_x =
-            data_reference_points[data_points_ptr + point_index_per_group * 2];
-        const __half reference_point_y =
-            data_reference_points[data_points_ptr + point_index_per_group * 2 +
-                                  1];
-        const __half loc_w = __hfma(reference_point_x, __int2half_rn(spatial_w),
-                                    data_sampling_offsets[data_offset_w_ptr]);
-        const __half loc_h =
-            __hfma(reference_point_y, __int2half_rn(spatial_h),
-                   data_sampling_offsets[data_offset_w_ptr + 1]);
-
-        const __half weight =
-            hexp(data_attn_weight[data_weight_ptr] - max_weight);
-        sum_weight += weight;
-
-        const __half h_im = __hsub(loc_h, __float2half(0.5f));
-        const __half w_im = __hsub(loc_w, __float2half(0.5f));
-
-        if (__hgt(h_im, __float2half(-1.f)) &&
-            __hgt(w_im, __float2half(-1.f)) &&
-            __hlt(h_im, __int2half_rn(spatial_h)) &&
-            __hlt(w_im, __int2half_rn(spatial_w))) {
-          output += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h,
-                                                   spatial_w, num_heads,
-                                                   channels, h_im, w_im) *
-                    weight;
-        }
-
-        data_weight_ptr += 1;
-        data_offset_w_ptr += 2;
-      }
-      data_value_ptr += spatial_h * spatial_w * channels * num_heads;
-    }
-    *data_output_ptr = __hdiv(output, sum_weight);
-  }
-}
-
-__global__ void ms_deformable_im2col_gpu_kernel_h2(
-    const int n, const __half2 *data_value, const int32_t *data_spatial_shapes,
-    const __half2 *data_reference_points, const __half2 *data_sampling_offsets,
-    const __half *data_attn_weight, const int spatial_size, const int num_heads,
-    const int channels, const int num_levels, const int num_query,
-    const int num_point, const int points_per_group, __half2 *data_col) {
-  CUDA_1D_KERNEL_LOOP(index, n) {
-    const int temp = index;
-    const int channel_index = index % channels;
-    index /= channels;
-    const int head_index = index % num_heads;
-    index /= num_heads;
-    const int query_index = index % num_query;
-    const int batch_index = index / num_query;
-
-    const __half2 *data_value_ptr =
-        data_value +
-        (batch_index * spatial_size * num_heads + head_index) * channels +
-        channel_index;
-    int data_weight_ptr =
-        ((batch_index * num_query + query_index) * num_heads + head_index) *
-        num_levels * num_point;
-    int data_points_ptr =
-        (batch_index * num_query + query_index) * points_per_group;
-    __half2 *data_output_ptr = data_col + temp;
-    __half2 output = __float2half2_rn(0.f);
-
-    const int all_points = num_levels * num_point;
-    __half max_weight = -cuda::std::numeric_limits<__half>::infinity();
-    __half sum_weight = 0.f;
-#pragma unroll
-    for (int w_index = 0; w_index < all_points; ++w_index) {
-      const __half weight = data_attn_weight[data_weight_ptr + w_index];
-      max_weight = hmax(max_weight, weight);
-    }
-
-    for (int level_index = 0; level_index < num_levels; ++level_index) {
-      const int spatial_h_ptr = level_index << 1;
-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];
-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];
-
-      for (int point_index = 0; point_index < num_point; ++point_index) {
-        const int point_index_per_group = point_index % points_per_group;
-        const __half2 reference_point_xy =
-            data_reference_points[data_points_ptr + point_index_per_group];
-        const __half2 offset = __hsub2(data_sampling_offsets[data_weight_ptr],
-                                       __float2half2_rn(0.5f));
-        const __half2 wh_im =
-            __hfma2(reference_point_xy,
-                    __floats2half2_rn(static_cast<float>(spatial_w),
-                                      static_cast<float>(spatial_h)),
-                    offset);
-
-        const __half weight =
-            hexp(data_attn_weight[data_weight_ptr] - max_weight);
-        sum_weight += weight;
-
-        const __half2 condition = __hmul2(
-            __hgt2(wh_im, __float2half2_rn(-1.f)),
-            __hlt2(wh_im, __floats2half2_rn(static_cast<float>(spatial_w),
-                                            static_cast<float>(spatial_h))));
-
-        if (__low2float(condition) * __high2float(condition)) {
-          output = __hfma2(ms_deform_attn_im2col_bilinear_h2(
-                               data_value_ptr, spatial_h, spatial_w, num_heads,
-                               channels, wh_im),
-                           __half2half2(weight), output);
-        }
-        data_weight_ptr += 1;
-      }
-      data_value_ptr += spatial_h * spatial_w * channels * num_heads;
-    }
-    *data_output_ptr = __h2div(output, __half2half2(sum_weight));
-  }
-}
-
-template <typename scalar_t>
-__global__ void ms_deformable_im2col_gpu_kernel_int8(
-    const int n, const int8_4 *data_value, float scale_value,
-    const int32_t *data_spatial_shapes, const scalar_t *data_reference_points,
-    const int8_4 *data_sampling_offsets, float scale_offset,
-    const int8_4 *data_attn_weight, float scale_weight, const int spatial_size,
-    const int num_heads, const int channels, const int num_levels,
-    const int num_query, const int num_point, const int points_per_group,
-    int8_4 *data_col, float scale_out) {
-  CUDA_1D_KERNEL_LOOP(index, n) {
-    const int temp = index;
-    const int channel_index = index % channels;
-    index /= channels;
-    const int head_index = index % num_heads;
-    index /= num_heads;
-    const int query_index = index % num_query;
-    const int batch_index = index / num_query;
-
-    const int8_4 *data_value_ptr =
-        data_value +
-        (batch_index * spatial_size * num_heads + head_index) * channels +
-        channel_index;
-    int data_weight_ptr =
-        ((batch_index * num_query + query_index) * num_heads + head_index) *
-        num_levels * num_point;
-    int data_offset_w_ptr = data_weight_ptr << 1;
-    int data_points_ptr =
-        (batch_index * num_query + query_index) * points_per_group * 2;
-    int8_4 *data_output_ptr = data_col + temp;
-    int32_4 output = 0;
-    const float scale_softmax = 127.f;
-    const float scale_o = scale_value * __frcp_rn(scale_out);
-    auto data_sampling_offsets_i8 =
-        reinterpret_cast<const int8_t *>(data_sampling_offsets);
-    auto data_attn_weight_i8 =
-        reinterpret_cast<const int8_t *>(data_attn_weight);
-
-    const int all_points = num_levels * num_point;
-    float max_weight = -cuda::std::numeric_limits<float>::infinity();
-    float sum_weight = 0.f;
-#pragma unroll
-    for (int w_index = 0; w_index < all_points; ++w_index) {
-      const float weight =
-          static_cast<float>(data_attn_weight_i8[data_weight_ptr + w_index]) *
-          scale_weight;
-      max_weight = max(max_weight, weight);
-    }
-
-    for (int level_index = 0; level_index < num_levels; ++level_index) {
-      const int spatial_h_ptr = level_index << 1;
-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];
-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];
-
-      for (int point_index = 0; point_index < num_point / 4; point_index++) {
-        int8_t values[16] = {0};
-        int8_t weights_arr[4];
-        auto weights = reinterpret_cast<int32_t *>(weights_arr);
-
-#pragma unroll
-        for (int i = 0; i < 4; i++, data_weight_ptr++, data_offset_w_ptr += 2) {
-          const int point_index_per_group =
-              (point_index * 4 + i) % points_per_group;
-          const float reference_point_x =
-              data_reference_points[data_points_ptr +
-                                    point_index_per_group * 2];
-          const float reference_point_y =
-              data_reference_points[data_points_ptr +
-                                    point_index_per_group * 2 + 1];
-          const float loc_w =
-              reference_point_x * spatial_w +
-              data_sampling_offsets_i8[data_offset_w_ptr] * scale_offset;
-          const float loc_h =
-              reference_point_y * spatial_h +
-              data_sampling_offsets_i8[data_offset_w_ptr + 1] * scale_offset;
-
-          const float h_im = loc_h - 0.5f;
-          const float w_im = loc_w - 0.5f;
-
-          weights_arr[i] =
-              T2int8(exp(data_attn_weight_i8[data_weight_ptr] * scale_weight -
-                         max_weight) *
-                     scale_softmax);
-          sum_weight += weights_arr[i];
-
-          if (!(h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w))
-            continue;
-          const int8_4 &val = ms_deform_attn_im2col_bilinear_int8(
-              data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im,
-              w_im);
-
-          values[i] = val.x;
-          values[i + 4] = val.y;
-          values[i + 8] = val.z;
-          values[i + 12] = val.w;
-        }
-
-        dp4a((const int32_t *)values, weights, output.x);
-        dp4a((const int32_t *)(values + 4), weights, output.y);
-        dp4a((const int32_t *)(values + 8), weights, output.z);
-        dp4a((const int32_t *)(values + 12), weights, output.w);
-      }
-      data_value_ptr += spatial_h * spatial_w * channels * num_heads;
-    }
-    int8_4 data_output;
-    qmulf(output, data_output, scale_o * __frcp_rn(sum_weight));
-    *data_output_ptr = data_output;
-  }
-}
-
-template <>
-__global__ void ms_deformable_im2col_gpu_kernel_int8(
-    const int n, const int8_4 *data_value, float scale_value,
-    const int32_t *data_spatial_shapes, const __half2 *data_reference_points,
-    const int8_4 *data_sampling_offsets, float scale_offset,
-    const int8_4 *data_attn_weight, float scale_weight, const int spatial_size,
-    const int num_heads, const int channels, const int num_levels,
-    const int num_query, const int num_point, const int points_per_group,
-    int8_4 *data_col, float scale_out) {
-  CUDA_1D_KERNEL_LOOP(index, n) {
-    const int temp = index;
-    const int channel_index = index % channels;
-    index /= channels;
-    const int head_index = index % num_heads;
-    index /= num_heads;
-    const int query_index = index % num_query;
-    const int batch_index = index / num_query;
-
-    const int8_4 *data_value_ptr =
-        data_value +
-        (batch_index * spatial_size * num_heads + head_index) * channels +
-        channel_index;
-    int data_weight_ptr =
-        ((batch_index * num_query + query_index) * num_heads + head_index) *
-            num_levels * num_point >>
-        2;
-    int data_points_ptr =
-        (batch_index * num_query + query_index) * points_per_group;
-    int8_4 *data_output_ptr = data_col + temp;
-    int32_4 output = 0;
-    const __half scale_o = __float2half(scale_value * __frcp_rn(scale_out));
-    const __half2 scale_softmax = __float2half2_rn(255.f);
-    const __half2 scale_weight_h2 = __float2half2_rn(scale_weight);
-    const __half2 scale_offset_h2 = __float2half2_rn(scale_offset);
-
-    const int all_points = num_levels * num_point;
-    __half2 max_weight =
-        __half2half2(-cuda::std::numeric_limits<__half>::infinity());
-    __half2 sum_weight = __float2half2_rn(0.f);
-#pragma unroll
-    for (int w_index = 0; w_index < all_points >> 2; ++w_index) {
-      const int8_4 weights = data_attn_weight[data_weight_ptr + w_index];
-
-      __half2 weight = __hmul2(__halves2half2(__short2half_rn(weights.x),
-                                              __short2half_rn(weights.y)),
-                               scale_weight_h2);
-      max_weight = hmax(max_weight, weight);
-
-      weight = __hmul2(__halves2half2(__short2half_rn(weights.z),
-                                      __short2half_rn(weights.w)),
-                       scale_weight_h2);
-      max_weight = hmax(max_weight, weight);
-    }
-    max_weight = hmax(max_weight, __lowhigh2highlow(max_weight));
-
-    for (int level_index = 0; level_index < num_levels; ++level_index) {
-      const int spatial_h_ptr = level_index << 1;
-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];
-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];
-      const int32_t value_step = spatial_h * spatial_w * channels * num_heads;
-      const __half2 spatial_wh =
-          __halves2half2(__int2half_rn(spatial_w), __int2half_rn(spatial_h));
-
-      for (int point_index = 0; point_index < (num_point >> 2); point_index++) {
-        int8_t values[16] = {0};
-        auto values_i32 = reinterpret_cast<int32_t *>(values);
-        uint8_2 weights_arr[4];
-        auto weights = reinterpret_cast<uint32_t *>(weights_arr);
-        auto weights_i84 = reinterpret_cast<int8_4 *>(weights_arr);
-        auto weights_h2 = reinterpret_cast<__half2 *>(weights_arr);
-        *weights_i84 = data_attn_weight[data_weight_ptr];
-        weights_h2[1] = __hmul2(
-            h2exp(__hfma2(__halves2half2(__short2half_rn(weights_i84->z),
-                                         __short2half_rn(weights_i84->w)),
-                          scale_weight_h2, __hneg2(max_weight))),
-            scale_softmax);
-        weights_h2[0] = __hmul2(
-            h2exp(__hfma2(__halves2half2(__short2half_rn(weights_i84->x),
-                                         __short2half_rn(weights_i84->y)),
-                          scale_weight_h2, __hneg2(max_weight))),
-            scale_softmax);
-
-#pragma unroll
-        for (int i = 0; i < 2; i++) {
-          const int8_4 offsets =
-              data_sampling_offsets[(data_weight_ptr << 1) + i];
-          const int point_index_per_group_0 =
-              ((point_index << 2) + (i << 1)) % points_per_group;
-          const int point_index_per_group_1 =
-              ((point_index << 2) + (i << 1) + 1) % points_per_group;
-          const __half2 reference_point_xy_0 =
-              data_reference_points[data_points_ptr + point_index_per_group_0];
-          const __half2 reference_point_xy_1 =
-              data_reference_points[data_points_ptr + point_index_per_group_1];
-
-          const __half2 offset_xy_0 =
-              __hfma2(__halves2half2(__short2half_rn(offsets.x),
-                                     __short2half_rn(offsets.y)),
-                      scale_offset_h2, __float2half2_rn(-0.5f));
-          const __half2 offset_xy_1 =
-              __hfma2(__halves2half2(__short2half_rn(offsets.z),
-                                     __short2half_rn(offsets.w)),
-                      scale_offset_h2, __float2half2_rn(-0.5f));
-
-          const __half2 wh_im_0 =
-              __hfma2(reference_point_xy_0, spatial_wh, offset_xy_0);
-          const __half2 wh_im_1 =
-              __hfma2(reference_point_xy_1, spatial_wh, offset_xy_1);
-
-          const __half2 condition_0 =
-              __hmul2(__hgt2(wh_im_0, __float2half2_rn(-1.f)),
-                      __hlt2(wh_im_0, spatial_wh));
-          const __half2 condition_1 =
-              __hmul2(__hgt2(wh_im_1, __float2half2_rn(-1.f)),
-                      __hlt2(wh_im_1, spatial_wh));
-
-          const __half2 condition = __halves2half2(
-              condition_0.x * condition_0.y, condition_1.x * condition_1.y);
-
-          if (condition.x) {
-            ms_deform_attn_im2col_bilinear_int8_h2(
-                data_value_ptr, spatial_h, spatial_w, num_heads, channels,
-                wh_im_0, values + 2 * i);
-          }
-          if (condition.y) {
-            ms_deform_attn_im2col_bilinear_int8_h2(
-                data_value_ptr, spatial_h, spatial_w, num_heads, channels,
-                wh_im_1, values + 2 * i + 1);
-          }
-          sum_weight = __hadd2(weights_h2[i], sum_weight);
-          weights_h2[i] = __hmul2(weights_h2[i], condition);
-          weights_arr[i].x = __half2ushort_rn(weights_h2[i].x);
-          weights_arr[i].y = __half2ushort_rn(weights_h2[i].y);
-        }
-        data_weight_ptr++;
-
-        dp4a(values_i32, weights, output.x);
-        dp4a(++values_i32, weights, output.y);
-        dp4a(++values_i32, weights, output.z);
-        dp4a(++values_i32, weights, output.w);
-      }
-      data_value_ptr += value_step;
-    }
-    int8_4 data_output;
-    qmulh(output, data_output, scale_o * hrcp(sum_weight.x + sum_weight.y));
-    *data_output_ptr = data_output;
-  }
-}
-
-template <typename scalar_t>
-void ms_deformable_im2col_cuda(const scalar_t *data_value,
-                               const int32_t *data_spatial_shapes,
-                               const scalar_t *data_reference_points,
-                               const scalar_t *data_sampling_offsets,
-                               const scalar_t *data_attn_weight,
-                               const int batch_size, const int spatial_size,
-                               const int num_heads, const int channels,
-                               const int num_levels, const int num_query,
-                               const int num_point, const int points_per_group,
-                               scalar_t *data_col, cudaStream_t stream) {
-  const int num_kernels = batch_size * num_query * num_heads * channels;
-  ms_deformable_im2col_gpu_kernel<scalar_t>
-      <<<GET_BLOCKS(num_kernels), THREADS_PER_BLOCK, 0, stream>>>(
-          num_kernels, data_value, data_spatial_shapes, data_reference_points,
-          data_sampling_offsets, data_attn_weight, spatial_size, num_heads,
-          channels, num_levels, num_query, num_point, points_per_group,
-          data_col);
-  cudaError_t err = cudaGetLastError();
-  if (err != cudaSuccess) {
-    printf("error in ms_deformable_im2col_cuda: %s\n", cudaGetErrorString(err));
-  }
-}
-
-template <>
-void ms_deformable_im2col_cuda(
-    const __half *data_value, const int32_t *data_spatial_shapes,
-    const __half *data_reference_points, const __half *data_sampling_offsets,
-    const __half *data_attn_weight, const int batch_size,
-    const int spatial_size, const int num_heads, const int channels,
-    const int num_levels, const int num_query, const int num_point,
-    const int points_per_group, __half *data_col, cudaStream_t stream) {
-  const int num_kernels = batch_size * num_query * num_heads * channels;
-  ms_deformable_im2col_gpu_kernel<__half>
-      <<<GET_BLOCKS(num_kernels), THREADS_PER_BLOCK, 0, stream>>>(
-          num_kernels, data_value, data_spatial_shapes, data_reference_points,
-          data_sampling_offsets, data_attn_weight, spatial_size, num_heads,
-          channels, num_levels, num_query, num_point, points_per_group,
-          data_col);
-  cudaError_t err = cudaGetLastError();
-  if (err != cudaSuccess) {
-    printf("error in ms_deformable_im2col_cuda: %s\n", cudaGetErrorString(err));
-  }
-}
-
-void ms_deformable_im2col_cuda_h2(
-    const __half2 *data_value, const int32_t *data_spatial_shapes,
-    const __half2 *data_reference_points, const __half2 *data_sampling_offsets,
-    const __half *data_attn_weight, const int batch_size,
-    const int spatial_size, const int num_heads, int channels,
-    const int num_levels, const int num_query, const int num_point,
-    const int points_per_group, __half2 *data_col, cudaStream_t stream) {
-  channels >>= 1;
-  const int num_kernels = batch_size * num_query * num_heads * channels;
-  ms_deformable_im2col_gpu_kernel_h2<<<GET_BLOCKS(num_kernels),
-                                       THREADS_PER_BLOCK, 0, stream>>>(
-      num_kernels, data_value, data_spatial_shapes, data_reference_points,
-      data_sampling_offsets, data_attn_weight, spatial_size, num_heads,
-      channels, num_levels, num_query, num_point, points_per_group, data_col);
-  cudaError_t err = cudaGetLastError();
-  if (err != cudaSuccess) {
-    printf("error in ms_deformable_im2col_cuda: %s\n", cudaGetErrorString(err));
-  }
-}
-
-template <typename scalar_t>
-void ms_deformable_im2col_cuda_int8(
-    const int8_4 *data_value, float scale_value,
-    const int32_t *data_spatial_shapes, const scalar_t *data_reference_points,
-    const int8_4 *data_sampling_offsets, float scale_offset,
-    const int8_4 *data_attn_weight, float scale_weight, const int batch_size,
-    const int spatial_size, const int num_heads, int channels,
-    const int num_levels, const int num_query, const int num_point,
-    const int points_per_group, int8_4 *data_col, float scale_out,
-    cudaStream_t stream) {
-  channels >>= 2;
-  const int num_kernels = batch_size * num_query * num_heads * channels;
-  ms_deformable_im2col_gpu_kernel_int8<scalar_t>
-      <<<GET_BLOCKS(num_kernels), THREADS_PER_BLOCK, 0, stream>>>(
-          num_kernels, data_value, scale_value, data_spatial_shapes,
-          data_reference_points, data_sampling_offsets, scale_offset,
-          data_attn_weight, scale_weight, spatial_size, num_heads, channels,
-          num_levels, num_query, num_point, points_per_group, data_col,
-          scale_out);
-  cudaError_t err = cudaGetLastError();
-  if (err != cudaSuccess) {
-    printf("error in ms_deformable_im2col_cuda: %s\n", cudaGetErrorString(err));
-  }
-}
-
-template <>
-void ms_deformable_im2col_cuda_int8(
-    const int8_4 *data_value, float scale_value,
-    const int32_t *data_spatial_shapes, const __half2 *data_reference_points,
-    const int8_4 *data_sampling_offsets, float scale_offset,
-    const int8_4 *data_attn_weight, float scale_weight, const int batch_size,
-    const int spatial_size, const int num_heads, int channels,
-    const int num_levels, const int num_query, const int num_point,
-    const int points_per_group, int8_4 *data_col, float scale_out,
-    cudaStream_t stream) {
-  channels >>= 2;
-  const int num_kernels = batch_size * num_query * num_heads * channels;
-  ms_deformable_im2col_gpu_kernel_int8<<<GET_BLOCKS(num_kernels),
-                                         THREADS_PER_BLOCK, 0, stream>>>(
-      num_kernels, data_value, scale_value, data_spatial_shapes,
-      data_reference_points, data_sampling_offsets, scale_offset,
-      data_attn_weight, scale_weight, spatial_size, num_heads, channels,
-      num_levels, num_query, num_point, points_per_group, data_col, scale_out);
-  cudaError_t err = cudaGetLastError();
-  if (err != cudaSuccess) {
-    printf("error in ms_deformable_im2col_cuda: %s\n", cudaGetErrorString(err));
-  }
-}
-
-template void ms_deformable_im2col_cuda<float>(
-    const float *data_value, const int32_t *data_spatial_shapes,
-    const float *data_reference_points, const float *data_sampling_offsets,
-    const float *data_attn_weight, const int batch_size, const int spatial_size,
-    const int num_heads, const int channels, const int num_levels,
-    const int num_query, const int num_point, const int points_per_group,
-    float *data_col, cudaStream_t stream);
-
-template void ms_deformable_im2col_cuda<__half>(
-    const __half *data_value, const int32_t *data_spatial_shapes,
-    const __half *data_reference_points, const __half *data_sampling_offsets,
-    const __half *data_attn_weight, const int batch_size,
-    const int spatial_size, const int num_heads, const int channels,
-    const int num_levels, const int num_query, const int num_point,
-    const int points_per_group, __half *data_col, cudaStream_t stream);
-
-template void ms_deformable_im2col_cuda_int8<float>(
-    const int8_4 *data_value, float scale_value,
-    const int32_t *data_spatial_shapes, const float *data_reference_points,
-    const int8_4 *data_sampling_offsets, float scale_offset,
-    const int8_4 *data_attn_weight, float scale_weight, const int batch_size,
-    const int spatial_size, const int num_heads, int channels,
-    const int num_levels, const int num_query, const int num_point,
-    const int points_per_group, int8_4 *data_col, float scale_out,
-    cudaStream_t stream);
-
-template void ms_deformable_im2col_cuda_int8<__half2>(
-    const int8_4 *data_value, float scale_value,
-    const int32_t *data_spatial_shapes, const __half2 *data_reference_points,
-    const int8_4 *data_sampling_offsets, float scale_offset,
-    const int8_4 *data_attn_weight, float scale_weight, const int batch_size,
-    const int spatial_size, const int num_heads, int channels,
-    const int num_levels, const int num_query, const int num_point,
-    const int points_per_group, int8_4 *data_col, float scale_out,
-    cudaStream_t stream);
diff --git a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.h b/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.h
deleted file mode 100644
index 78785d1..0000000
--- a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.h
+++ /dev/null
@@ -1,40 +0,0 @@
-//
-// Created by Derry Lin on 2022/11/7.
-//
-
-#ifndef TENSORRT_OPS_MULTISCALEDEFORMABLEATTNKERNEL_H
-#define TENSORRT_OPS_MULTISCALEDEFORMABLEATTNKERNEL_H
-
-#include "cuda_int8.h"
-#include <cuda_fp16.h>
-#include <cuda_runtime.h>
-
-template <typename T>
-void ms_deformable_im2col_cuda(
-    const T *data_value, const int32_t *data_spatial_shapes,
-    const T *data_reference_points, const T *data_sampling_offsets,
-    const T *data_attn_weight, const int batch_size, const int spatial_size,
-    const int num_heads, const int channels, const int num_levels,
-    const int num_query, const int num_point, const int points_per_group,
-    T *data_col, cudaStream_t stream);
-
-void ms_deformable_im2col_cuda_h2(
-    const __half2 *data_value, const int32_t *data_spatial_shapes,
-    const __half2 *data_reference_points, const __half2 *data_sampling_offsets,
-    const __half *data_attn_weight, const int batch_size,
-    const int spatial_size, const int num_heads, int channels,
-    const int num_levels, const int num_query, const int num_point,
-    const int points_per_group, __half2 *data_col, cudaStream_t stream);
-
-template <typename T>
-void ms_deformable_im2col_cuda_int8(
-    const int8_4 *data_value, float scale_value,
-    const int32_t *data_spatial_shapes, const T *data_reference_points,
-    const int8_4 *data_sampling_offsets, float scale_offset,
-    const int8_4 *data_attn_weight, float scale_weight, const int batch_size,
-    const int spatial_size, const int num_heads, int channels,
-    const int num_levels, const int num_query, const int num_point,
-    const int points_per_group, int8_4 *data_col, float scale_out,
-    cudaStream_t stream);
-
-#endif // TENSORRT_OPS_MULTISCALEDEFORMABLEATTNKERNEL_H
diff --git a/TensorRT/plugin/bev_pool_v2/bevPoolPlugin.cpp b/TensorRT/plugins/bevPoolPlugin.cpp
similarity index 73%
rename from TensorRT/plugin/bev_pool_v2/bevPoolPlugin.cpp
rename to TensorRT/plugins/bevPoolPlugin.cpp
index 3aae089..bdec92f 100644
--- a/TensorRT/plugin/bev_pool_v2/bevPoolPlugin.cpp
+++ b/TensorRT/plugins/bevPoolPlugin.cpp
@@ -19,6 +19,7 @@ namespace {
 constexpr char const *BP_PLUGIN_VERSION{"1"};
 constexpr char const *BP_PLUGIN_NAME{"BEVPoolV2TRT"};
 constexpr char const *BP_PLUGIN_NAME2{"BEVPoolV2TRT2"};
+constexpr int32_t BP_PLUGIN_SERIALIZATION_VERSION = 1;
 } // namespace
 
 PluginFieldCollection BEVPoolPluginCreator::mFC{};
@@ -33,6 +34,14 @@ BEVPoolPlugin::BEVPoolPlugin(int outWidth, int outHeight, bool use_h2)
 BEVPoolPlugin::BEVPoolPlugin(const void *serialData, size_t serialLength,
                              bool use_h2)
     : use_h2(use_h2) {
+  // Deserialize with added version control
+  int32_t version = 0;
+  deserialize_value(&serialData, &serialLength, &version);
+  
+  if (version != BP_PLUGIN_SERIALIZATION_VERSION) {
+    throw std::runtime_error("BEVPoolPlugin: Unsupported serialization version");
+  }
+
   deserialize_value(&serialData, &serialLength, &mOutWidth);
   deserialize_value(&serialData, &serialLength, &mOutHeight);
 }
@@ -45,75 +54,85 @@ DimsExprs BEVPoolPlugin::getOutputDimensions(
     int32_t outputIndex, const nvinfer1::DimsExprs *inputs, int32_t nbInputs,
     nvinfer1::IExprBuilder &exprBuilder) noexcept {
   nvinfer1::DimsExprs ret;
-  ret.nbDims = 4;
+  ret.nbDims = 5;
   ret.d[0] = exprBuilder.constant(1);
-  ret.d[1] = exprBuilder.constant(mOutHeight);
-  ret.d[2] = exprBuilder.constant(mOutWidth);
-  ret.d[3] = inputs[1].d[3];
-  return ret;
+  ret.d[1] = exprBuilder.constant(8);
+  ret.d[2] = exprBuilder.constant(mOutHeight); 
+  ret.d[3] = exprBuilder.constant(mOutWidth);
+  ret.d[4] = inputs[1].d[4];
+  return ret; 
 }
 
 int32_t BEVPoolPlugin::initialize() noexcept { return 0; }
 
 void BEVPoolPlugin::terminate() noexcept {}
 
-size_t
-BEVPoolPlugin::getWorkspaceSize(const nvinfer1::PluginTensorDesc *inputs,
-                                int32_t nbInputs,
-                                const nvinfer1::PluginTensorDesc *outputs,
-                                int32_t nbOutputs) const noexcept {
+size_t BEVPoolPlugin::getWorkspaceSize(
+    const nvinfer1::PluginTensorDesc *inputs, int32_t nbInputs,
+    const nvinfer1::PluginTensorDesc *outputs, int32_t nbOutputs) const noexcept {
   return 0;
 }
 
-int32_t BEVPoolPlugin::enqueue(const nvinfer1::PluginTensorDesc *inputDesc,
-                               const nvinfer1::PluginTensorDesc *outputDesc,
-                               const void *const *inputs, void *const *outputs,
-                               void *workspace, cudaStream_t stream) noexcept {
-  nvinfer1::Dims feat_dims = inputDesc[1].dims;     // bnhwc
-  nvinfer1::Dims interval_dims = inputDesc[5].dims; // n
-  nvinfer1::Dims out_dims = outputDesc[0].dims;     // bhwc
+int32_t BEVPoolPlugin::enqueue(
+    const nvinfer1::PluginTensorDesc *inputDesc,
+    const nvinfer1::PluginTensorDesc *outputDesc,
+    const void *const *inputs, void *const *outputs, void *workspace,
+    cudaStream_t stream) noexcept {
+  nvinfer1::Dims feat_dims = inputDesc[1].dims;
+  nvinfer1::Dims interval_dims = inputDesc[5].dims;
+  nvinfer1::Dims out_dims = outputDesc[0].dims;
   auto data_type = inputDesc[0].type;
-  int num_points =
-      out_dims.d[0] * out_dims.d[1] * out_dims.d[2] * out_dims.d[3];
-  switch (data_type) {
-  case nvinfer1::DataType::kFLOAT:
-    bev_pool_v2(feat_dims.d[3], interval_dims.d[0], num_points,
-                (float *)inputs[0], (float *)inputs[1], (int *)inputs[2],
-                (int *)inputs[3], (int *)inputs[4], (int *)inputs[5],
-                (int *)inputs[6], (float *)outputs[0], stream);
-    break;
-  case nvinfer1::DataType::kHALF:
-    if (use_h2) {
-      bev_pool_v2_h2(feat_dims.d[3], interval_dims.d[0], num_points,
-                     (__half *)inputs[0], (__half2 *)inputs[1],
-                     (int *)inputs[2], (int *)inputs[3], (int *)inputs[4],
-                     (int *)inputs[5], (int *)inputs[6], (__half2 *)outputs[0],
-                     stream);
-    } else {
-      bev_pool_v2(feat_dims.d[3], interval_dims.d[0], num_points,
-                  (__half *)inputs[0], (__half *)inputs[1], (int *)inputs[2],
+
+  int num_points = out_dims.d[0] * out_dims.d[1] * out_dims.d[2] * out_dims.d[3] * out_dims.d[4];
+
+  try {
+    switch (data_type) {
+    case nvinfer1::DataType::kFLOAT:
+      bev_pool_v2(feat_dims.d[4], interval_dims.d[0], num_points,
+                  (float *)inputs[0], (float *)inputs[1], (int *)inputs[2],
                   (int *)inputs[3], (int *)inputs[4], (int *)inputs[5],
-                  (int *)inputs[6], (__half *)outputs[0], stream);
+                  (int *)inputs[6], (float *)outputs[0], stream);
+      break;
+    case nvinfer1::DataType::kHALF:
+      if (use_h2) {
+        bev_pool_v2_h2(feat_dims.d[4], interval_dims.d[0], num_points,
+                       (__half *)inputs[0], (__half2 *)inputs[1],
+                       (int *)inputs[2], (int *)inputs[3], (int *)inputs[4],
+                       (int *)inputs[5], (int *)inputs[6], (__half2 *)outputs[0],
+                       stream);
+      } else {
+        bev_pool_v2(feat_dims.d[4], interval_dims.d[0], num_points,
+                    (__half *)inputs[0], (__half *)inputs[1], (int *)inputs[2],
+                    (int *)inputs[3], (int *)inputs[4], (int *)inputs[5],
+                    (int *)inputs[6], (__half *)outputs[0], stream);
+      }
+      break;
+    case nvinfer1::DataType::kINT8:
+      bev_pool_v2_int8(feat_dims.d[4], interval_dims.d[0], num_points,
+                       (int8_t *)inputs[0], inputDesc[0].scale, 
+                       (int8_4 *)inputs[1], inputDesc[1].scale,
+                       (int *)inputs[2], (int *)inputs[3], (int *)inputs[4], 
+                       (int *)inputs[5], (int *)inputs[6], (int8_4 *)outputs[0],
+                       outputDesc[0].scale, stream);
+      break;
+    default:
+      return 1;
     }
-    break;
-  case nvinfer1::DataType::kINT8:
-    bev_pool_v2_int8(
-        feat_dims.d[3], interval_dims.d[0], num_points, (int8_t *)inputs[0],
-        inputDesc[0].scale, (int8_4 *)inputs[1], inputDesc[1].scale,
-        (int *)inputs[2], (int *)inputs[3], (int *)inputs[4], (int *)inputs[5],
-        (int *)inputs[6], (int8_4 *)outputs[0], outputDesc[0].scale, stream);
-    break;
-  default:
+  } catch (const std::exception& e) {
+    caughtError(e);
     return 1;
   }
   return 0;
 }
 
 size_t BEVPoolPlugin::getSerializationSize() const noexcept {
-  return serialized_size(mOutWidth) + serialized_size(mOutHeight);
+  return serialized_size(BP_PLUGIN_SERIALIZATION_VERSION) +
+         serialized_size(mOutWidth) + 
+         serialized_size(mOutHeight);
 }
 
 void BEVPoolPlugin::serialize(void *buffer) const noexcept {
+  serialize_value(&buffer, BP_PLUGIN_SERIALIZATION_VERSION); // version control
   serialize_value(&buffer, mOutWidth);
   serialize_value(&buffer, mOutHeight);
 }
@@ -157,6 +176,10 @@ IPluginV2DynamicExt *BEVPoolPlugin::clone() const noexcept {
   return nullptr;
 }
 
+// Remaining methods such as setPluginNamespace, getOutputDataType, 
+// configurePlugin, etc. remain similar with consistent error handling as needed.
+
+
 void BEVPoolPlugin::setPluginNamespace(const char *pluginNamespace) noexcept {
   mPluginNamespace = pluginNamespace;
 }
diff --git a/TensorRT/plugin/grid_sampler/gridSamplerPlugin.cpp b/TensorRT/plugins/gridSamplerPlugin.cpp
similarity index 99%
rename from TensorRT/plugin/grid_sampler/gridSamplerPlugin.cpp
rename to TensorRT/plugins/gridSamplerPlugin.cpp
index cc52952..0685125 100644
--- a/TensorRT/plugin/grid_sampler/gridSamplerPlugin.cpp
+++ b/TensorRT/plugins/gridSamplerPlugin.cpp
@@ -121,6 +121,7 @@ int32_t GridSamplerPlugin::enqueue(const nvinfer1::PluginTensorDesc *inputDesc,
   auto data_type = inputDesc[0].type;
   ASSERT(data_type == DataType::kFLOAT || data_type == DataType::kHALF ||
          data_type == DataType::kINT8)
+  
 
   switch (data_type) {
   case DataType::kFLOAT:
@@ -130,12 +131,15 @@ int32_t GridSamplerPlugin::enqueue(const nvinfer1::PluginTensorDesc *inputDesc,
                        mMode, mPaddingMode, mAlignCorners, stream);
     break;
   case DataType::kHALF:
+    printf("Entered case DataType::kHALF\n");
     if (use_h2) {
+      printf("Using __half2 for grid_sample\n");
       grid_sample<__half2>(
           (__half2 *)outputs[0], (__half2 *)inputs[0], (__half2 *)inputs[1],
           &(output_dims.d[0]), &(input_dims.d[0]), &(grid_dims.d[0]),
           input_dims.nbDims, mMode, mPaddingMode, mAlignCorners, stream);
     } else {
+      printf("Using __half for grid_sample\n");
       grid_sample<__half>(
           (__half *)outputs[0], (__half *)inputs[0], (__half *)inputs[1],
           &(output_dims.d[0]), &(input_dims.d[0]), &(grid_dims.d[0]),
diff --git a/TensorRT/plugin/inverse/inversePlugin.cpp b/TensorRT/plugins/inversePlugin.cpp
similarity index 100%
rename from TensorRT/plugin/inverse/inversePlugin.cpp
rename to TensorRT/plugins/inversePlugin.cpp
diff --git a/TensorRT/plugin/modulated_deformable_conv2d/modulatedDeformableConv2dPlugin.cpp b/TensorRT/plugins/modulatedDeformableConv2dPlugin.cpp
similarity index 100%
rename from TensorRT/plugin/modulated_deformable_conv2d/modulatedDeformableConv2dPlugin.cpp
rename to TensorRT/plugins/modulatedDeformableConv2dPlugin.cpp
diff --git a/TensorRT/plugin/multi_head_attn/multiHeadAttnPlugin.cpp b/TensorRT/plugins/multiHeadAttnPlugin.cpp
similarity index 100%
rename from TensorRT/plugin/multi_head_attn/multiHeadAttnPlugin.cpp
rename to TensorRT/plugins/multiHeadAttnPlugin.cpp
diff --git a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnPlugin.cpp b/TensorRT/plugins/multiScaleDeformableAttnPlugin.cpp
similarity index 77%
rename from TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnPlugin.cpp
rename to TensorRT/plugins/multiScaleDeformableAttnPlugin.cpp
index 67886e0..871691e 100644
--- a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnPlugin.cpp
+++ b/TensorRT/plugins/multiScaleDeformableAttnPlugin.cpp
@@ -49,11 +49,14 @@ DimsExprs MultiScaleDeformableAttnPlugin::getOutputDimensions(
     int32_t outputIndex, const nvinfer1::DimsExprs *inputs, int32_t nbInputs,
     nvinfer1::IExprBuilder &exprBuilder) noexcept {
   DimsExprs outputDim;
-  outputDim.nbDims = 4;
+  outputDim.nbDims = 3;
   outputDim.d[0] = inputs[0].d[0];
   outputDim.d[1] = inputs[3].d[1];
-  outputDim.d[2] = inputs[0].d[2];
-  outputDim.d[3] = inputs[0].d[3];
+  outputDim.d[2] = exprBuilder.operation(
+    nvinfer1::DimensionOperation::kPROD, *inputs[0].d[2], *inputs[0].d[3]);
+
+  // outputDim.d[2] = inputs[0].d[2];
+  // outputDim.d[3] = inputs[0].d[3];
   return outputDim;
 }
 
@@ -72,67 +75,60 @@ int32_t MultiScaleDeformableAttnPlugin::enqueue(
     const nvinfer1::PluginTensorDesc *inputDesc,
     const nvinfer1::PluginTensorDesc *outputDesc, const void *const *inputs,
     void *const *outputs, void *workspace, cudaStream_t stream) noexcept {
-  float scale_value = inputDesc[0].scale, scale_ref = inputDesc[2].scale,
-        scale_offset = inputDesc[3].scale, scale_weight = inputDesc[4].scale,
-        scale_out = outputDesc[0].scale;
   Dims value_dims = inputDesc[0].dims;
   const int batch = value_dims.d[0];
   const int spatial_size = value_dims.d[1];
   const int num_heads = value_dims.d[2];
   const int channels = value_dims.d[3];
 
-  const int num_levels = inputDesc[1].dims.d[0];
-
-  const int points_per_group = inputDesc[2].dims.d[3] / 2;
-  const int num_query = inputDesc[3].dims.d[1];
-  const int num_point = inputDesc[4].dims.d[3] / num_levels;
-
+  const int num_levels = inputDesc[1].dims.d[0]; //spatial_shapes
+  const int num_query = inputDesc[3].dims.d[1]; // sampling_locations
+  const int num_point = inputDesc[3].dims.d[4];
+  
   auto data_type = inputDesc[0].type;
-  auto data_type_rp = inputDesc[2].type;
-  ASSERT(data_type == DataType::kFLOAT || data_type == DataType::kHALF ||
-         data_type == DataType::kINT8)
-  ASSERT(data_type_rp == DataType::kFLOAT || data_type_rp == DataType::kHALF)
 
   switch (data_type) {
   case DataType::kFLOAT:
     ms_deformable_im2col_cuda<float>(
-        (float *)inputs[0], (int32_t *)inputs[1], (float *)inputs[2],
-        (float *)inputs[3], (float *)inputs[4], batch, spatial_size, num_heads,
-        channels, num_levels, num_query, num_point, points_per_group,
-        (float *)outputs[0], stream);
+        (float *)inputs[0], 
+        (int *)inputs[1], 
+        (int *)inputs[2], 
+        (float *)inputs[3], 
+        (float *)inputs[4], 
+        batch, 
+        spatial_size, 
+        num_heads, 
+        channels, 
+        num_levels, 
+        num_query, 
+        num_point,
+        (float *)outputs[0], 
+        stream);
     break;
   case DataType::kHALF:
-    if (use_h2) {
-      ms_deformable_im2col_cuda_h2(
-          (__half2 *)inputs[0], (int32_t *)inputs[1], (__half2 *)inputs[2],
-          (__half2 *)inputs[3], (__half *)inputs[4], batch, spatial_size,
-          num_heads, channels, num_levels, num_query, num_point,
-          points_per_group, (__half2 *)outputs[0], stream);
-    } else {
-      ms_deformable_im2col_cuda<__half>(
-          (__half *)inputs[0], (int32_t *)inputs[1], (__half *)inputs[2],
+    ms_deformable_im2col_cuda<__half>(
+          (__half *)inputs[0], (int  *)inputs[1], (int *)inputs[2],
           (__half *)inputs[3], (__half *)inputs[4], batch, spatial_size,
           num_heads, channels, num_levels, num_query, num_point,
-          points_per_group, (__half *)outputs[0], stream);
-    }
-    break;
-  case DataType::kINT8:
-    if (data_type_rp == DataType::kHALF) {
-      ms_deformable_im2col_cuda_int8<__half2>(
-          (int8_4 *)inputs[0], scale_value, (int32_t *)inputs[1],
-          (__half2 *)inputs[2], (int8_4 *)inputs[3], scale_offset,
-          (int8_4 *)inputs[4], scale_weight, batch, spatial_size, num_heads,
-          channels, num_levels, num_query, num_point, points_per_group,
-          (int8_4 *)outputs[0], scale_out, stream);
-    } else {
-      ms_deformable_im2col_cuda_int8<float>(
-          (int8_4 *)inputs[0], scale_value, (int32_t *)inputs[1],
-          (float *)inputs[2], (int8_4 *)inputs[3], scale_offset,
-          (int8_4 *)inputs[4], scale_weight, batch, spatial_size, num_heads,
-          channels, num_levels, num_query, num_point, points_per_group,
-          (int8_4 *)outputs[0], scale_out, stream);
-    }
+          (__half *)outputs[0], stream);
     break;
+  // case DataType::kINT8:
+  //   if (data_type_rp == DataType::kHALF) {
+  //     ms_deformable_im2col_cuda_int8<__half2>(
+  //         (int8_4 *)inputs[0], scale_value, (int32_t *)inputs[1],
+  //         (__half2 *)inputs[2], (int8_4 *)inputs[3], scale_offset,
+  //         (int8_4 *)inputs[4], scale_weight, batch, spatial_size, num_heads,
+  //         channels, num_levels, num_query, num_point, points_per_group,
+  //         (int8_4 *)outputs[0], scale_out, stream);
+  //   } else {
+  //     ms_deformable_im2col_cuda_int8<float>(
+  //         (int8_4 *)inputs[0], scale_value, (int32_t *)inputs[1],
+  //         (float *)inputs[2], (int8_4 *)inputs[3], scale_offset,
+  //         (int8_4 *)inputs[4], scale_weight, batch, spatial_size, num_heads,
+  //         channels, num_levels, num_query, num_point, points_per_group,
+  //         (int8_4 *)outputs[0], scale_out, stream);
+  //   }
+  //   break;
   default:
     return 1;
   }
@@ -166,13 +162,7 @@ bool MultiScaleDeformableAttnPlugin::supportsFormatCombination(
     return inOut[pos].type == nvinfer1::DataType::kINT32 &&
            inOut[pos].format == nvinfer1::TensorFormat::kLINEAR;
   case 2:
-    if (inOut[0].type == nvinfer1::DataType::kFLOAT ||
-        inOut[0].type == nvinfer1::DataType::kHALF) {
-      return inOut[pos].type == inOut[0].type &&
-             inOut[pos].format == nvinfer1::TensorFormat::kLINEAR;
-    }
-    return (inOut[pos].type == nvinfer1::DataType::kHALF ||
-            inOut[pos].type == nvinfer1::DataType::kFLOAT) &&
+    return inOut[pos].type == nvinfer1::DataType::kINT32 &&
            inOut[pos].format == nvinfer1::TensorFormat::kLINEAR;
   case 3:
     return inOut[pos].type == inOut[0].type &&
diff --git a/TensorRT/plugin/rotate/rotatePlugin.cpp b/TensorRT/plugins/rotatePlugin.cpp
similarity index 100%
rename from TensorRT/plugin/rotate/rotatePlugin.cpp
rename to TensorRT/plugins/rotatePlugin.cpp
-- 
2.25.1

