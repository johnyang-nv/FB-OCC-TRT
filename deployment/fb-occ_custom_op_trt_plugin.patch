From c93887f23ae4bc8e95322aaaba457574aa05e440 Mon Sep 17 00:00:00 2001
From: "John Yang (SW-TEGRA)" <johnyang@nvidia.com>
Date: Fri, 20 Dec 2024 22:38:32 -0800
Subject: [PATCH] fbocc_custom_op_trt_plugin

---
 TensorRT/Makefile                             |  61 +++
 TensorRT/plugin/bev_pool_v2/bevPoolKernel.cu  |   2 +-
 TensorRT/plugin/bev_pool_v2/bevPoolKernel.h   |   5 +-
 TensorRT/plugin/bev_pool_v2/bevPoolPlugin.cpp |  59 +--
 TensorRT/plugin/bev_pool_v2/bevPoolPlugin.h   |   2 +-
 .../plugin/grid_sampler/gridSamplerPlugin.cpp |   1 +
 .../multiScaleDeformableAttnKernel.cu         | 369 ++++++++----------
 .../multiScaleDeformableAttnKernel.h          |  22 +-
 .../multiScaleDeformableAttnPlugin.cpp        |  30 +-
 .../multiScaleDeformableAttnPlugin.h          |   2 +-
 det2trt/models/functions/__init__.py          |  16 +-
 det2trt/models/functions/bev_pool_v2.py       |  19 +-
 det2trt/models/functions/grid_sampler.py      |   2 +-
 .../functions/multi_scale_deformable_attn.py  | 121 ++----
 14 files changed, 336 insertions(+), 375 deletions(-)
 create mode 100644 TensorRT/Makefile

diff --git a/TensorRT/Makefile b/TensorRT/Makefile
new file mode 100644
index 0000000..8fb3e43
--- /dev/null
+++ b/TensorRT/Makefile
@@ -0,0 +1,61 @@
+# Define paths
+CUDA_DIR ?= /usr/local/cuda
+DLSW_TRIPLE = aarch64-linux-gnu
+TRT_LIB_DIR ?= /usr/lib/$(DLSW_TRIPLE)
+TRT_INC_DIR ?= /usr/include/$(DLSW_TRIPLE)
+SYSROOT ?= 
+OUTPUT_DIR ?= build
+
+
+
+# Compiler and flags
+CXX := aarch64-linux-gnu-g++
+NVCC := $(CUDA_DIR)/bin/nvcc
+
+CXXFLAGS := -std=c++14 -O3 -Wno-deprecated-declarations -fPIC \
+    -I$(TRT_INC_DIR)/ \
+    -I$(CUDA_DIR)/include \
+    -I$(SYSROOT)/include \
+    -Icommon \
+
+NVCCFLAGS := -std=c++14 --compiler-options '-fPIC' \
+    -I$(TRT_INC_DIR)/ \
+    -I$(CUDA_DIR)/include \
+    -I$(SYSROOT)/include \
+    -Icommon \
+    -gencode arch=compute_87,code=sm_87
+
+NVCCFLAGS += --compiler-bindir=/usr/bin/aarch64-linux-gnu-g++
+
+# Library paths and libraries
+LDFLAGS := -L$(TRT_LIB_DIR)  -L$(CUDA_DIR)/targets/aarch64-linux/lib
+LDLIBS := -lnvinfer -lnvinfer_plugin -lcudart 
+
+# Source and object files
+CPP_SRCS := $(wildcard common/*.cpp plugin/**/*.cpp)
+CU_SRCS := $(wildcard common/*.cu plugin/**/*.cu)
+CPP_OBJS := $(addprefix $(OUTPUT_DIR)/, $(CPP_SRCS:.cpp=.o))
+CU_OBJS := $(addprefix $(OUTPUT_DIR)/, $(CU_SRCS:.cu=.o))
+
+# Output shared library
+OUTPUT_LIB := ./fb-occ_trt_plugin_aarch64.so
+
+# Rules
+all: $(OUTPUT_DIR) $(OUTPUT_LIB)
+
+$(OUTPUT_DIR):
+	mkdir -p $(OUTPUT_DIR)
+
+$(OUTPUT_DIR)/%.o: %.cpp
+	mkdir -p $(dir $@)
+	$(CXX) $(CXXFLAGS) -c $< -o $@
+
+$(OUTPUT_DIR)/%.o: %.cu
+	mkdir -p $(dir $@)
+	$(NVCC) $(NVCCFLAGS) -c $< -o $@
+
+$(OUTPUT_LIB): $(CPP_OBJS) $(CU_OBJS)
+	$(CXX) -shared -o $@ $^ $(LDFLAGS) $(LDLIBS)
+
+clean:
+	rm -rf $(OUTPUT_DIR)
\ No newline at end of file
diff --git a/TensorRT/plugin/bev_pool_v2/bevPoolKernel.cu b/TensorRT/plugin/bev_pool_v2/bevPoolKernel.cu
index 6697df9..e969e03 100644
--- a/TensorRT/plugin/bev_pool_v2/bevPoolKernel.cu
+++ b/TensorRT/plugin/bev_pool_v2/bevPoolKernel.cu
@@ -201,4 +201,4 @@ template void bev_pool_v2(int c, int n_intervals, int num_points,
                           const int *ranks_depth, const int *ranks_feat,
                           const int *ranks_bev, const int *interval_starts,
                           const int *interval_lengths, __half *out,
-                          cudaStream_t stream);
+                          cudaStream_t stream);
\ No newline at end of file
diff --git a/TensorRT/plugin/bev_pool_v2/bevPoolKernel.h b/TensorRT/plugin/bev_pool_v2/bevPoolKernel.h
index e23eb81..f8f57c7 100644
--- a/TensorRT/plugin/bev_pool_v2/bevPoolKernel.h
+++ b/TensorRT/plugin/bev_pool_v2/bevPoolKernel.h
@@ -1,6 +1,3 @@
-//
-// Created by Derry Lin on 2023/5/11.
-//
 
 #ifndef TENSORRT_OPS_BEVPOOLKERNEL_H
 #define TENSORRT_OPS_BEVPOOLKERNEL_H
@@ -30,4 +27,4 @@ void bev_pool_v2_int8(int c, int n_intervals, int num_points,
                       const int *interval_lengths, int8_4 *out,
                       const float &scale_o, cudaStream_t stream);
 
-#endif // TENSORRT_OPS_BEVPOOLKERNEL_H
+#endif // TENSORRT_OPS_BEVPOOLKERNEL_H
\ No newline at end of file
diff --git a/TensorRT/plugin/bev_pool_v2/bevPoolPlugin.cpp b/TensorRT/plugin/bev_pool_v2/bevPoolPlugin.cpp
index 3aae089..bc18032 100644
--- a/TensorRT/plugin/bev_pool_v2/bevPoolPlugin.cpp
+++ b/TensorRT/plugin/bev_pool_v2/bevPoolPlugin.cpp
@@ -45,12 +45,13 @@ DimsExprs BEVPoolPlugin::getOutputDimensions(
     int32_t outputIndex, const nvinfer1::DimsExprs *inputs, int32_t nbInputs,
     nvinfer1::IExprBuilder &exprBuilder) noexcept {
   nvinfer1::DimsExprs ret;
-  ret.nbDims = 4;
+  ret.nbDims = 5;
   ret.d[0] = exprBuilder.constant(1);
-  ret.d[1] = exprBuilder.constant(mOutHeight);
-  ret.d[2] = exprBuilder.constant(mOutWidth);
-  ret.d[3] = inputs[1].d[3];
-  return ret;
+  ret.d[1] = exprBuilder.constant(8);
+  ret.d[2] = exprBuilder.constant(mOutHeight); 
+  ret.d[3] = exprBuilder.constant(mOutWidth);
+  ret.d[4] = inputs[1].d[4];
+  return ret; 
 }
 
 int32_t BEVPoolPlugin::initialize() noexcept { return 0; }
@@ -74,31 +75,31 @@ int32_t BEVPoolPlugin::enqueue(const nvinfer1::PluginTensorDesc *inputDesc,
   nvinfer1::Dims out_dims = outputDesc[0].dims;     // bhwc
   auto data_type = inputDesc[0].type;
   int num_points =
-      out_dims.d[0] * out_dims.d[1] * out_dims.d[2] * out_dims.d[3];
-  switch (data_type) {
-  case nvinfer1::DataType::kFLOAT:
-    bev_pool_v2(feat_dims.d[3], interval_dims.d[0], num_points,
-                (float *)inputs[0], (float *)inputs[1], (int *)inputs[2],
-                (int *)inputs[3], (int *)inputs[4], (int *)inputs[5],
-                (int *)inputs[6], (float *)outputs[0], stream);
-    break;
-  case nvinfer1::DataType::kHALF:
-    if (use_h2) {
-      bev_pool_v2_h2(feat_dims.d[3], interval_dims.d[0], num_points,
-                     (__half *)inputs[0], (__half2 *)inputs[1],
-                     (int *)inputs[2], (int *)inputs[3], (int *)inputs[4],
-                     (int *)inputs[5], (int *)inputs[6], (__half2 *)outputs[0],
-                     stream);
-    } else {
-      bev_pool_v2(feat_dims.d[3], interval_dims.d[0], num_points,
-                  (__half *)inputs[0], (__half *)inputs[1], (int *)inputs[2],
+      out_dims.d[0] * out_dims.d[1] * out_dims.d[2] * out_dims.d[3] * out_dims.d[4];
+    switch (data_type) {
+    case nvinfer1::DataType::kFLOAT:
+      bev_pool_v2(feat_dims.d[4], interval_dims.d[0], num_points,
+                  (float *)inputs[0], (float *)inputs[1], (int *)inputs[2],
                   (int *)inputs[3], (int *)inputs[4], (int *)inputs[5],
-                  (int *)inputs[6], (__half *)outputs[0], stream);
-    }
-    break;
-  case nvinfer1::DataType::kINT8:
+                  (int *)inputs[6], (float *)outputs[0], stream);
+      break;
+    case nvinfer1::DataType::kHALF:
+      if (use_h2) {
+        bev_pool_v2_h2(feat_dims.d[4], interval_dims.d[0], num_points,
+                       (__half *)inputs[0], (__half2 *)inputs[1],
+                       (int *)inputs[2], (int *)inputs[3], (int *)inputs[4],
+                       (int *)inputs[5], (int *)inputs[6], (__half2 *)outputs[0],
+                       stream);
+      } else {
+        bev_pool_v2(feat_dims.d[4], interval_dims.d[0], num_points,
+                    (__half *)inputs[0], (__half *)inputs[1], (int *)inputs[2],
+                    (int *)inputs[3], (int *)inputs[4], (int *)inputs[5],
+                    (int *)inputs[6], (__half *)outputs[0], stream);
+      }
+      break;
+    case nvinfer1::DataType::kINT8:
     bev_pool_v2_int8(
-        feat_dims.d[3], interval_dims.d[0], num_points, (int8_t *)inputs[0],
+        feat_dims.d[4], interval_dims.d[0], num_points, (int8_t *)inputs[0],
         inputDesc[0].scale, (int8_4 *)inputs[1], inputDesc[1].scale,
         (int *)inputs[2], (int *)inputs[3], (int *)inputs[4], (int *)inputs[5],
         (int *)inputs[6], (int8_4 *)outputs[0], outputDesc[0].scale, stream);
@@ -314,4 +315,4 @@ IPluginV2DynamicExt *BEVPoolPluginCreator2::deserializePlugin(
 }
 
 REGISTER_TENSORRT_PLUGIN(BEVPoolPluginCreator);
-REGISTER_TENSORRT_PLUGIN(BEVPoolPluginCreator2);
+REGISTER_TENSORRT_PLUGIN(BEVPoolPluginCreator2);
\ No newline at end of file
diff --git a/TensorRT/plugin/bev_pool_v2/bevPoolPlugin.h b/TensorRT/plugin/bev_pool_v2/bevPoolPlugin.h
index 75663cd..16bdb93 100644
--- a/TensorRT/plugin/bev_pool_v2/bevPoolPlugin.h
+++ b/TensorRT/plugin/bev_pool_v2/bevPoolPlugin.h
@@ -136,4 +136,4 @@ private:
 
 } // namespace trt_plugin
 
-#endif // TENSORRT_OPS_BEVPOOLPLUGIN_H
+#endif // TENSORRT_OPS_BEVPOOLPLUGIN_H
\ No newline at end of file
diff --git a/TensorRT/plugin/grid_sampler/gridSamplerPlugin.cpp b/TensorRT/plugin/grid_sampler/gridSamplerPlugin.cpp
index cc52952..66b03dd 100644
--- a/TensorRT/plugin/grid_sampler/gridSamplerPlugin.cpp
+++ b/TensorRT/plugin/grid_sampler/gridSamplerPlugin.cpp
@@ -121,6 +121,7 @@ int32_t GridSamplerPlugin::enqueue(const nvinfer1::PluginTensorDesc *inputDesc,
   auto data_type = inputDesc[0].type;
   ASSERT(data_type == DataType::kFLOAT || data_type == DataType::kHALF ||
          data_type == DataType::kINT8)
+  
 
   switch (data_type) {
   case DataType::kFLOAT:
diff --git a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.cu b/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.cu
index e1a70ac..5bd83de 100644
--- a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.cu
+++ b/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.cu
@@ -134,9 +134,9 @@ template <typename scalar_t>
 __device__ scalar_t ms_deform_attn_im2col_bilinear(
     const scalar_t *&bottom_data, const int &height, const int &width,
     const int &nheads, const int &channels, const scalar_t &h,
-    const scalar_t &w) {
-  const int h_low = floorf(h);
-  const int w_low = floorf(w);
+    const scalar_t &w, const int &m, const int &c) {
+  const int h_low = floor(h);
+  const int w_low = floor(w);
   const int h_high = h_low + 1;
   const int w_high = w_low + 1;
 
@@ -144,35 +144,36 @@ __device__ scalar_t ms_deform_attn_im2col_bilinear(
   const scalar_t lw = w - w_low;
   const scalar_t hh = 1 - lh, hw = 1 - lw;
 
-  const int h_low_ptr_offset = h_low * width;
-  const int h_high_ptr_offset = h_low_ptr_offset + width;
-  const int w_low_ptr_offset = w_low;
-  const int w_high_ptr_offset = w_low_ptr_offset + 1;
-  const int step = channels * nheads;
+  const int w_stride = nheads * channels;
+  const int h_stride = width * w_stride;
+  const int h_low_ptr_offset = h_low * h_stride;
+  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;
+  const int w_low_ptr_offset = w_low * w_stride;
+  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;
+  const int base_ptr = m * channels + c;
 
   scalar_t v1 = 0;
   if (h_low >= 0 && w_low >= 0) {
-    const int ptr1 = (h_low_ptr_offset + w_low_ptr_offset) * step;
+    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;
     v1 = bottom_data[ptr1];
   }
   scalar_t v2 = 0;
   if (h_low >= 0 && w_high <= width - 1) {
-    const int ptr2 = (h_low_ptr_offset + w_high_ptr_offset) * step;
+    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;
     v2 = bottom_data[ptr2];
   }
   scalar_t v3 = 0;
   if (h_high <= height - 1 && w_low >= 0) {
-    const int ptr3 = (h_high_ptr_offset + w_low_ptr_offset) * step;
+    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;
     v3 = bottom_data[ptr3];
   }
   scalar_t v4 = 0;
   if (h_high <= height - 1 && w_high <= width - 1) {
-    const int ptr4 = (h_high_ptr_offset + w_high_ptr_offset) * step;
+    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;
     v4 = bottom_data[ptr4];
   }
 
   const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;
-
   const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);
   return val;
 }
@@ -180,52 +181,61 @@ __device__ scalar_t ms_deform_attn_im2col_bilinear(
 template <>
 __device__ __half ms_deform_attn_im2col_bilinear(
     const __half *&bottom_data, const int &height, const int &width,
-    const int &nheads, const int &channels, const __half &h, const __half &w) {
-  const __half h_low = hfloor(h);
-  const __half w_low = hfloor(w);
-  const __half h_high = __hadd(h_low, __float2half(1.f));
-  const __half w_high = __hadd(w_low, __float2half(1.f));
-
-  const __half lh = __hsub(h, h_low);
-  const __half lw = __hsub(w, w_low);
-  const __half hh = __hsub(__float2half(1.f), lh),
-               hw = __hsub(__float2half(1.f), lw);
-
-  const int h_low_ptr_offset = __half2int_rn(h_low) * width;
-  const int h_high_ptr_offset = h_low_ptr_offset + width;
-  const int w_low_ptr_offset = __half2int_rn(w_low);
-  const int w_high_ptr_offset = w_low_ptr_offset + 1;
-  const int step = channels * nheads;
+    const int &nheads, const int &channels, const __half &h, 
+    const __half &w, const int &m, const int &c) {
+  
+  const int h_low = __half2int_rn(h);
+  const int w_low = __half2int_rn(w);
+  const int h_high = h_low + 1;
+  const int w_high = w_low + 1;
+
+  const __half lh = __hsub(h, __int2half_rn(h_low));
+  const __half lw = __hsub(w, __int2half_rn(w_low));
+  const __half hh = __hsub(__float2half(1.f), lh);
+  const __half hw = __hsub(__float2half(1.f), lw);
+
+  const int w_stride = nheads * channels;
+  const int h_stride = width * w_stride;
+  const int h_low_ptr_offset = h_low * h_stride;
+  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;
+  const int w_low_ptr_offset = w_low * w_stride;
+  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;
+  const int base_ptr = m * channels + c;
 
   __half v1 = 0;
-  if (__hge(h_low, __float2half(0.f)) && __hge(w_low, __float2half(0.f))) {
-    const int ptr1 = (h_low_ptr_offset + w_low_ptr_offset) * step;
+  if (h_low >= 0 && w_low >= 0) {
+    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;
     v1 = bottom_data[ptr1];
   }
   __half v2 = 0;
-  if (__hge(h_low, __float2half(0.f)) &&
-      __hle(w_high, __float2half(static_cast<float>(width - 1)))) {
-    const int ptr2 = (h_low_ptr_offset + w_high_ptr_offset) * step;
+  if (h_low >= 0 && w_high <= width - 1) {
+    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;
     v2 = bottom_data[ptr2];
   }
   __half v3 = 0;
-  if (__hle(h_high, __float2half(static_cast<float>(height - 1))) &&
-      __hge(w_low, __float2half(0.f))) {
-    const int ptr3 = (h_high_ptr_offset + w_low_ptr_offset) * step;
+  if (h_high <= height - 1 && w_low >= 0) {
+    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;
     v3 = bottom_data[ptr3];
   }
   __half v4 = 0;
-  if (__hle(h_high, __float2half(static_cast<float>(height - 1))) &&
-      __hle(w_high, __float2half(static_cast<float>(width - 1)))) {
-    const int ptr4 = (h_high_ptr_offset + w_high_ptr_offset) * step;
+  if (h_high <= height - 1 && w_high <= width - 1) {
+    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;
     v4 = bottom_data[ptr4];
   }
-  const __half w1 = __hmul(hh, hw), w2 = __hmul(hh, lw), w3 = __hmul(lh, hw),
-               w4 = __hmul(lh, lw);
+
+  const __half w1 = __hmul(hh, hw);
+  const __half w2 = __hmul(hh, lw);
+  const __half w3 = __hmul(lh, hw);
+  const __half w4 = __hmul(lh, lw);
+
   return __hadd(__hadd(__hmul(w1, v1), __hmul(w2, v2)),
                 __hadd(__hmul(w3, v3), __hmul(w4, v4)));
 }
 
+
+
+
+
 __device__ __half2 ms_deform_attn_im2col_bilinear_h2(
     const __half2 *&bottom_data, const int &height, const int &width,
     const int &nheads, const int &channels, const __half2 &wh) {
@@ -610,162 +620,115 @@ __device__ void ms_deform_attn_im2col_bilinear_int8_h2_(
 
 template <typename scalar_t>
 __global__ void ms_deformable_im2col_gpu_kernel(
-    const int n, const scalar_t *data_value, const int32_t *data_spatial_shapes,
-    const scalar_t *data_reference_points,
-    const scalar_t *data_sampling_offsets, const scalar_t *data_attn_weight,
+    const int n, const scalar_t *data_value, const int *data_spatial_shapes,
+    const int *data_level_start_index, 
+    const scalar_t *data_sampling_loc, const scalar_t *data_attn_weight, const int batch_size,
     const int spatial_size, const int num_heads, const int channels,
     const int num_levels, const int num_query, const int num_point,
-    const int points_per_group, scalar_t *data_col) {
+    scalar_t *data_col) {
   CUDA_1D_KERNEL_LOOP(index, n) {
-    const int temp = index;
-    const int channel_index = index % channels;
-    index /= channels;
-    const int head_index = index % num_heads;
-    index /= num_heads;
-    const int query_index = index % num_query;
-    const int batch_index = index / num_query;
-
-    const scalar_t *data_value_ptr =
-        data_value +
-        (batch_index * spatial_size * num_heads + head_index) * channels +
-        channel_index;
-    int data_weight_ptr =
-        ((batch_index * num_query + query_index) * num_heads + head_index) *
-        num_levels * num_point;
-    int data_offset_w_ptr = data_weight_ptr << 1;
-    int data_points_ptr =
-        (batch_index * num_query + query_index) * points_per_group * 2;
-    scalar_t *data_output_ptr = data_col + temp;
-    scalar_t output = 0;
-
-    const int all_points = num_levels * num_point;
-    scalar_t max_weight = -cuda::std::numeric_limits<scalar_t>::infinity();
-    scalar_t sum_weight = 0.f;
-#pragma unroll
-    for (int w_index = 0; w_index < all_points; ++w_index) {
-      const scalar_t weight = data_attn_weight[data_weight_ptr + w_index];
-      max_weight = max(max_weight, weight);
-    }
-
-    for (int level_index = 0; level_index < num_levels; ++level_index) {
-      const int spatial_h_ptr = level_index << 1;
+    int _temp = index;
+    const int c_col = _temp % channels;
+    _temp /= channels;
+    const int sampling_index = _temp;
+    const int m_col = _temp % num_heads;
+    _temp /= num_heads;
+    _temp /= num_query;
+    const int b_col = _temp;
+
+    scalar_t *data_col_ptr = data_col + index;
+    int data_weight_ptr = sampling_index * num_levels * num_point;
+    int data_loc_w_ptr = data_weight_ptr << 1;
+    const int qid_stride = num_heads * channels;
+    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;
+    scalar_t col = 0;
+    
+    for (int l_col = 0; l_col < num_levels; ++l_col) {
+      const int level_start_id = data_level_start_index[l_col];
+      const int spatial_h_ptr = l_col << 1;
       const int spatial_h = data_spatial_shapes[spatial_h_ptr];
       const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];
+      
+      const scalar_t *data_value_ptr =
+          data_value + (data_value_ptr_init_offset + level_start_id * qid_stride);
+      for (int p_col = 0; p_col < num_point; ++p_col) {
+        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];
+        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];
+        const scalar_t weight = data_attn_weight[data_weight_ptr];
 
-      for (int point_index = 0; point_index < num_point; ++point_index) {
-        const int point_index_per_group = point_index % points_per_group;
-        const scalar_t reference_point_x =
-            data_reference_points[data_points_ptr + point_index_per_group * 2];
-        const scalar_t reference_point_y =
-            data_reference_points[data_points_ptr + point_index_per_group * 2 +
-                                  1];
-        const scalar_t loc_w = reference_point_x * spatial_w +
-                               data_sampling_offsets[data_offset_w_ptr];
-        const scalar_t loc_h = reference_point_y * spatial_h +
-                               data_sampling_offsets[data_offset_w_ptr + 1];
-
-        const scalar_t weight =
-            exp(data_attn_weight[data_weight_ptr] - max_weight);
-        sum_weight += weight;
-
-        const scalar_t h_im = loc_h - 0.5f;
-        const scalar_t w_im = loc_w - 0.5f;
+        const scalar_t h_im = loc_h * spatial_h - 0.5;
+        const scalar_t w_im = loc_w * spatial_w - 0.5;
 
         if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w) {
-          output += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h,
-                                                   spatial_w, num_heads,
-                                                   channels, h_im, w_im) *
-                    weight;
+          col += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h,
+                                                spatial_w, num_heads, channels,
+                                                h_im, w_im, m_col, c_col) * weight;
         }
-
         data_weight_ptr += 1;
-        data_offset_w_ptr += 2;
+        data_loc_w_ptr += 2;
       }
-      data_value_ptr += spatial_h * spatial_w * channels * num_heads;
     }
-    *data_output_ptr = output / sum_weight;
+    *data_col_ptr = col;
   }
 }
 
 template <>
 __global__ void ms_deformable_im2col_gpu_kernel(
-    const int n, const __half *data_value, const int32_t *data_spatial_shapes,
-    const __half *data_reference_points, const __half *data_sampling_offsets,
-    const __half *data_attn_weight, const int spatial_size, const int num_heads,
-    const int channels, const int num_levels, const int num_query,
-    const int num_point, const int points_per_group, __half *data_col) {
+    const int n, const __half *data_value, const int *data_spatial_shapes,
+    const int *data_level_start_index, const __half *data_sampling_loc,
+    const __half *data_attn_weight, const int batch_size, const int spatial_size, const int num_heads, 
+    const int channels, const int num_levels, const int num_query, 
+    const int num_point,  __half *data_col) {
   CUDA_1D_KERNEL_LOOP(index, n) {
-    const int temp = index;
-    const int channel_index = index % channels;
-    index /= channels;
-    const int head_index = index % num_heads;
-    index /= num_heads;
-    const int query_index = index % num_query;
-    const int batch_index = index / num_query;
-
-    const __half *data_value_ptr =
-        data_value +
-        (batch_index * spatial_size * num_heads + head_index) * channels +
-        channel_index;
-    int data_weight_ptr =
-        ((batch_index * num_query + query_index) * num_heads + head_index) *
-        num_levels * num_point;
-    int data_offset_w_ptr = data_weight_ptr << 1;
-    int data_points_ptr =
-        (batch_index * num_query + query_index) * points_per_group * 2;
-    __half *data_output_ptr = data_col + temp;
-    __half output = 0.f;
-
-    const int all_points = num_levels * num_point;
-    __half max_weight = -cuda::std::numeric_limits<__half>::infinity();
-    __half sum_weight = 0.f;
-#pragma unroll
-    for (int w_index = 0; w_index < all_points; ++w_index) {
-      const __half weight = data_attn_weight[data_weight_ptr + w_index];
-      max_weight = hmax(max_weight, weight);
-    }
-
-    for (int level_index = 0; level_index < num_levels; ++level_index) {
-      const int spatial_h_ptr = level_index << 1;
+    int _temp = index;
+    const int c_col = _temp % channels;
+    _temp /= channels;
+    const int sampling_index = _temp;
+    const int m_col = _temp % num_heads;
+    _temp /= num_heads;
+    _temp /= num_query;
+    const int b_col = _temp;
+
+    __half *data_col_ptr = data_col + index;
+    int data_weight_ptr = sampling_index * num_levels * num_point;
+    int data_loc_w_ptr = data_weight_ptr << 1;
+    const int qid_stride = num_heads * channels;
+    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;
+    __half col = 0;
+    
+    for (int l_col = 0; l_col < num_levels; ++l_col) {
+      const int level_start_id = data_level_start_index[l_col];
+      const int spatial_h_ptr = l_col << 1;
       const int spatial_h = data_spatial_shapes[spatial_h_ptr];
       const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];
-
-      for (int point_index = 0; point_index < num_point; ++point_index) {
-        const int point_index_per_group = point_index % points_per_group;
-        const __half reference_point_x =
-            data_reference_points[data_points_ptr + point_index_per_group * 2];
-        const __half reference_point_y =
-            data_reference_points[data_points_ptr + point_index_per_group * 2 +
-                                  1];
-        const __half loc_w = __hfma(reference_point_x, __int2half_rn(spatial_w),
-                                    data_sampling_offsets[data_offset_w_ptr]);
-        const __half loc_h =
-            __hfma(reference_point_y, __int2half_rn(spatial_h),
-                   data_sampling_offsets[data_offset_w_ptr + 1]);
-
-        const __half weight =
-            hexp(data_attn_weight[data_weight_ptr] - max_weight);
-        sum_weight += weight;
-
-        const __half h_im = __hsub(loc_h, __float2half(0.5f));
-        const __half w_im = __hsub(loc_w, __float2half(0.5f));
-
-        if (__hgt(h_im, __float2half(-1.f)) &&
-            __hgt(w_im, __float2half(-1.f)) &&
-            __hlt(h_im, __int2half_rn(spatial_h)) &&
-            __hlt(w_im, __int2half_rn(spatial_w))) {
-          output += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h,
-                                                   spatial_w, num_heads,
-                                                   channels, h_im, w_im) *
-                    weight;
+      
+      const __half *data_value_ptr =
+          data_value + (data_value_ptr_init_offset + level_start_id * qid_stride);
+      for (int p_col = 0; p_col < num_point; ++p_col) {
+        const __half loc_w = data_sampling_loc[data_loc_w_ptr];
+        const __half loc_h = data_sampling_loc[data_loc_w_ptr + 1];
+        const __half weight = data_attn_weight[data_weight_ptr];
+
+        const __half h_im = __hsub(__hmul(loc_h, __int2half_rn(spatial_h)), __float2half(0.5f));
+        const __half w_im = __hsub(__hmul(loc_w, __int2half_rn(spatial_w)), __float2half(0.5f));
+
+        if (__hgt(h_im, __float2half(-1.f)) && __hgt(w_im, __float2half(-1.f)) &&
+            __hlt(h_im, __int2half_rn(spatial_h)) && __hlt(w_im, __int2half_rn(spatial_w))) {
+          // Pass data_value_ptr by reference
+          col += ms_deform_attn_im2col_bilinear(data_value_ptr,
+                                                   spatial_h, 
+                                                   spatial_w, 
+                                                   num_heads, 
+                                                   channels, 
+                                                   h_im, w_im, 
+                                                   m_col, c_col) * weight;
         }
-
+        
         data_weight_ptr += 1;
-        data_offset_w_ptr += 2;
+        data_loc_w_ptr += 2;
       }
-      data_value_ptr += spatial_h * spatial_w * channels * num_heads;
     }
-    *data_output_ptr = __hdiv(output, sum_weight);
+    *data_col_ptr = col;
   }
 }
 
@@ -1102,24 +1065,26 @@ __global__ void ms_deformable_im2col_gpu_kernel_int8(
     *data_output_ptr = data_output;
   }
 }
-
 template <typename scalar_t>
 void ms_deformable_im2col_cuda(const scalar_t *data_value,
-                               const int32_t *data_spatial_shapes,
-                               const scalar_t *data_reference_points,
-                               const scalar_t *data_sampling_offsets,
+                               const int *data_spatial_shapes,
+                               const int *data_level_start_index,
+                               const scalar_t *data_sampling_loc,
                                const scalar_t *data_attn_weight,
                                const int batch_size, const int spatial_size,
                                const int num_heads, const int channels,
                                const int num_levels, const int num_query,
-                               const int num_point, const int points_per_group,
-                               scalar_t *data_col, cudaStream_t stream) {
+                               const int num_point, scalar_t *data_col, 
+                               cudaStream_t stream) {
   const int num_kernels = batch_size * num_query * num_heads * channels;
+  const int num_threads = THREADS_PER_BLOCK;
+  cudaMemset((scalar_t *)data_col, 0, num_kernels * sizeof(scalar_t));
+  
   ms_deformable_im2col_gpu_kernel<scalar_t>
-      <<<GET_BLOCKS(num_kernels), THREADS_PER_BLOCK, 0, stream>>>(
-          num_kernels, data_value, data_spatial_shapes, data_reference_points,
-          data_sampling_offsets, data_attn_weight, spatial_size, num_heads,
-          channels, num_levels, num_query, num_point, points_per_group,
+      <<<GET_BLOCKS(num_kernels), num_threads, 0, stream>>>(
+          num_kernels, data_value, data_spatial_shapes, data_level_start_index, 
+          data_sampling_loc, data_attn_weight, batch_size, spatial_size, num_heads, 
+          channels, num_levels, num_query, num_point,
           data_col);
   cudaError_t err = cudaGetLastError();
   if (err != cudaSuccess) {
@@ -1127,21 +1092,26 @@ void ms_deformable_im2col_cuda(const scalar_t *data_value,
   }
 }
 
+
 template <>
 void ms_deformable_im2col_cuda(
-    const __half *data_value, const int32_t *data_spatial_shapes,
-    const __half *data_reference_points, const __half *data_sampling_offsets,
+    const __half *data_value, const int *data_spatial_shapes,
+    const int *data_level_start_index, const __half *data_sampling_loc,
     const __half *data_attn_weight, const int batch_size,
     const int spatial_size, const int num_heads, const int channels,
     const int num_levels, const int num_query, const int num_point,
-    const int points_per_group, __half *data_col, cudaStream_t stream) {
+    __half *data_col, cudaStream_t stream) {
   const int num_kernels = batch_size * num_query * num_heads * channels;
+  const int num_threads = THREADS_PER_BLOCK;
+  cudaMemset(data_col, 0, num_kernels * sizeof(__half));
   ms_deformable_im2col_gpu_kernel<__half>
-      <<<GET_BLOCKS(num_kernels), THREADS_PER_BLOCK, 0, stream>>>(
-          num_kernels, data_value, data_spatial_shapes, data_reference_points,
-          data_sampling_offsets, data_attn_weight, spatial_size, num_heads,
-          channels, num_levels, num_query, num_point, points_per_group,
+      <<<GET_BLOCKS(num_kernels), num_threads, 0, stream>>>(
+          num_kernels, data_value, data_spatial_shapes, 
+          data_level_start_index, data_sampling_loc, 
+          data_attn_weight, batch_size, spatial_size, 
+          num_heads, channels, num_levels, num_query, num_point,
           data_col);
+  
   cudaError_t err = cudaGetLastError();
   if (err != cudaSuccess) {
     printf("error in ms_deformable_im2col_cuda: %s\n", cudaGetErrorString(err));
@@ -1168,6 +1138,7 @@ void ms_deformable_im2col_cuda_h2(
   }
 }
 
+
 template <typename scalar_t>
 void ms_deformable_im2col_cuda_int8(
     const int8_4 *data_value, float scale_value,
@@ -1217,21 +1188,24 @@ void ms_deformable_im2col_cuda_int8(
   }
 }
 
+
 template void ms_deformable_im2col_cuda<float>(
-    const float *data_value, const int32_t *data_spatial_shapes,
-    const float *data_reference_points, const float *data_sampling_offsets,
+    const float *data_value, const int *data_spatial_shapes,
+    const int *data_level_start_index, const float *data_sampling_loc,
     const float *data_attn_weight, const int batch_size, const int spatial_size,
     const int num_heads, const int channels, const int num_levels,
-    const int num_query, const int num_point, const int points_per_group,
+    const int num_query, const int num_point,
     float *data_col, cudaStream_t stream);
 
+
 template void ms_deformable_im2col_cuda<__half>(
-    const __half *data_value, const int32_t *data_spatial_shapes,
-    const __half *data_reference_points, const __half *data_sampling_offsets,
+    const __half *data_value, const int *data_spatial_shapes,
+    const int *data_level_start_index, const __half *data_sampling_loc,
     const __half *data_attn_weight, const int batch_size,
     const int spatial_size, const int num_heads, const int channels,
     const int num_levels, const int num_query, const int num_point,
-    const int points_per_group, __half *data_col, cudaStream_t stream);
+    __half *data_col, 
+    cudaStream_t stream);
 
 template void ms_deformable_im2col_cuda_int8<float>(
     const int8_4 *data_value, float scale_value,
@@ -1252,3 +1226,4 @@ template void ms_deformable_im2col_cuda_int8<__half2>(
     const int num_levels, const int num_query, const int num_point,
     const int points_per_group, int8_4 *data_col, float scale_out,
     cudaStream_t stream);
+
diff --git a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.h b/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.h
index 78785d1..5beea54 100644
--- a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.h
+++ b/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnKernel.h
@@ -11,12 +11,20 @@
 
 template <typename T>
 void ms_deformable_im2col_cuda(
-    const T *data_value, const int32_t *data_spatial_shapes,
-    const T *data_reference_points, const T *data_sampling_offsets,
-    const T *data_attn_weight, const int batch_size, const int spatial_size,
-    const int num_heads, const int channels, const int num_levels,
-    const int num_query, const int num_point, const int points_per_group,
-    T *data_col, cudaStream_t stream);
+    const T *data_value, 
+    const int *data_spatial_shapes,
+    const int *data_level_start_index, 
+    const T *data_sampling_loc,
+    const T *data_attn_weight, 
+    const int batch_size, 
+    const int spatial_size,
+    const int num_heads, 
+    const int channels, 
+    const int num_levels,
+    const int num_query, 
+    const int num_point,
+    T *data_col, 
+    cudaStream_t stream);
 
 void ms_deformable_im2col_cuda_h2(
     const __half2 *data_value, const int32_t *data_spatial_shapes,
@@ -37,4 +45,4 @@ void ms_deformable_im2col_cuda_int8(
     const int points_per_group, int8_4 *data_col, float scale_out,
     cudaStream_t stream);
 
-#endif // TENSORRT_OPS_MULTISCALEDEFORMABLEATTNKERNEL_H
+#endif // TENSORRT_OPS_MULTISCALEDEFORMABLEATTNKERNEL_H
\ No newline at end of file
diff --git a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnPlugin.cpp b/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnPlugin.cpp
index 67886e0..4f6de1a 100644
--- a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnPlugin.cpp
+++ b/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnPlugin.cpp
@@ -49,11 +49,11 @@ DimsExprs MultiScaleDeformableAttnPlugin::getOutputDimensions(
     int32_t outputIndex, const nvinfer1::DimsExprs *inputs, int32_t nbInputs,
     nvinfer1::IExprBuilder &exprBuilder) noexcept {
   DimsExprs outputDim;
-  outputDim.nbDims = 4;
+  outputDim.nbDims = 3;
   outputDim.d[0] = inputs[0].d[0];
   outputDim.d[1] = inputs[3].d[1];
-  outputDim.d[2] = inputs[0].d[2];
-  outputDim.d[3] = inputs[0].d[3];
+  outputDim.d[2] = exprBuilder.operation(
+    nvinfer1::DimensionOperation::kPROD, *inputs[0].d[2], *inputs[0].d[3]);
   return outputDim;
 }
 
@@ -85,8 +85,8 @@ int32_t MultiScaleDeformableAttnPlugin::enqueue(
 
   const int points_per_group = inputDesc[2].dims.d[3] / 2;
   const int num_query = inputDesc[3].dims.d[1];
-  const int num_point = inputDesc[4].dims.d[3] / num_levels;
-
+  const int num_point = inputDesc[3].dims.d[4];
+  
   auto data_type = inputDesc[0].type;
   auto data_type_rp = inputDesc[2].type;
   ASSERT(data_type == DataType::kFLOAT || data_type == DataType::kHALF ||
@@ -96,9 +96,9 @@ int32_t MultiScaleDeformableAttnPlugin::enqueue(
   switch (data_type) {
   case DataType::kFLOAT:
     ms_deformable_im2col_cuda<float>(
-        (float *)inputs[0], (int32_t *)inputs[1], (float *)inputs[2],
-        (float *)inputs[3], (float *)inputs[4], batch, spatial_size, num_heads,
-        channels, num_levels, num_query, num_point, points_per_group,
+        (float *)inputs[0], (int *)inputs[1], (int *)inputs[2], 
+        (float *)inputs[3], (float *)inputs[4], batch, spatial_size, num_heads, 
+        channels, num_levels, num_query, num_point,
         (float *)outputs[0], stream);
     break;
   case DataType::kHALF:
@@ -110,10 +110,10 @@ int32_t MultiScaleDeformableAttnPlugin::enqueue(
           points_per_group, (__half2 *)outputs[0], stream);
     } else {
       ms_deformable_im2col_cuda<__half>(
-          (__half *)inputs[0], (int32_t *)inputs[1], (__half *)inputs[2],
+          (__half *)inputs[0], (int  *)inputs[1], (int *)inputs[2],
           (__half *)inputs[3], (__half *)inputs[4], batch, spatial_size,
           num_heads, channels, num_levels, num_query, num_point,
-          points_per_group, (__half *)outputs[0], stream);
+          (__half *)outputs[0], stream);
     }
     break;
   case DataType::kINT8:
@@ -166,13 +166,7 @@ bool MultiScaleDeformableAttnPlugin::supportsFormatCombination(
     return inOut[pos].type == nvinfer1::DataType::kINT32 &&
            inOut[pos].format == nvinfer1::TensorFormat::kLINEAR;
   case 2:
-    if (inOut[0].type == nvinfer1::DataType::kFLOAT ||
-        inOut[0].type == nvinfer1::DataType::kHALF) {
-      return inOut[pos].type == inOut[0].type &&
-             inOut[pos].format == nvinfer1::TensorFormat::kLINEAR;
-    }
-    return (inOut[pos].type == nvinfer1::DataType::kHALF ||
-            inOut[pos].type == nvinfer1::DataType::kFLOAT) &&
+    return inOut[pos].type == nvinfer1::DataType::kINT32 &&
            inOut[pos].format == nvinfer1::TensorFormat::kLINEAR;
   case 3:
     return inOut[pos].type == inOut[0].type &&
@@ -343,4 +337,4 @@ IPluginV2DynamicExt *MultiScaleDeformableAttnPluginCreator2::deserializePlugin(
 }
 
 REGISTER_TENSORRT_PLUGIN(MultiScaleDeformableAttnPluginCreator);
-REGISTER_TENSORRT_PLUGIN(MultiScaleDeformableAttnPluginCreator2);
+REGISTER_TENSORRT_PLUGIN(MultiScaleDeformableAttnPluginCreator2);
\ No newline at end of file
diff --git a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnPlugin.h b/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnPlugin.h
index c93610b..8794e32 100644
--- a/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnPlugin.h
+++ b/TensorRT/plugin/multi_scale_deformable_attn/multiScaleDeformableAttnPlugin.h
@@ -132,4 +132,4 @@ private:
 };
 } // namespace trt_plugin
 
-#endif // TENSORRT_OPS_MULTISCALEDEFORMABLEATTN_H
+#endif // TENSORRT_OPS_MULTISCALEDEFORMABLEATTN_H
\ No newline at end of file
diff --git a/det2trt/models/functions/__init__.py b/det2trt/models/functions/__init__.py
index 479b6b6..56f668d 100644
--- a/det2trt/models/functions/__init__.py
+++ b/det2trt/models/functions/__init__.py
@@ -1,17 +1,17 @@
-from .grid_sampler import grid_sampler, grid_sampler2
-from .multi_scale_deformable_attn import (
+from deployment.custom_op_functions.grid_sampler import grid_sampler, grid_sampler2
+from deployment.custom_op_functions.multi_scale_deformable_attn import (
     multi_scale_deformable_attn,
     multi_scale_deformable_attn2,
 )
-from .modulated_deformable_conv2d import (
+from deployment.custom_op_functions.modulated_deformable_conv2d import (
     modulated_deformable_conv2d,
     modulated_deformable_conv2d2,
 )
-from .rotate import rotate, rotate2
-from .inverse import inverse
-from .bev_pool_v2 import bev_pool_v2, bev_pool_v2_2
-from .multi_head_attn import qkv, qkv2
-from ..utils.register import TRT_FUNCTIONS
+from deployment.custom_op_functions.rotate import rotate, rotate2
+from deployment.custom_op_functions.inverse import inverse
+from deployment.custom_op_functions.bev_pool_v2 import bev_pool_v2, bev_pool_v2_2
+from deployment.custom_op_functions.multi_head_attn import qkv, qkv2
+from deployment.utils.trt_register import TRT_FUNCTIONS
 
 
 TRT_FUNCTIONS.register_module(module=grid_sampler)
diff --git a/det2trt/models/functions/bev_pool_v2.py b/det2trt/models/functions/bev_pool_v2.py
index 1a95bcc..6f4f079 100644
--- a/det2trt/models/functions/bev_pool_v2.py
+++ b/det2trt/models/functions/bev_pool_v2.py
@@ -1,6 +1,5 @@
 from torch.autograd import Function
-from third_party.bev_mmdet3d.ops.bev_pool_v2 import bev_pool_v2_gpu
-
+from mmdet3d.ops.bev_pool_v2 import bev_pool_v2_gpu
 
 class _BEVPoolV2(Function):
     @staticmethod
@@ -43,11 +42,9 @@ class _BEVPoolV2(Function):
         out_width,
     ):
         """run forward."""
-        feat = feat.unsqueeze(0)
-        depth = depth.unsqueeze(0)
         bev_feat_shape = (
             depth.shape[0],
-            1,
+            8,
             out_height,
             out_width,
             feat.shape[-1],
@@ -62,8 +59,6 @@ class _BEVPoolV2(Function):
             interval_starts,
             interval_lengths,
         )
-        bev_feat = bev_feat.squeeze(2)
-        bev_feat = bev_feat.permute(0, 2, 3, 1)
         return bev_feat
 
     @staticmethod
@@ -111,8 +106,8 @@ def bev_pool_v2(
     ranks_bev,
     interval_starts,
     interval_lengths,
-    out_height=128,
-    out_width=128,
+    out_height=100,
+    out_width=100,
 ):
     return _bev_pool_v2(
         depth,  # N,D,H,W
@@ -135,8 +130,8 @@ def bev_pool_v2_2(
     ranks_bev,
     interval_starts,
     interval_lengths,
-    out_height=128,
-    out_width=128,
+    out_height=100,
+    out_width=100,
 ):
     return _bev_pool_v2_2(
         depth,  # N,D,H,W
@@ -148,4 +143,4 @@ def bev_pool_v2_2(
         interval_lengths.int(),
         out_height,
         out_width,
-    )
+    )
\ No newline at end of file
diff --git a/det2trt/models/functions/grid_sampler.py b/det2trt/models/functions/grid_sampler.py
index 0c7e63f..8a68c71 100644
--- a/det2trt/models/functions/grid_sampler.py
+++ b/det2trt/models/functions/grid_sampler.py
@@ -76,7 +76,7 @@ class _GridSampler3D(Function):
         padding_mode: _int,
         align_corners: _bool,
     ):
-        grid = grid.permute(0, 2, 3, 4, 1)
+        grid = grid.permute(0, 2, 3, 4, 1)  
         grid_ = grid / 10
         output = torch.ops.aten.grid_sampler(
             input, grid_, interpolation_mode, padding_mode, align_corners
diff --git a/det2trt/models/functions/multi_scale_deformable_attn.py b/det2trt/models/functions/multi_scale_deformable_attn.py
index 36db587..3e88a99 100644
--- a/det2trt/models/functions/multi_scale_deformable_attn.py
+++ b/det2trt/models/functions/multi_scale_deformable_attn.py
@@ -13,16 +13,16 @@ class _MultiScaleDeformableAttnFunction(Function):
         g,
         value,
         value_spatial_shapes,
-        reference_points,
-        sampling_offsets,
+        value_level_start_index, 
+        sampling_locations,
         attention_weights,
     ):
         return g.op(
             "MultiScaleDeformableAttnTRT",
             value,
             value_spatial_shapes,
-            reference_points,
-            sampling_offsets,
+            value_level_start_index, 
+            sampling_locations,
             attention_weights,
         )
 
@@ -31,88 +31,20 @@ class _MultiScaleDeformableAttnFunction(Function):
         ctx,
         value,
         value_spatial_shapes,
-        reference_points,
-        sampling_offsets,
+        value_level_start_index, 
+        sampling_locations,
         attention_weights,
     ):
-        """GPU version of multi-scale deformable attention.
-
-        Args:
-            value (Tensor): The value has shape
-                (bs, mum_heads, embed_dims//num_heads, num_keys)
-            value_spatial_shapes (Tensor): Spatial shape of
-                each feature map, has shape (num_levels, 2),
-                last dimension 2 represent (h, w)
-            reference_points (Tensor): The reference points.
-            sampling_offsets (Tensor): The offset of sampling points,
-                has shape
-                (bs, num_heads, num_queries, num_levels*num_points*2),
-                the last dimension 2 represent (x, y).
-            attention_weights (Tensor): The weight of sampling points used
-                when calculate the attention, has shape
-                (bs, num_heads, num_queries, num_levels*num_points).
-
-        Returns:
-            Tensor: has shape (bs, embed_dims, num_queries)
-        """
-        num_heads, channel = value.shape[2:]
-        num_level = value_spatial_shapes.shape[0]
-        bs, num_queries = reference_points.shape[:2]
-
-        points_per_group = torch.div(
-            reference_points.shape[-1], 2, rounding_mode="floor"
-        )
-        sampling_offsets = sampling_offsets.view(
-            bs, num_queries, num_heads, num_level, -1, points_per_group, 2,
-        )
-        dim = sampling_offsets.shape[4] * num_level * 2 * points_per_group
-        offset_normalizer = torch.stack(
-            [value_spatial_shapes[..., 1], value_spatial_shapes[..., 0]], -1
-        )
-        sampling_locations = reference_points.view(
-            bs, num_queries, 1, 1, 1, -1, 2
-        ) + sampling_offsets / offset_normalizer.view(1, 1, 1, -1, 1, 1, 2)
-        sampling_locations = sampling_locations.view(
-            bs,
-            num_queries,
-            num_heads,
-            num_level,
-            torch.div(dim, num_level * 2, rounding_mode="floor"),
-            2,
-        )
-        attention_weights = attention_weights.view(
-            -1, num_level * torch.div(dim, num_level * 2, rounding_mode="floor")
-        ).softmax(-1)
-        attention_weights = attention_weights.view(
-            bs,
-            num_queries,
-            num_heads,
-            num_level,
-            torch.div(dim, num_level * 2, rounding_mode="floor"),
-        )
         im2col_step = value.shape[0]
         ctx.im2col_step = im2col_step
-
-        ctx.fp16 = False
-        if value.dtype == torch.float16:
-            ctx.fp16 = True
-            value = value.float()
-            sampling_locations = sampling_locations.float()
-            attention_weights = attention_weights.float()
-
-        value_level_start_index = torch.zeros_like(value_spatial_shapes[:, 0])
-        value_level_start_index[1:] = torch.cumsum(
-            value_spatial_shapes[:, 0] * value_spatial_shapes[:, 1], dim=0
-        )[:-1]
-
         output = ext_module.ms_deform_attn_forward(
-            value,
-            value_spatial_shapes,
-            value_level_start_index,
-            sampling_locations,
-            attention_weights,
-            im2col_step=ctx.im2col_step,
-        ).view(bs, num_queries, num_heads, channel)
+            value, 
+            value_spatial_shapes, 
+            value_level_start_index, 
+            sampling_locations, 
+            attention_weights, 
+            im2col_step=ctx.im2col_step)
+        
         ctx.save_for_backward(
             value,
             value_spatial_shapes,
@@ -120,27 +52,24 @@ class _MultiScaleDeformableAttnFunction(Function):
             sampling_locations,
             attention_weights,
         )
-        return output.half() if ctx.fp16 else output
+        return output
 
 
 class _MultiScaleDeformableAttnFunction2(_MultiScaleDeformableAttnFunction):
     @staticmethod
-    def symbolic(
-        g,
-        value,
-        value_spatial_shapes,
-        reference_points,
-        sampling_offsets,
-        attention_weights,
-    ):
+    def symbolic(g, 
+                 value, 
+                 value_spatial_shapes,
+                 level_start_index, 
+                 sampling_locations, 
+                 attention_weights):
         return g.op(
             "MultiScaleDeformableAttnTRT2",
-            value,
+            value, 
             value_spatial_shapes,
-            reference_points,
-            sampling_offsets,
-            attention_weights,
-        )
+            level_start_index, 
+            sampling_locations, 
+            attention_weights)
 
 
 _multi_scale_deformable_attn_gpu = _MultiScaleDeformableAttnFunction.apply
@@ -214,4 +143,4 @@ def multi_scale_deformable_attn2(
         reference_points,
         sampling_offsets,
         attention_weights,
-    )
+    )
\ No newline at end of file
-- 
2.25.1

