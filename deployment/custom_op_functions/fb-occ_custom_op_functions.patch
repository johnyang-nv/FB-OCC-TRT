From 5b947232c99391cfa81808db9beb4976a471afbb Mon Sep 17 00:00:00 2001
From: "John Yang (SW-TEGRA)" <johnyang@nvidia.com>
Date: Wed, 11 Dec 2024 11:12:07 -0800
Subject: [PATCH] FB-OCC trt_fn patch on derryhub_fn

---
 deployment/custom_op_functions/__init__.py          |  17 +-
 deployment/custom_op_functions/bev_pool_v2.py       |  41 +--
 deployment/custom_op_functions/grid_sampler.py      |   3 +-
 .../modulated_deformable_conv2d.py            |   1 -
 .../multi_scale_deformable_attn.py            | 249 ++++++------------
 5 files changed, 117 insertions(+), 194 deletions(-)

diff --git a/deployment/custom_op_functions/__init__.py b/deployment/custom_op_functions/__init__.py
index 479b6b6..5869e62 100644
--- a/deployment/custom_op_functions/__init__.py
+++ b/deployment/custom_op_functions/__init__.py
@@ -1,17 +1,17 @@
-from .grid_sampler import grid_sampler, grid_sampler2
-from .multi_scale_deformable_attn import (
+from deployment.custom_op_functions.grid_sampler import grid_sampler, grid_sampler2
+from deployment.custom_op_functions.multi_scale_deformable_attn import (
     multi_scale_deformable_attn,
     multi_scale_deformable_attn2,
 )
-from .modulated_deformable_conv2d import (
+from deployment.custom_op_functions.modulated_deformable_conv2d import (
     modulated_deformable_conv2d,
     modulated_deformable_conv2d2,
 )
-from .rotate import rotate, rotate2
-from .inverse import inverse
-from .bev_pool_v2 import bev_pool_v2, bev_pool_v2_2
-from .multi_head_attn import qkv, qkv2
-from ..utils.register import TRT_FUNCTIONS
+from deployment.custom_op_functions.rotate import rotate, rotate2
+from deployment.custom_op_functions.inverse import inverse
+from deployment.custom_op_functions.bev_pool_v2 import bev_pool_v2, bev_pool_v2_2, bev_pool_v2_gpu
+from deployment.custom_op_functions.multi_head_attn import qkv, qkv2
+from deployment.utils.trt_register import TRT_FUNCTIONS
 
 
 TRT_FUNCTIONS.register_module(module=grid_sampler)
@@ -33,3 +33,4 @@ TRT_FUNCTIONS.register_module(module=bev_pool_v2_2)
 
 TRT_FUNCTIONS.register_module(module=qkv)
 TRT_FUNCTIONS.register_module(module=qkv2)
+
diff --git a/deployment/custom_op_functions/bev_pool_v2.py b/deployment/custom_op_functions/bev_pool_v2.py
index 1a95bcc..de5be61 100644
--- a/deployment/custom_op_functions/bev_pool_v2.py
+++ b/deployment/custom_op_functions/bev_pool_v2.py
@@ -1,6 +1,14 @@
+import torch
 from torch.autograd import Function
-from third_party.bev_mmdet3d.ops.bev_pool_v2 import bev_pool_v2_gpu
 
+from mmdet3d.ops.bev_pool_v2.bev_pool import QuickCumsumCuda
+
+def bev_pool_v2_gpu(depth, feat, ranks_depth, ranks_feat, ranks_bev, 
+                    bev_feat_shape, interval_starts, interval_lengths):
+    x = QuickCumsumCuda.apply(depth, feat, ranks_depth, ranks_feat, ranks_bev,
+                              bev_feat_shape, interval_starts,
+                              interval_lengths)
+    return x
 
 class _BEVPoolV2(Function):
     @staticmethod
@@ -14,7 +22,7 @@ class _BEVPoolV2(Function):
         interval_starts,
         interval_lengths,
         out_height,
-        out_width,
+        out_width
     ):
         return g.op(
             "BEVPoolV2TRT",
@@ -26,7 +34,7 @@ class _BEVPoolV2(Function):
             interval_starts,
             interval_lengths,
             out_height_i=out_height,
-            out_width_i=out_width,
+            out_width_i=out_width
         )
 
     @staticmethod
@@ -40,14 +48,13 @@ class _BEVPoolV2(Function):
         interval_starts,
         interval_lengths,
         out_height,
-        out_width,
+        out_width
     ):
-        """run forward."""
-        feat = feat.unsqueeze(0)
-        depth = depth.unsqueeze(0)
+        # feat = feat.unsqueeze(0)
+        # depth = depth.unsqueeze(0)
         bev_feat_shape = (
             depth.shape[0],
-            1,
+            8,
             out_height,
             out_width,
             feat.shape[-1],
@@ -62,15 +69,13 @@ class _BEVPoolV2(Function):
             interval_starts,
             interval_lengths,
         )
-        bev_feat = bev_feat.squeeze(2)
-        bev_feat = bev_feat.permute(0, 2, 3, 1)
         return bev_feat
 
     @staticmethod
     def backward(ctx, grad_output):
         raise NotImplementedError
 
-
 class _BEVPoolV2_2(_BEVPoolV2):
     @staticmethod
     def symbolic(
@@ -111,8 +116,8 @@ def bev_pool_v2(
     ranks_bev,
     interval_starts,
     interval_lengths,
-    out_height=128,
-    out_width=128,
+    out_height=100,
+    out_width=100
 ):
     return _bev_pool_v2(
         depth,  # N,D,H,W
@@ -123,20 +128,20 @@ def bev_pool_v2(
         interval_starts.int(),
         interval_lengths.int(),
         out_height,
-        out_width,
+        out_width
     )
 
 
 def bev_pool_v2_2(
-    depth,  # N,D,H,W
+   depth,  # N,D,H,W
     feat,  # N,H,W,C
     ranks_depth,
     ranks_feat,
     ranks_bev,
     interval_starts,
     interval_lengths,
-    out_height=128,
-    out_width=128,
+    out_height=100,
+    out_width=100
 ):
     return _bev_pool_v2_2(
         depth,  # N,D,H,W
@@ -147,5 +152,5 @@ def bev_pool_v2_2(
         interval_starts.int(),
         interval_lengths.int(),
         out_height,
-        out_width,
+        out_width
     )
diff --git a/deployment/custom_op_functions/grid_sampler.py b/deployment/custom_op_functions/grid_sampler.py
index 0c7e63f..1b40e1e 100644
--- a/deployment/custom_op_functions/grid_sampler.py
+++ b/deployment/custom_op_functions/grid_sampler.py
@@ -3,7 +3,6 @@ from torch import Tensor
 from torch.types import _int, _bool
 from torch.autograd import Function
 
-
 class _GridSampler2D(Function):
     @staticmethod
     def symbolic(g, input, grid, interpolation_mode, padding_mode, align_corners):
@@ -76,7 +75,7 @@ class _GridSampler3D(Function):
         padding_mode: _int,
         align_corners: _bool,
     ):
-        grid = grid.permute(0, 2, 3, 4, 1)
+        grid = grid.permute(0, 2, 3, 4, 1)  
         grid_ = grid / 10
         output = torch.ops.aten.grid_sampler(
             input, grid_, interpolation_mode, padding_mode, align_corners
diff --git a/deployment/custom_op_functions/modulated_deformable_conv2d.py b/deployment/custom_op_functions/modulated_deformable_conv2d.py
index 76cf535..762c480 100644
--- a/deployment/custom_op_functions/modulated_deformable_conv2d.py
+++ b/deployment/custom_op_functions/modulated_deformable_conv2d.py
@@ -9,7 +9,6 @@ ext_module = ext_loader.load_ext(
     "_ext", ["modulated_deform_conv_forward", "modulated_deform_conv_backward"]
 )
 
-
 class _ModulatedDeformableConv2dFunction(Function):
     @staticmethod
     def symbolic(
diff --git a/deployment/custom_op_functions/multi_scale_deformable_attn.py b/deployment/custom_op_functions/multi_scale_deformable_attn.py
index 36db587..616c69f 100644
--- a/deployment/custom_op_functions/multi_scale_deformable_attn.py
+++ b/deployment/custom_op_functions/multi_scale_deformable_attn.py
@@ -1,4 +1,7 @@
+
+
 import torch
+import torch.nn.functional as F
 from torch.autograd import Function
 from mmcv.utils import ext_loader
 
@@ -6,23 +9,22 @@ ext_module = ext_loader.load_ext(
     "_ext", ["ms_deform_attn_backward", "ms_deform_attn_forward"]
 )
 
-
 class _MultiScaleDeformableAttnFunction(Function):
     @staticmethod
     def symbolic(
         g,
         value,
         value_spatial_shapes,
-        reference_points,
-        sampling_offsets,
+        value_level_start_index, 
+        sampling_locations,
         attention_weights,
     ):
         return g.op(
             "MultiScaleDeformableAttnTRT",
             value,
             value_spatial_shapes,
-            reference_points,
-            sampling_offsets,
+            value_level_start_index, 
+            sampling_locations,
             attention_weights,
         )
 
@@ -31,88 +33,21 @@ class _MultiScaleDeformableAttnFunction(Function):
         ctx,
         value,
         value_spatial_shapes,
-        reference_points,
-        sampling_offsets,
+        value_level_start_index, 
+        sampling_locations,
         attention_weights,
     ):
-        """GPU version of multi-scale deformable attention.
-
-        Args:
-            value (Tensor): The value has shape
-                (bs, mum_heads, embed_dims//num_heads, num_keys)
-            value_spatial_shapes (Tensor): Spatial shape of
-                each feature map, has shape (num_levels, 2),
-                last dimension 2 represent (h, w)
-            reference_points (Tensor): The reference points.
-            sampling_offsets (Tensor): The offset of sampling points,
-                has shape
-                (bs, num_heads, num_queries, num_levels*num_points*2),
-                the last dimension 2 represent (x, y).
-            attention_weights (Tensor): The weight of sampling points used
-                when calculate the attention, has shape
-                (bs, num_heads, num_queries, num_levels*num_points).
-
-        Returns:
-            Tensor: has shape (bs, embed_dims, num_queries)
-        """
-        num_heads, channel = value.shape[2:]
-        num_level = value_spatial_shapes.shape[0]
-        bs, num_queries = reference_points.shape[:2]
-
-        points_per_group = torch.div(
-            reference_points.shape[-1], 2, rounding_mode="floor"
-        )
-        sampling_offsets = sampling_offsets.view(
-            bs, num_queries, num_heads, num_level, -1, points_per_group, 2,
-        )
-        dim = sampling_offsets.shape[4] * num_level * 2 * points_per_group
-        offset_normalizer = torch.stack(
-            [value_spatial_shapes[..., 1], value_spatial_shapes[..., 0]], -1
-        )
-        sampling_locations = reference_points.view(
-            bs, num_queries, 1, 1, 1, -1, 2
-        ) + sampling_offsets / offset_normalizer.view(1, 1, 1, -1, 1, 1, 2)
-        sampling_locations = sampling_locations.view(
-            bs,
-            num_queries,
-            num_heads,
-            num_level,
-            torch.div(dim, num_level * 2, rounding_mode="floor"),
-            2,
-        )
-        attention_weights = attention_weights.view(
-            -1, num_level * torch.div(dim, num_level * 2, rounding_mode="floor")
-        ).softmax(-1)
-        attention_weights = attention_weights.view(
-            bs,
-            num_queries,
-            num_heads,
-            num_level,
-            torch.div(dim, num_level * 2, rounding_mode="floor"),
-        )
         im2col_step = value.shape[0]
         ctx.im2col_step = im2col_step
-
-        ctx.fp16 = False
-        if value.dtype == torch.float16:
-            ctx.fp16 = True
-            value = value.float()
-            sampling_locations = sampling_locations.float()
-            attention_weights = attention_weights.float()
-
-        value_level_start_index = torch.zeros_like(value_spatial_shapes[:, 0])
-        value_level_start_index[1:] = torch.cumsum(
-            value_spatial_shapes[:, 0] * value_spatial_shapes[:, 1], dim=0
-        )[:-1]
-
+        # print(value_spatial_shapes)
         output = ext_module.ms_deform_attn_forward(
-            value,
-            value_spatial_shapes,
-            value_level_start_index,
-            sampling_locations,
-            attention_weights,
-            im2col_step=ctx.im2col_step,
-        ).view(bs, num_queries, num_heads, channel)
+            value, 
+            value_spatial_shapes, 
+            value_level_start_index, 
+            sampling_locations, 
+            attention_weights, 
+            im2col_step=ctx.im2col_step)
+        
         ctx.save_for_backward(
             value,
             value_spatial_shapes,
@@ -120,98 +55,82 @@ class _MultiScaleDeformableAttnFunction(Function):
             sampling_locations,
             attention_weights,
         )
-        return output.half() if ctx.fp16 else output
-
+        return output
 
 class _MultiScaleDeformableAttnFunction2(_MultiScaleDeformableAttnFunction):
     @staticmethod
-    def symbolic(
-        g,
-        value,
-        value_spatial_shapes,
-        reference_points,
-        sampling_offsets,
-        attention_weights,
-    ):
+    def symbolic(g, 
+                 value, 
+                 value_spatial_shapes,
+                 level_start_index, 
+                 sampling_locations, 
+                 attention_weights):
         return g.op(
             "MultiScaleDeformableAttnTRT2",
-            value,
+            value, 
             value_spatial_shapes,
-            reference_points,
-            sampling_offsets,
-            attention_weights,
-        )
-
+            level_start_index, 
+            sampling_locations, 
+            attention_weights)
 
 _multi_scale_deformable_attn_gpu = _MultiScaleDeformableAttnFunction.apply
 _multi_scale_deformable_attn_gpu2 = _MultiScaleDeformableAttnFunction2.apply
 
-
-def multi_scale_deformable_attn(
-    value, value_spatial_shapes, reference_points, sampling_offsets, attention_weights
-):
-    """Multi-scale deformable attention.
-
-    Support TensorRT plugin MultiScaleDeformableAttnTRT: FP32 and FP16(nv_half).
-
-    Args:
-        value (Tensor): The value has shape
-            (bs, num_keys, mum_heads, embed_dims//num_heads)
-        value_spatial_shapes (Tensor): Spatial shape of
-            each feature map, has shape (num_levels, 2),
-            last dimension 2 represent (h, w)
-        reference_points (Tensor): The reference points.
-        sampling_offsets (Tensor): The offset of sampling points,
-            has shape
-            (bs, num_heads, num_queries, num_levels*num_points*2),
-            the last dimension 2 represent (x, y).
-        attention_weights (Tensor): The weight of sampling points used
-            when calculate the attention, has shape
-            (bs ,num_queries, num_heads, num_levels, num_points).
-
-    Returns:
-        Tensor: has shape (bs, num_queries, embed_dims)
-    """
+def multi_scale_deformable_attn(value, 
+                                spatial_shapes, 
+                                level_start_index, 
+                                sampling_locations, 
+                                attention_weights):
     assert value.is_cuda
-    return _multi_scale_deformable_attn_gpu(
-        value,
-        value_spatial_shapes,
-        reference_points,
-        sampling_offsets,
-        attention_weights,
-    )
-
-
-def multi_scale_deformable_attn2(
-    value, value_spatial_shapes, reference_points, sampling_offsets, attention_weights
-):
-    """Multi-scale deformable attention.
-
-    Support TensorRT plugin MultiScaleDeformableAttnTRT2: FP32 and FP16(nv_half2).
-
-    Args:
-        value (Tensor): The value has shape
-            (bs, num_keys, mum_heads, embed_dims//num_heads)
-        value_spatial_shapes (Tensor): Spatial shape of
-            each feature map, has shape (num_levels, 2),
-            last dimension 2 represent (h, w)
-        reference_points (Tensor): The reference points.
-        sampling_offsets (Tensor): The offset of sampling points,
-            has shape
-            (bs, num_heads, num_queries, num_levels*num_points*2),
-            the last dimension 2 represent (x, y).
-        attention_weights (Tensor): The weight of sampling points used
-            when calculate the attention, has shape
-            (bs ,num_queries, num_heads, num_levels, num_points).
-
-    Returns:
-        Tensor: has shape (bs, num_queries, embed_dims)
-    """
+    return _multi_scale_deformable_attn_gpu(value, 
+                                            spatial_shapes,
+                                            level_start_index, 
+                                            sampling_locations, 
+                                            attention_weights)
+
+def multi_scale_deformable_attn2(value, 
+                                spatial_shapes, 
+                                level_start_index, 
+                                sampling_locations, 
+                                attention_weights):
     assert value.is_cuda
-    return _multi_scale_deformable_attn_gpu2(
-        value,
-        value_spatial_shapes,
-        reference_points,
-        sampling_offsets,
-        attention_weights,
-    )
+    return _multi_scale_deformable_attn_gpu2(value, 
+                                            spatial_shapes,
+                                            level_start_index, 
+                                            sampling_locations, 
+                                            attention_weights)
+
+def multi_scale_deformable_attn_pytorch(
+        value: torch.Tensor, 
+        value_spatial_shapes: torch.Tensor,
+        sampling_locations: torch.Tensor,
+        attention_weights: torch.Tensor) -> torch.Tensor:
+
+    bs, _, num_heads, embed_dims = value.shape
+    _, num_queries, num_heads, num_levels, num_points, _ =\
+        sampling_locations.shape
+    value_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes],
+                             dim=1)
+    sampling_grids = 2 * sampling_locations - 1
+    sampling_value_list = []
+    for level, (H_, W_) in enumerate(value_spatial_shapes):
+        value_l_ = value_list[level].flatten(2).transpose(1, 2).reshape(
+            bs * num_heads, embed_dims, H_, W_)
+        sampling_grid_l_ = sampling_grids[:, :, :,
+                                          level].transpose(1, 2).flatten(0, 1)
+        sampling_value_l_ = F.grid_sample(
+            value_l_,
+            sampling_grid_l_,
+            mode='bilinear',
+            padding_mode='zeros',
+            align_corners=False)
+        
+        sampling_value_list.append(sampling_value_l_)
+
+    attention_weights = attention_weights.transpose(1, 2).reshape(
+        bs * num_heads, 1, num_queries, num_levels * num_points)
+    output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
+              attention_weights).sum(-1).view(bs, num_heads * embed_dims,
+                                              num_queries)
+    return output.transpose(1, 2).contiguous()
+
-- 
2.25.1
